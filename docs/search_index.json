[["index.html", "Introducción al Aprendizaje automático en ciencias de la salud 1 Preámbulo 1.1 Instalación de librerías necesarias para el curso", " Introducción al Aprendizaje automático en ciencias de la salud Juan R González 2022-10-16 1 Preámbulo Este bookdown sirve como notas para el curso Introducción al Aprendizaje automático en ciencias de la salud impartido en el Insituto Aragonés de Ciencias de la Salud Objetivo general: Este es un curso totalmente introductorio que permitirá al alumno profundizar en las principales técnicas de aprendizaje automático aplicadas a ciencias de la salud Objetivos específicos: Introducir al alumno a los modelos básicos de predicción Conocer cómo tratar el problema de datos no balanceados Introducir al alumno a la creación de nomogramas y métodos que faciliten la aplicación de modelos predictivos aplicados a problemas de salud Programa: Introducción al aprendizaje automático Métodos de validación cruzada Regresión Logística y nomogramas 4 Árboles de decisión: clasificación y regresión (CART), Bagged y Random Forest Métodos para tratar datos desbalanceados 1.1 Instalación de librerías necesarias para el curso Para poder reproducir todo el código de este libro se necesitan tener instaladas las siguientes librería install.packages(c(&quot;caret&quot;)) Los datos están accesibles en esta carpeta https://github.com/isglobal-brge/curso_machine_learning/tree/main/datos Este material está licenciado bajo una Creative Commons Attribution 4.0 International License. "],["introducción-al-aprendizaje-automático.html", "2 Introducción al Aprendizaje Automático", " 2 Introducción al Aprendizaje Automático El aprendizaje automático (AA) (Machine Learning en inglés) es una disciplina científica que suele incluirse en el ámbito de la Inteligencia Artificial (IA) que crea sistemas que aprenden automáticamente. Aprender en este contexto quiere decir identificar patrones complejos en millones de datos. La máquina/ordenador que realmente aprende es un algoritmo que usando datos existentes es capaz de predecir comportamientos futuros. Automáticamente, también en este contexto, implica que estos sistemas se mejoran de forma autónoma con el tiempo, sin intervención humana. En esta figura podemos observar la conexión que hay entre estas áreas y una más reciente conocida como aprendizaje profundo (AP) (Deep Learning en inglés) que veréis en el curso de Aprendizaje Automático 2. Relación entre AA, AI y AP La principal diferencia entre estas áreas radica en el objetivo (e.g pregunta científica) que queremos tratar. Así, la IA vendría a representar a un sistema no biológico que es inteligente basándose en reglas. El AA se basa en algoritmos que entrenan modelos usando datos existentes, y el AP se basa en algoritmos que parametriza redes neuronales de múltiples capas que representan los datos mediante diferentes niveles de abstracción. En la siguiente figura podemos ver la clasificación (de manera muy genérica) de los tipos de AA a los que podemos enfrentarnos Tipos de Aprendizaje Automático Sobre 2010 el AP (Deep learning) obtuvo una gran popularidad ya que ha permitido acercarse a sitemas de inteligencia artificial de forma más eficientge que ML. Los tres términos están ligados y cada uno forma una parte esencial de los otros. DL permite llevar a cabo ML, que en última instancia permite la AI. No obstante, es más fácil aprender ML como herramienta para AI. Debería realizarse un curso más avanzado para estudiar técnicas de DL que quizás no sean de mucha utilizad en problemas de biomedicina aplicados en ciencias de la salud, donde las bases de datos no son tan grandes ni complejas como las que pueden aparecer, por ejemplo, en el análisis de imágenes o en la genómica. En la siguiente figura podemos observar la principal diferencia entre AA y AP donde básicamente el AA pretende seleccionar aquellas variables que mejor predicien nuestra varaible respuesta y creando un modelo que ayude a dicha clasificación y el AP es una especi de caja negra donde todas las variables disponibles pasan a formar parte de un sistema que, replicando lo que hace el cerebro humano, aprende a cómo predecir nuestra variable resultado Un ejemplo claro y muy usado en nuestra vida cotidiana es la eliminación del rojo que aparecía antiguamente en las fotos. El modelo de AP es capaz de detectar un ojo en una imagen y elimiar dicho de color de forma automática Sin embargo, el AA trata problemas más sencillos que suelen ser a los que nos enfretamos en los estudios biomédicos en salud. En estadística, el AA se ha considerado como una ciencia independiente en la que se dispone de un conjunto de herramientas basadas en diferentes métodos y algoritmos que permiten clasificar individuos según una serie de variables. Concer estas técnicas estadísticas es de gran ayuda para la IA y el AP. Este es un ejemplo donde a partir de dos variables se intenta crear/entrenar un modelo (o regla de decisión - línea curva negra) que nos permita clasificar individuos para los que desconocemos su variable respuesta (figura de la derecha). Existen muchos métodos para llevar a cabo esta tarea y que permitan ser usados en la práctica clínica o en la toma de decisiones a nivel epidemiológico o de salud poblacional. Los cursos de aprendiaje automático normalmente suelen hablar de estos modelos/algoritmos de preducción Regresión logística Árboles de clasificación Análisis lineal discriminante KNN Regresión lasso (ridge, elastic net) Random Forest Boosting XGBoost En este primer curso introductorio hablaremos de los dos primeros métodos que son bastante usados en problemas de salud y también trataremos aspectos muy importantes para la creación de estos modelos predictivos que incluyen: cómo evaluar la capacidad predictiva de un modelo, cómo validar los modelos mediante validación cruzada, cómo tratar problemas de clasificación con grupos desbalanceados, y cómo crear nomogramas (y Shiny Apps automáticas) para usar estos modelos predictivos en la práctica clínica Con esta base, en un segundo curso más avanzado, podremos ver técnicas más sofisticadas que permitirán crear modelos con mejor capacidad predictiva que la regresión logística o los árboles de clasificación. Estas incluyen el boosting, XGboost o la regresión lasso que permite analizar datos con muchas más variables que individuos. "],["validación-cruzada.html", "3 Validación cruzada 3.1 Validación en un conjunto de datos externo 3.2 Leave-one-out cross validation (LOOCV) 3.3 K-fold cross validation (K-fold CV) 3.4 Uso de CV para estimar el hiper-parámetro 3.5 Uso de bootstrap 3.6 Imputación de datos faltantes (Información extra para los que venís al curso)", " 3 Validación cruzada La validación cruzada (CV por sus siglas en inglés) es la técnica que usamos para evaluar si un modelo está sobreajustado y para estimar cómo funcionará con nuevos datos. El sobreajuste es un peligro importante en el análisis predictivo, especialmente cuando se utilizan algoritmos de aprendizaje automático que, sin el ajuste adecuado, puede aprender datos de nuestra muestra casi a la perfección, esencialmente ajustando el ruido (o variabilidad). Cuando se utiliza un modelo de este tipo para predecir nuevos datos, con un ruido (o variabilidad) diferente, el rendimiento del modelo puede ser sorprendentemente malo. Usamos CV para ayudarnos a identificar y evitar tales situaciones. ¿Cómo podemos hacer esto? Muchos algoritmos de aprendizaje automático requieren que el usuario especifique ciertos parámetros (hiper-parámetros). Veremos más adelante que, por ejemplo, necesitaremos especificar un valor para \\(m\\) que corresponde al número de predictores elegidos al azar que se utilizarán en cada división de árbol cuando usemos random forest como algoritmo de aprendizaje. Cuanto menor sea \\(m\\), más simple será el árbol. Podemos usar CV para elegir el valor de \\(m\\) que minimiza la variación y reduce el sobreajuste. La regresión lineal no tiene parámetros que debe especificar el usuario, pero la CV aún nos ayuda a evaluar cuánto podría sobreajustarse un modelo a los datos de muestra. De manera breve, los algoritmos de cross-validation se pueden resumir como: Reserva una parte pequeña de los datos Crea (o entrena) el modelo usando el resto de datos Testa el modelo en los datos reservados. A continuación se describen algunas de las distintas técnicas de validación cruzada que existen. 3.1 Validación en un conjunto de datos externo La versión más simple de CV es el llamado método de conjunto de validación, que consta de los siguientes pasos: Dividir los datos de la muestra en dos partes: un conjunto de entrenamiento y otro de validacións. Los investigadores usan diferentes proporciones, pero es común seleccionar al azar el 70% de los datos como conjunto de entrenamiento y el 30% como conjunto de prueba o validación. . (Obviamente, debemos tener suficientes datos en la muestra para ajustar un modelo después de dividir los datos). Debido a que CV se basa en un muestreo aleatorio, nuestros resultados variarán a menos que usemos set.seed (). Demostraremos usando los datos de Hitters, que es un estudio sobre bateadores de USA donde se pretende crear un modelo que prediga el salario que tendría un jugador en función de sus habilidades. La variable de interés es el salario (Salary) y como es una variable continua usaremos un modelo de regresión lineal para ilustrar el concepto de validación cruzada ya que este modelo es conocido de otros cursos anteriores. Es importante notar que para el aprendizaje automático necesitamos que nuestra base de datos teng casos completos. Es decir, es importante no tener missings (existen métodos para imputar datos pero está fuera de lo que cubre este curso). Los datos están en la librería ISLR que se puede instalar de CRAN. También cargamos la libería tidyverse que nos hará falta para el manejo de datos library(arm) library(tidyverse) library(ISLR) set.seed(123) # para que los resultados sean comparables entre ordenadores Hitters_complete &lt;- Hitters[complete.cases(Hitters), ] rows &lt;- sample(nrow(Hitters_complete), .7 * nrow(Hitters_complete)) train &lt;- Hitters_complete[rows, ] test &lt;- Hitters_complete[-rows, ] Ajustar un modelo en el conjunto de entrenamiento usando un procedimiento de selección de variables apropiado. Crearemos dos modelos para comparar: uno con todas las variables, luego otro con solo las variables elegidas por regsubsets (). full_model &lt;- lm(Salary ~., data = train) select_model &lt;- lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = train) Utilizar ese modelo para predecir en el conjunto de prueba. El rendimiento en el conjunto de prueba es la estimación de CV para el rendimiento fuera de la muestra del modelo. Para ello se puede usar cualquier medida de ajuste, que para los modelos lineales puede ser el rmse (root mean square error). Veremos más adelante otras medidas de ajuste cuando nuestra variable de interés es categórica (e.g. caso/control, responde/no responde, ) rmse &lt;- function(fitted, actual){ sqrt(mean((fitted - actual)^2)) } results &lt;- data.frame(Model = c(&quot;Modelo completo muestra entrenamiento&quot;, &quot;Modelo seleccionado muestra entrenamiento&quot;, &quot;Modelo completo muestra validación&quot;, &quot;Modelo seleccionado muestra validación&quot;), RMSE = round(c(rmse(fitted(full_model), train$Salary), rmse(fitted(select_model), train$Salary), rmse(predict(full_model, newdata = test), test$Salary), rmse(predict(select_model, newdata = test), test$Salary)),1)) results Model RMSE 1 Modelo completo muestra entrenamiento 297.8 2 Modelo seleccionado muestra entrenamiento 326.1 3 Modelo completo muestra validación 368.2 4 Modelo seleccionado muestra validación 306.4 Podemos ver que el modelo completo está sobreajustado  el RMSE dentro de la muestra es mejor que el RMSE fuera de muestra  mientras que el modelo seleccionado elegido por regsubsets () usando BIC no está sobreajustado. De hecho, el modelo seleccionado funciona mucho mejor fuera de la muestra que dentro de la muestra, aunque este resultado en particular es probablemente una cuestión de azar, una función de división aleatoria que estamos usando. Sin embargo, en general, estos resultados ilustran el peligro de la complejidad del modelo y por qué tiene sentido elegir predictores utilizando medidas de ajuste del modelo que penalicen la complejidad. Los modelos simples tienden a generalizar mejor. Esta figura muestra estas relaciones: Sobreajuste 3.2 Leave-one-out cross validation (LOOCV) Este método funciona de la siguiente manera: Extrae una observación de los datos y usa el resto para entrenar el modelo Testa el modelo con la observación que ha sido extraída en el paso anterior y guarda el error asociado a esa predicción Repite el proceso para todos los puntos Calcula el error de predicción global usando el promedio de todos los errores estimados en el paso 2. Veremos más adelante cómo hacer estos cálculos con una libería específica. Aquellos que tengáis un nivel medio/alto de R quizás os podríais plantear este ejercicio (no es obligatorio - pondré la solución en moodle) EJERCICIO (no obligatorio): Crea una función R que lleve a cabo el procedimiento de LOOCV y estima el valor de LOOCV para el modelo completo (full_model) y el modelo seleccionado (select_model) del ejemplo anterior. 3.3 K-fold cross validation (K-fold CV) La diferencia con LOOCV es que este método evalúa el comportamiento del modelo en un conjunto de datos de distingo tamaño (K). El algoritmo es el siguiente: Separa los datos en k-subconjuntos (k-fold) de forma aleatoria Guarda uno de los subconjuntos de datos y entrena el modelo con el resto de individuos Testa el modelo con los datos resevados y guarda el error de predicción promedio. Repite el proceso hasta que los k subconjuntos hayan servido de muestra test. Calcula el promedio de los k errores que han sido guardados. Este valor es el error de cross-validación y nos sirve para evaluar el comportamiento de nuestro modelo como si lo usáramos en una base de datos externa. La principal ventaja de este método respecto a LOOCV es el coste computacional. Otra ventaja que no es tan obvia, es que este método a menudo da mejores estimaciones del error del modelo que LOOCV1. Una pregunta típica es cómo se escoje el valor óptimo de K. Valores pequeños de K da estimaciones sesgadas. Por otro lado, valores grandes de K están menos sesgados, pero tienen mucha variabilidad. En la práctica, normalmente se usan valores de k = 5 or k = 10, ya que estos valores se han mostrado de forma empírica como los que tienen tasas de error estimadas no demasiado sesgadas ni con mucha varianza. Al igual que en el caso anterior veremos unas liberías adecuadas para hacer estos análisis de forma eficiente. De momento se pòdría realizar el siguiente ejercicio (pondré la solución - no es obligatorio): EJERCICIO (no obligatorio): Crea una función R que lleve a cabo el procedimiento de K-fold CV y estima el valor de K-fold CV para el modelo completo (full_model) y el modelo seleccionado (select_model) del ejemplo anterior. Haz que la función tenga un parámetro que dependa de K, y da los resultados para K=5 y K=10. 3.4 Uso de CV para estimar el hiper-parámetro Si el algoritmo de aprendizaje automático que vamos a utilizar para realizar predicciones tiene un parámetro que controla el comportamiento (por ejemplo grado de polinomio en regresión no lineal, o el número de nodos en árboles de clasificación) éste podría elegirse de forma que minimizara el error de clasificación. Esta selección también puede dar problemas de sobre ajuste ya que podríamos seleccionar de forma que ajustara perféctamente a nuestros datos. Para evitar el problema, se puede utilizar cualquiera de las técnicas vistas con anterioridad. Aquí tenemos un ejemplo donde se ha usado un modelo de aprendizaje que se basa en introducir términos polinómicos de varaibles para realizar una mejor predicción mediante regresión lineal usando sólo términos lineales. Sobreajuste según un hiper-parámetro 3.5 Uso de bootstrap Si en vez de partir nuestra muestra en \\(K\\) submuestras, realizamos una selección aleatoria de muestras con reemplazamiento, nos encontraremos ante una aproximación de tipo bootstrap que es una técnica muy usada en estadística para hacer inferencia cuando la distribución del estadístico es desconocida basada en el remuestreo [aquí tenéis una descripción sencilla de esta metodología]. Boostrap Boostrap De manera que el procedimiento bootstrap aplicado a regresión sería: Sacar una muestra aleatoria con remplazamiento de tamaño \\(n\\) de nuestros datos (tenemos \\(n\\) observaciones) Guardar las muestras que no han sido seleccionadas (datos de prueba) Entrena el modelo con la muestra bootstrap Testa el modelo con los datos de prueba y guarda el error de predicción promedio. Repite el proceso \\(B\\) veces Calcula el promedio de los \\(B\\) errores que han sido guardados. Este valor es el error bootstrap y nos sirve para evaluar el comportamiento de nuestro modelo. 3.6 Imputación de datos faltantes (Información extra para los que venís al curso) La mayoría de métodos para aprendizaje automático requiren casos completos. Sin embargo, los datos reales a menudo tienen observaciones faltantes. La función lm (), analiza casos completos sin indicar nada al usuario, pero  ¿Deberíamos eliminar estas filas o imputar las observaciones que faltan? Casi siempre es mejor imputar, aunque, en la práctica puede que no valga la pena imputar algunas observaciones faltantes, ya que eliminarlas no suele cambiar el ajuste en absoluto. La imputación de datos faltantes es un tema extenso y complicado; aquí haremos una breve introducción y discutiremos los principales temas a tener en cuenta. Tipos de valores perdidos: Falta completamente al azar (MCAR por sus siglas en inglés): la probabilidad de que falte una observación es la misma para todos los casos. Eliminar los casos que faltan en esta instancia no causará sesgos, aunque es posible que perdamos información. Missing at random (MAR pos sus siglas en inglés): la probabilidad de que falte una observación depende de un mecanismo conocido. Por ejemplo, es menos probable que algunos grupos respondan encuestas. Si conocemos la pertenencia a un grupo, podemos eliminar las observaciones faltantes siempre que incluyamos el grupo como factor en una regresión. Sin embargo, generalmente podemos hacer algo mejor que simplemente eliminar estos casos. Missing not at random (MNAR por sus siglas en inglés) : la probabilidad de que falte una observación depende de algún mecanismo desconocido  una variable no observada. Tratar los problemas del MNAR es difícil o incluso imposible. Nos centraremos en los problemas MAR. Una solución simple es completar o imputar los valores MAR. Hay dos estrategias principales: Imputación simple reemplaza los valores perdidos según una estadística univariante o un modelo de regresión multivariable. Existen muchas librerías que implementan diferentes métodos (en este curso veremos algunas). En la imputación con medianas imputamos los datos faltantes usando la mediana de la variable que presenta datos faltantes (La mediana es mejor que la media cuando los datos de la columna están sesgados). Podemos imputar también usando KNN o random forest creando un modelo multivariante de las observaciones faltantes usando otras variables y usar ese modelo para predecir los valores faltantes. El problema con la imputación simple, teóricamente, es que la variabilidad de la variable imputada es menor de lo que habría sido la variabilidad en la variable real, creando un sesgo hacia 0 en los coeficientes. Por tanto, mientras que la eliminación pierde información, la imputación única puede provocar sesgos. (Sin embargo, no me queda claro cuán grande es este problema en la práctica). La imputación múltiple aborda estos problemas imputando los valores faltantes con un modelo multivariante, pero agregando la variabilidad de nuevo al volver a incluir la variación del error que normalmente veríamos en los datos. El término múltiple en la imputación múltiple se refiere a los múltiples conjuntos de datos creados en el proceso de estimación de los coeficientes de regresión. Los pasos son los siguientes: Crear \\(m\\) conjuntos de datos completos con valores perdidos imputados. Las imputaciones se realizan extrayendo aleatoriamente distribuciones de valores plausibles para cada vector de columna (variables). Ajustar un modelo lineal en cada conjunto de datos imputados y almacene \\(\\hat \\beta\\)s y SE. Promediar los \\(\\hat \\beta\\)s y combinar los SE para producir coeficientes basados en múltiples conjuntos de datos imputados. Específicamente, \\[\\hat \\beta_ {j} = \\frac {1} {m} \\sum_ {i} \\hat \\beta_ {ij}\\] y \\[s ^ 2_j = \\frac {1} {m} \\sum_{i} s^2_{ij} + var \\hat \\beta_ {ij} (1 + 1 / m),\\] donde \\(\\hat \\beta_{ij}\\) y \\(s_{ij}\\) son las estimaciones y los errores estándar del resultado imputado \\(i^{th}\\) para \\(i=1, ..., m\\) y para el parámetro \\(j^{th}\\). La imputación múltiple funciona mejor para la descripción que para la predicción, y probablemente sea preferible a la imputación única si sólo queremos estimar coeficientes. Para la predicción (como es el caso del aprendizaje automático), normalmente bastará con utilizar imputación simple. Demostraremos métodos de imputación utilizando los datos de Carseats del paquete ISLR. Este es un conjunto de datos simulado de ventas de asientos de coche, del cual eliminaremos aleatoriamente el 25% de las observaciones usando la función prodNA () en el paquete missForest (teniendo cuidado de dejar la variable de resultado, Sales, intacta). library(missForest) data(Carseats, package=&quot;ISLR&quot;) levels(Carseats$ShelveLoc) &lt;- c(&quot;Bad&quot;,&quot;Medium&quot;,&quot;Good&quot;) # Reordenamos los niveles de la variable set.seed(123) carseats_missx &lt;- prodNA(Carseats[,-1], noNA=.25) carseats_miss &lt;- cbind(Sales=Carseats[, 1], carseats_missx) glimpse(carseats_miss) Rows: 400 Columns: 11 $ Sales &lt;dbl&gt; 9.50, 11.22, 10.06, 7.40, 4.15, 10.81, 6.63, 11.85, 6.54, 4.69, 9.01, 11.96, 3.98, 10.96, 11.17, 8.71, 7.58, 12.29, 1~ $ CompPrice &lt;dbl&gt; 138, 111, 113, 117, 141, 124, 115, NA, NA, NA, 121, 117, NA, 115, 107, NA, 118, NA, 110, 129, 125, 134, 128, NA, 145,~ $ Income &lt;dbl&gt; 73, 48, 35, 100, 64, 113, NA, 81, 110, 113, 78, 94, NA, 28, 117, 95, 32, 74, 110, 76, NA, NA, 46, NA, 119, 32, 115, 1~ $ Advertising &lt;dbl&gt; 11, 16, NA, 4, 3, 13, NA, 15, 0, 0, 9, 4, 2, NA, 11, 5, NA, 13, 0, 16, 2, 12, 6, 0, 16, 0, 11, 0, NA, 15, NA, 16, 12,~ $ Population &lt;dbl&gt; 276, 260, 269, NA, 340, 501, 45, 425, 108, 131, 150, 503, NA, 29, 148, 400, 284, 251, 408, 58, 367, 239, 497, 292, 29~ $ Price &lt;dbl&gt; 120, NA, NA, 97, 128, 72, 108, 120, NA, 124, 100, NA, NA, NA, 118, 144, 110, 131, 68, 121, NA, 109, 138, NA, 113, 82,~ $ ShelveLoc &lt;fct&gt; Bad, NA, Good, NA, Bad, Bad, Good, NA, Good, Good, Bad, Medium, NA, Medium, Medium, Good, Medium, Medium, Medium, Goo~ $ Age &lt;dbl&gt; 42, 65, NA, 55, 38, NA, 71, 67, 76, 76, 26, 50, NA, 53, 52, 76, 63, 52, 46, 69, NA, NA, NA, 79, 42, 54, 50, 64, NA, 5~ $ Education &lt;dbl&gt; NA, 10, 12, NA, 13, 16, 15, 10, 10, 17, 10, 13, NA, NA, NA, 18, 13, 10, 17, 12, 18, NA, NA, NA, 12, 11, 11, 17, 11, 1~ $ Urban &lt;fct&gt; NA, Yes, Yes, Yes, Yes, NA, NA, Yes, No, NA, NA, Yes, Yes, Yes, Yes, No, Yes, Yes, No, NA, Yes, No, Yes, NA, Yes, No,~ $ US &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, No, Yes, NA, Yes, Yes, Yes, No, Yes, Yes, No, No, NA, Yes, Yes, NA, Yes, No, No, NA, No,~ Observamos que hay datos faltantes. Podemos tener una estadística global de la falta de información que hay en nuestros tanto de forma numérica mediante la librería skimr: skimr::skim(carseats_miss) Table 3.1: Data summary Name carseats_miss Number of rows 400 Number of columns 11 _______________________ Column type frequency: factor 3 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts ShelveLoc 93 0.77 FALSE 3 Goo: 164, Bad: 73, Med: 70 Urban 101 0.75 FALSE 2 Yes: 207, No: 92 US 104 0.74 FALSE 2 Yes: 193, No: 103 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sales 0 1.00 7.50 2.82 0 5.39 7.49 9.32 16.27  CompPrice 94 0.76 123.88 14.97 77 115.00 123.00 134.00 161.00  Income 100 0.75 68.63 27.85 21 43.50 68.50 90.25 120.00  Advertising 94 0.76 6.70 6.71 0 0.00 5.00 12.00 29.00  Population 107 0.73 268.47 147.29 10 144.00 272.00 402.00 509.00  Price 96 0.76 115.34 24.02 24 100.75 117.00 131.00 191.00  Age 104 0.74 52.88 16.04 25 39.00 54.00 65.00 80.00  Education 107 0.73 13.82 2.63 10 11.00 14.00 16.00 18.00  Comprobamos como la falta de información en las variables (excepto Sales) es de aproximadamente el 25% (columna complete_rate ~ 75%). Pero si tuviéramos que analizar datos de dos o más covariables, el porcentaje de datos completos disminuiría radicalmente. # Total de individuos nrow(carseats_miss) [1] 400 # Total de individuos con casos completos carseats_miss %&gt;% complete.cases() %&gt;% sum() [1] 23 Es decir, si tuviéramos que estimar un modelo con todas las variables de nuestra base de datos sólo dispondríamos de información efectiva para 23 individuos del total de 400. La librería VIM nos puede ayudar a tener esta información de forma gráfica: library(VIM) aggr(carseats_miss) Ahora faltan muchas observaciones. Cuando ajustamos un modelo de regresión para la variable Sales observamos que lm () analiza casos completos y se estima un modelo basado en un subconjunto muy pequeño de datos. lm(Sales ~ CompPrice + Income + Advertising + Population + Price, data = carseats_miss) Call: lm(formula = Sales ~ CompPrice + Income + Advertising + Population + Price, data = carseats_miss) Coefficients: (Intercept) CompPrice Income Advertising Population Price 6.23698 0.10467 0.01373 0.12638 -0.00121 -0.11283 Sólo tenemos 93 observaciones de las 400 originales! Demostraremos la imputación múltiple usando la función mice () de la librería mice. library(mice) names(Carseats) [1] &quot;Sales&quot; &quot;CompPrice&quot; &quot;Income&quot; &quot;Advertising&quot; &quot;Population&quot; &quot;Price&quot; &quot;ShelveLoc&quot; &quot;Age&quot; &quot;Education&quot; [10] &quot;Urban&quot; &quot;US&quot; carseats_imp &lt;- mice(carseats_miss, printFlag = F) El objeto carseats_imp incluye (entre muchas otras cosas) \\(m\\) conjuntos de datos imputados (la configuración predeterminada es \\(m\\) = 5). Los conjuntos de datos imputados difieren porque las imputaciones se extraen aleatoriamente de distribuciones de valores plausibles. Podemos visualizar la variabilidad de los predictores en estos conjuntos de datos imputados usando la función densityplot (). library(lattice) densityplot(carseats_imp) Las líneas azules continuas representan la distribución real de los predictores, mientras que las líneas rojas muestran las distribuciones imputadas. El siguiente paso es usar estos conjuntos de datos imputados para promediar los \\(\\beta\\)s y los SE utilizando la función pool () de la librería mice. carseats_model_imp &lt;- with(data = carseats_imp, exp = lm(Sales ~ CompPrice + Income + Advertising + Population + Price)) mi &lt;- summary(pool(carseats_model_imp)) Estos coeficientes son similares a los del modelo anterior ajustado utilizando los datos no imputados, pero deberían estar más cerca de los valores de la población porque, en lugar de simplemente eliminar los casos incompletos, utiliza información de distribución para hacer suposiciones fundamentadas sobre los datos faltantes. La imputación múltiple funciona mejor para fines de descripción  estimar coeficientes para informar en un artículo académico, por ejemplo  pero usarla para predecir nuevos datos es incómodo o imposible, por las siguientes razones: Si los nuevos datos están completos, podemos utilizar las estimaciones de coeficientes derivadas de la imputación múltiple en una ecuación de regresión para la predicción. Pero esto es difícil ya que hay que hacerlo manualmente. Usamos los datos originales de Carseats como ilustración. preds &lt;- mi[1, 2] + mi[2, 2]*Carseats$CompPrice + mi[3, 2]*Carseats$Income + mi[4, 2]*Carseats$Advertising + mi[5, 2]*Carseats$Population + mi[6, 2]*Carseats$Price head(preds) [1] 9.044928 10.251755 9.775482 8.417813 7.358304 12.816057 Si los nuevos datos no están completos, entonces estos coeficientes imputados son inútiles para predecir en filas con observaciones faltantes. Esto, por ejemplo, es el resultado de intentar predecir utilizando los datos con observaciones faltantes. preds &lt;- mi[1, 2] + mi[2, 2]*carseats_miss$CompPrice + mi[3, 2]*carseats_miss$Income + mi[4, 2]*carseats_miss$Advertising + mi[5, 2]*carseats_miss$Population + mi[6, 2]*carseats_miss$Price head(preds) [1] 9.044928 NA NA NA 7.358304 12.816057 La imputación múltiple, por lo tanto, no resuelve el principal problema al que nos enfrentamos a menudo con los datos faltantes, que es que, aunque hayamos ajustado con éxito un modelo en nuestros datos, el conjunto de validación también puede tener observaciones faltantes, y nuestras predicciones utilizando esos datos puede no poder realizarse. Podríamos usar uno de los conjuntos de datos imputados, pero entonces ya no estamos haciendo imputación múltiple sino imputación simple. En ese momento, los métodos disponibles en el paquete mice ya no ofrecen ninguna ventaja especial sobre los de los paquetes caret y missForest. De hecho, podrían ser peores ya que la función mice () no fue diseñado para producir la mejor imputación individual, sino más bien una gama de imputaciones plausibles. Usando caret, podemos hacer una imputación simple usando knnImpute, medianImpute o bagImpute. Estos métodos solo funcionan para variables numéricas, por lo que crearemos una función personalizada para convertir los factores  Shelveloc, Urban y US  en números enteros. (Al usar el conjunto de datos imputados para la regresión, podríamos dejar estas variables como números enteros, siempre que los valores enteros correspondan a los niveles de los factores). library(caret) make_df_numeric &lt;- function(df){ data.frame(sapply(df, function(x) as.numeric(x))) } carseats_miss_num &lt;- make_df_numeric(carseats_miss) med_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;medianImpute&quot;)), carseats_miss_num) knn_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;knnImpute&quot;)), carseats_miss_num) bag_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;bagImpute&quot;)), carseats_miss_num) El paquete missForest ofrece otra solución de imputación única, que es más simple que las funciones de caret porque maneja datos categóricos automáticamente. Si bien missForest funciona bien para conjuntos de datos pequeños y proporciona imputaciones de buena calidad, es muy lento en conjuntos de datos grandes. De hecho, lo mismo ocurrirá con la función bagImpute () de caret. En tales casos, podría tener sentido usar la función medianImpute () de caret en su lugar que es muy rápida. mf_imp &lt;- missForest(carseats_miss, verbose = F) missForest iteration 1 in progress...done! missForest iteration 2 in progress...done! missForest iteration 3 in progress...done! missForest iteration 4 in progress...done! missForest iteration 5 in progress...done! missForest iteration 6 in progress...done! missForest iteration 7 in progress...done! Comparemos los errores asociados con estos diferentes métodos de imputación. Podemos hacer esto porque, habiendo creado las observaciones faltantes en primer lugar, podemos comparar las observaciones imputadas con las observaciones verdaderas calculando la suma de los cuadrados de la diferencia. Para las imputaciones usando mice () calculamos los errores para cada uno de los 5 conjuntos de datos imputados. Los resultados de knnImpute () no son comparables porque la función automáticamente centra y escala las variables y los hemos omitido. comparison &lt;- data.frame(Method = c(&quot;mice 1&quot;, &quot;mice 2&quot;, &quot;mice 3&quot;, &quot;mice 4&quot;, &quot;mice 5&quot;, &quot;medianImpute&quot;, &quot;bagImpute&quot;, &quot;missForest&quot;), RMSE = c(rmse(make_df_numeric(complete(carseats_imp, 1)), make_df_numeric(Carseats)), rmse(make_df_numeric(complete(carseats_imp, 2)), make_df_numeric(Carseats)), rmse(make_df_numeric(complete(carseats_imp, 3)), make_df_numeric(Carseats)), rmse(make_df_numeric(complete(carseats_imp, 4)), make_df_numeric(Carseats)), rmse(make_df_numeric(complete(carseats_imp, 5)), make_df_numeric(Carseats)), rmse(med_imp, make_df_numeric(Carseats)), rmse(bag_imp, make_df_numeric(Carseats)), rmse(make_df_numeric(mf_imp$ximp), make_df_numeric(Carseats)))) comparison %&gt;% mutate(RMSE = round(RMSE)) %&gt;% arrange(RMSE) Method RMSE 1 mice 1 NA 2 mice 2 NA 3 mice 3 NA 4 mice 4 NA 5 mice 5 NA 6 medianImpute NA 7 bagImpute NA 8 missForest NA missforest obtiene los mejores resultados, aunque medianImpute compara muy bien. Los resultados de mice no son muy buenos, probablemente por las razones mencionadas anteriormente: está diseñado para una imputación múltiple, no simple. James et al. 2014 "],["regresión-logística.html", "4 Regresión logística 4.1 La función logit inversa 4.2 Ejemplo de regresión logística 4.3 Coeficientes de regresión logística como probabilidades 4.4 Coeficientes de regresión logística como razones de odds 4.5 Capacidad predictiva de un modelo de clasificación 4.6 Ejemplo de regresión logística: modelización de riesgo diabetes 4.7 Creación de un modelo y validación 4.8 Nomogramas", " 4 Regresión logística Este capítulo introduce la regresión logística como el método más sencillo para crear modelos predictivos en problemas de clasificación que es el principal objetivo del curso. Se cubrirán los siguientes temas: Conocer la función logística Cómo interpretar los coeficientes de los modelos Cómo evaluar la capacidad predictiva de un modelo Cómo interpretar variables Ilustrar un ejemplo de análisis completo Aprender a hacer nomogramas fijos y dinámicos Hasta ahora, nuestra variable de resultado era continua. Pero si la variable de resultado es binaria (0/1, No/Sí), entonces nos enfrentamos a un problema de clasificación. El objetivo de la clasificación es crear un modelo capaz de clasificar el resultado  y, cuando se usa el modelo para la predicción, nuevas observaciones en una de dos categorías. La regresión logística se introduce en el contexto de la epidemiología como un modelo de regresión que extiende el modelo lineal cuando nuestra variable respuesta es binaria, pero tambié es, probablemente, el método estadístico más utilizado para la clasificación y el más sencillo. Una de las grandes ventajas de estos modelos respecto a otros que veremo más adelante es que este método produce un modelo de probabilidad para nuestra variable resultado. En otras palabras, los valores ajustados en un modelo logístico o logit no son binarios sino que son probabilidades que representan la probabilidad de que el resultado pertenezca a una de nuestras dos categorías. Desafortunadamente, debemos afrontar nuevas complicaciones cuando trabajamos con regresión logística, lo que hace que estos modelos sean inherentemente más difíciles de interpretar que los modelos lineales. Las complicaciones surgen del hecho de que con la regresión logística modelamos la probabilidad de que \\(y\\) = 1, y la probabilidad siempre se escala entre 0 y 1. Pero el predictor lineal, \\(X_i \\beta\\), oscila entre \\(\\pm \\infty\\) (donde \\(X\\) representa todos los predictores del modelo). Esta diferencia de escala requiere transformar la variable de resultado, lo cual se logra con la función logit: \\[ \\text{logit}(x) = \\text{log}\\left( \\frac{x}{1-x} \\right) \\] La función logit asigna el rango del resultado (0,1) al rango del predictor lineal \\((-\\infty, +\\infty)\\). El resultado transformado, \\(\\text{logit} (x)\\), se expresa en logaritmos de probabilidades (\\(\\frac{x}{1-x}\\) se conoce como probabilidades del resultado - razón de odds en inglés - momios en castellano). Así que el modelo también se puede escribir como: \\[\\text{Pr}(y_i = 1) = p_i\\] \\[\\text{logit}(p_i) = X_i\\beta\\] Las probabilidades logarítmicas (e.g. el log-odds) no tienen interpretación (que no sea el signo y la magnitud) y deben transformarse nuevamente en cantidades interpretables, ya sea en probabilidades, usando el logit inverso, o en razones de probabilidades, mediante el uso de la función exponencial. Discutimos ambas transformaciones a continuación. 4.1 La función logit inversa El modelo logístico se puede escribir, alternativamente, usando el logit inverso: \\[ \\operatorname{Pr}(y_i = 1 | X_i) = \\operatorname{logit}^{-1}(X_i \\beta) \\] donde \\(y_i\\) es la respuesta binaria, \\(\\operatorname{logit}^{- 1}\\) es la función logit inversa y \\(X_i \\beta\\) es el predictor lineal. Podemos interpretar esta formulación diciendo que la probabilidad de que \\(y = 1\\) es igual al logit inverso del predictor lineal \\((X_i, \\ beta)\\). Por lo tanto, podemos expresar los valores ajustados del modelo logístico y los coeficientes como probabilidades utilizando la transformación logit inversa. Pero, ¿qué es exactamente el logit inverso? Pues es: \\[\\operatorname{logit}^{-1}(x) = \\frac{e^{x}}{1 + e^{x}}\\] donde \\(e\\) es la función exponencial. Podemos tener una idea de cómo la función logit inversa transforma el predictor lineal mediante una gráfica. Aquí usamos un rango arbitrario de valores de x en (-6, 6) para demostrar la transformación. x &lt;- seq(-6, 6, .01) y &lt;- exp(x)/(1 + exp(x)) ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_line() + ylab(expression(paste(logit^-1,&quot;(x)&quot;))) + ggtitle(expression(paste(&quot;y = &quot;, logit^-1,&quot;(x)&quot;))) Los valores \\(x\\), que van de -6 a 6, son comprimidos por la función logit inversa en el rango 0-1. El logit inverso es curvo, por lo que la diferencia esperada en \\(y\\) correspondiente a una diferencia fija en \\(x\\) no es constante. A valores bajos y valores altos de \\(x\\), un cambio de unidad corresponde a un cambio muy pequeño en \\(y\\), mientras que en la mitad de la curva un pequeño cambio en \\(x\\) corresponde a un cambio relativamente grande en \\(y\\). En la regresión lineal, la diferencia esperada en \\(y\\) correspondiente a una diferencia fija en \\(x\\) es, por el contrario, constante. Por lo tanto, cuando interpretamos los resultados logísticos debemos elegir en qué parte de la curva queremos evaluar la probabilidad del resultado, dado el modelo. 4.2 Ejemplo de regresión logística Ilustremos estos conceptos utilizando el conjunto de datos Default del ISLR. Este conjunto de datos simulado contiene una variable binaria que representa el incumplimiento en los pagos de la tarjeta de crédito (variable default), que modelaremos como una función de la variable balance (la cantidad de deuda que tiene la tarjeta) y la variable income (ingresos). Los datos pueden obtenerse mediante library(ISLR) data(Default) str(Default) &#39;data.frame&#39;: 10000 obs. of 4 variables: $ default: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ student: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 2 1 1 ... $ balance: num 730 817 1074 529 786 ... $ income : num 44362 12106 31767 35704 38463 ... Antes de Primero visualizaremos cómo es esta relación. library(gridExtra) ggplot(Default, aes(x = balance, y = income, col = default)) + geom_point(alpha = .4) + ggtitle(&quot;Balance vs. Income by Default&quot;) grid.arrange( ggplot(Default, aes(default, balance)) + geom_boxplot() + ggtitle(&quot;Balance by Default&quot;) + ylab(&quot;balance&quot;), ggplot(Default, aes(default, income)) + geom_boxplot() + ggtitle(&quot;Income by Default&quot;) + ylab(&quot;income&quot;), ncol = 2) Claramente, los valores altos de saldo están asociados con el incumplimiento en todos los niveles de ingresos. Esto sugiere que los ingresos en realidad no son un fuerte predictor de incumplimiento, en comparación con el saldo, que es exactamente lo que vemos en los diagramas de cajas. Exploremos estas relaciones usando la regresión logística. En R ajustamos un modelo logístico usando la función glm () con family = binomial.2 Centraremos y escalaremos las variables para facilitar la interpretación. glm(default ~ balance + income, data = Default, family = binomial) %&gt;% standardize %&gt;% display glm(formula = default ~ z.balance + z.income, family = binomial, data = Default) coef.est coef.se (Intercept) -6.13 0.19 z.balance 5.46 0.22 z.income 0.56 0.13 --- n = 10000, k = 3 residual deviance = 1579.0, null deviance = 2920.6 (difference = 1341.7) NOTA: Se puede apreciar la ventaja del uso de tidyverse (pipe) - no se necesita crear las variables estandarizadas, ni guardar el resultado para luego hacer un print Interpretamos esta salida exactamente como lo haríamos para un modelo lineal con un predictor centrado y escalado: Intercept: -6,13 representa las probabilidades logarítmicas (log odds) de incumplimiento cuando el saldo es promedio (835.37) y el ingreso es promedio (3.351698^{4} ). (Promedio porque las variables se han centrado). z.balance: 5.46 representa el cambio predicho en las probabilidades logarítmicas de incumplimiento asociado con un aumento de 1 unidad en el saldo (z.balance), manteniendo constante el ingreso (z.income). Un aumento de 1 unidad en el saldo (z.balance) es equivalente a un aumento de 2 desviaciones estándar en el saldo (balance) (967.43). Este coeficiente es estadísticamente significativo ya que 5.46 - 2 x .22 &gt; 0 (el IC del 95% que no contiene 0 indica significación estadística). z.income: .56 representa el cambio predicho en las probabilidades logarítmicas (log odds) de incumplimiento asociadas con un aumento de 1 unidad en el ingreso (z.income), manteniendo constante el balance (z.balance). Un aumento de 1 unidad en el ingreso (z.income) es equivalente a un aumento de 2 desviaciones estándar en el ingreso (income) (2.667328^{4}). Este coeficiente también es estadísticamente significativo ya que .56 - 2 x .13 &gt; 0. ¿Qué significa que las probabilidades logarítmicas de incumplimiento aumenten en 5.46 o .56? En términos precisos, ¿quién sabe? Para que estas cantidades tengan una mejor interpretación, necesitamos transformarlas, ya sea en probabilidades (odds) o en razones de probabilidades (razón de odds -&gt; odds ratio). Sin embargo, debemos señalar que el signo y la magnitud de los coeficientes si son informativas: la relación con el incumplimiento del pago es positiva en ambos casos y, como ya se había visto de forma gráfica en los diagramas de cajas, el efecto del saldo (balance) es mucho mayor que el del ingreso (income). 4.3 Coeficientes de regresión logística como probabilidades Podemos dar una interpretación más específica de la regresión logística más allá del efecto y magnitud. Para ello, podemos usar la función logit inversa para convertir las probabilidades logarítmicas (log-odds) de incumplimiento de pago en las tarjetas (cuando el saldo y los ingresos son promedio) en una probabilidad: invlogit &lt;- function(x) exp(x)/(1 + exp(x)) invlogit(-6.13 + 5.46 * 0 + .56 * 0) [1] 0.002171854 La probabilidad de incumplimiento para aquellos con un saldo promedio de tarjeta de crédito de (835.37) y un ingreso promedio de (3.351698^{4}) es de hecho bastante bajo: solo 0.002. Asimismo, podemos calcular el cambio en la probabilidad de incumplimiento en el pago asociado con un aumento de 1 unidad en el saldo, manteniendo el ingreso constante en el promedio (z.ingreso=0). Esto equivaldría a aumentar el saldo en casi 1000$, de 835.37 a 1802.8. invlogit(-6.13 + 5.46 * 1) - invlogit(-6.13 + 5.46 * 0) [1] 0.336325 4.4 Coeficientes de regresión logística como razones de odds También podemos interpretar los coeficientes de regresión logística como razones de odds (OR).3 Si dos resultados tienen probabilidades \\((p, 1-p)\\), entonces \\(\\frac {p} {1-p}\\) se conoce como odds (probabilidades o momio) del resultado. Las odds son simplemente diferentes formas de representar la misma información: \\(\\text{odds} = \\frac{p}{1-p}\\) y \\(p = \\frac{\\text{odds}} {1+ \\text{odds}}\\). Por ejemplo, una odds de 1 es equivalente a una odds de .5  es decir, resultados igualmente probables para \\(p\\) y \\(1-p\\): \\(\\text{odds(p = .5)} = \\frac{.5}{1-.5} = 1\\) y \\(p(\\text{oods} = 1) = \\frac{\\text{1}}{1 + 1} = .5.\\) La razón de dos odds es una OR: \\[ \\frac{\\frac{p_2}{1-p_2}}{\\frac{p_1}{1-p_1}} \\] Una razón de odds se puede interpretar como un cambio en la probabilidad. Por ejemplo, un cambio en la probabilidad de \\(p_1 = .33\\) a \\(p_2 = .5\\) da como resultado un OR de 2, de la siguiente manera: \\[ \\frac{\\frac{.5}{.5}}{\\frac{.33}{.66}} = \\frac{1}{.5} = 2 \\] También podemos interpretar el OR como el aumento porcentual de las probabilidades de un evento. Aquí, un OR de 2 equivale a aumentar las probabilidades en un 100%, de 0,5 a 1. Recordemos que representamos el modelo logit de esta manera: \\[ \\text{log} \\left(\\frac{p}{1-p}\\right) = \\alpha + \\beta x \\] La parte izquierda de la ecuación, expresado como logaritmos de probabilidades, está en la misma escala que la derecha derecho: \\(\\pm \\infty\\). Por lo tanto, no hay no linealidad en esta relación, y aumentar \\(x\\) en 1 unidad tiene el mismo efecto que en la regresión lineal: cambia el resultado predicho en \\(\\beta\\). Entonces, pasar de \\(x\\) a \\(x + 1\\) equivale a sumar \\(\\beta\\) a ambos lados de la ecuación anterior. Centrándonos solo en el lado izquierdo, tenemos, después de exponenciar, las probabilidades originales multiplicadas por \\(e^\\beta\\): \\[ e^{\\text{log} \\left(\\frac{p}{1-p}\\right) + \\beta} = \\frac{p}{1-p} *e^ \\beta \\] (ya que \\(e^{a+b} = e^a*e^b\\)). Podemos pensar en \\(e^\\beta\\) como el cambio de la odds del resultado cuando \\(x\\) cambia en 1 unidad, que se puede representar, utilizando la formulación anterior, como una OR: \\[ \\frac{\\frac{p_2}{1-p_2}}{\\frac{p_1}{1-p_1}} = \\frac{\\frac{p_1}{1-p_1} * e^\\beta }{\\frac{p_1}{1-p_1}} = e^\\beta. \\] Por lo tanto, \\(e^\\beta\\) se puede interpretar como el cambio en las probabilidades asociadas con un aumento de 1 unidad en \\(x\\), expresado en términos porcentuales. En el caso de OR = \\(\\frac{1}{. 5} = 2\\), el porcentaje de aumento en las probabilidades del resultado es del 100%. Cuando la OR es \\(&gt;2\\) se suele expresar como x-veces más (OR=3.5, hay 3.5 veces más probabilidad de observar el evento que no observarlo cuando se cambia \\(x\\) en 1 unidad), y cuando la OR es \\(&lt;1\\) se suele hablar de protección a no tener el evento y el porcentaje se calcula como 1-OR. Apliquemos esta información a nuestro modelo anterior de aplicando la exponencial a los coeficientes de saldo e ingresos: exp(5.46) [1] 235.0974 exp(.56) [1] 1.750673 Podemos interpretar estos ORs como el porcentaje o cambio multiplicativo en las probabilidades asociadas con un aumento de 1 unidad en el predictor (mientras se mantienen constantes los demás), de 1 a 235 (un aumento de 23,400%) en el caso de balance, y de 1 a 1,75 (un aumento del 75%) para los ingresos. Por ejemplo, podemos decir que la probabilidad de incumplimiento es un 75% mayor cuando los ingresos aumentan en 1 unidad. Cuando las variables predictoras son categóricas (como en biomedicina: sexo, estadío tumoral, fumar, beber, ) la interpretación se hace más sencilla porque el cambio de 1 unidad en estas variables, supone el cambio de una categoría respecto a la basal (ya que se usan dummy variables). Así, por ejemplo, si nuestro outcomes tener cáncer de pulmón o no, y nuestro predictor es ser fumanor o no, si nuestros análisis nos dan una OR de 6 asociado a ser fumador, la interpretación sería: \"La odds (probabilidad, abusando de lenguaje - también riesgo si el outcome es poco frecuente) de sufrir cáncer de pulmón es 6 veces mayor en los fumadores que en los no fumadores. 4.5 Capacidad predictiva de un modelo de clasificación Podemos evaluar el rendimiento (es decir, la capacidad predictiva) del modelo logístico utilizando el AIC, así como mediante el uso de otras medidas como: la desviación (deviance) residual, la precisión, la sensibilidad, la especificidad y el área bajo la curva (AUC). Al igual que AIC, la deviance es una medida de error, por lo que una deviance más baja significa un mejor ajuste a los datos. Esperamos que la desviación disminuya en 1 para cada predictor, por lo que con un predictor informativo (e.g. variable imporante para el modleo), la deviance disminuirá en más de 1. Deviance = \\(-2ln(L)\\), donde \\(ln\\) es el logaritmo natural y \\(L\\) es la función de verosimilitud . Veámoslo con nuestro ejemplo: logistic_model1 &lt;- glm(default ~ balance, data = Default, family = binomial) logistic_model1$deviance [1] 1596.452 logistic_model2 &lt;- glm(default ~ balance + income, data = Default, family = binomial) logistic_model2$deviance [1] 1578.966 En este caso, la deviance se redujo en más de 1, lo que indica que los ingresos mejoran el ajuste del modelo. Podemos hacer una prueba formal de la diferencia usando, como para los modelos lineales, la prueba de razón de verosimilitud: lmtest::lrtest(logistic_model1, logistic_model2) Likelihood ratio test Model 1: default ~ balance Model 2: default ~ balance + income #Df LogLik Df Chisq Pr(&gt;Chisq) 1 2 -798.23 2 3 -789.48 1 17.485 2.895e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 También podemos traducir las probabilidades de un modelo logístico para el incumplimiento del pago en predicciones de clase asignando Sí (predeterminado) a probabilidades mayores o iguales a .5 y No (sin valor predeterminado) a probabilidades menores que .5, y luego contar el número de veces que el modelo asigna la clase predeterminada correcta. Si dividimos este número por el total de observaciones, habremos calculado la precisión. La precisión se utiliza a menudo como medida del rendimiento del modelo. preds &lt;- ifelse(fitted(logistic_model2) &gt;= .5, &quot;Yes&quot;, &quot;No&quot;) (length(which(preds == Default$default)) / nrow(Default))*100 [1] 97.37 El modelo es 97.37% preciso. Valores superirores al 50% mostrarían una mejora en la predicción ya que por azar, se espera que el modelo tenga una precisión del 50%. Una forma sencilla de obtener un buen modelo de clasificación sería asignar a todos la categoría más frecuente. Por ejemplo, en nuestros datos, la clase mayoritaria es No para la variable incimpliminto por un amplio margen (9667 a 333). La mayoría de las personas no incumplen. ¿Cuál es nuestra precisión, entonces, si simplemente predecimos No para todos los casos? La proporción de No en los datos es 96.67%, por lo que si siempre predijimos No esa sería nuestra precisión (9667 / (9667 + 333) = .9667). El modelo logístico, sorprendentemente, ofrece solo una ligera mejora. Sin embargo, al evaluar el rendimiento del clasificador, debemos reconocer que no todos los errores son iguales y que la precisión tiene limitaciones como métrica de rendimiento. En algunos casos, el modelo puede haber predicho el incumplimiento cuando no lo había. Esto se conoce como falso positivo. En otros casos, el modelo puede haber predicho que no hubo incumplimiento cuando hubo incumplimiento. Esto se conoce como falso negativo. En la clasificación, utilizamos lo que se conoce como matriz de confusión para resumir estos diferentes tipos de errores del modelo, denominados así porque la matriz resume cómo se confunde el modelo. Usaremos la función confusionMatrix () de la librería caret para calcular rápidamente estos valores: confusionMatrix(as.factor(preds), Default$default, positive = &quot;Yes&quot;) Confusion Matrix and Statistics Reference Prediction No Yes No 9629 225 Yes 38 108 Accuracy : 0.9737 95% CI : (0.9704, 0.9767) No Information Rate : 0.9667 P-Value [Acc &gt; NIR] : 3.067e-05 Kappa : 0.4396 Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 Sensitivity : 0.3243 Specificity : 0.9961 Pos Pred Value : 0.7397 Neg Pred Value : 0.9772 Prevalence : 0.0333 Detection Rate : 0.0108 Detection Prevalence : 0.0146 Balanced Accuracy : 0.6602 &#39;Positive&#39; Class : Yes Esta función produce una gran cantidad de resultados. Podemos ver que informa la misma precisión que calculamos anteriormente: .97. La matriz de confusión 2 x 2 está en la parte superior. Podemos caracterizar estos 4 valores en la matriz de la siguiente manera: 9629 negativos verdaderos (TN): cuando el modelo predice correctamente No 108 verdaderos positivos (TP): cuando el modelo predice correctamente Sí 225 falsos negativos (FN): cuando el modelo predice incorrectamente No 38 falsos positivos (FP): cuando el modelo predice incorrectamente Sí La siguiente tabla resume estas posibilidades: Reference Predicted No Yes No TN FN Yes FP TP Hay dos medidas clave, además de la precisión, para caracterizar el rendimiento del modelo. Mientras que la precisión mide el error general, la sensibilidad y la especificidad miden errores específicos de clase. Precisión = 1 - (FP + FN) / Total: 1 - (38 + 225) / 10000 [1] 0.9737 Sensibilidad (o la tasa de verdaderos positivos): TP / (TP + FN). En este caso, la sensibilidad mide la proporción de incumplimientos que se clasificaron correctamente como tales. 108 / (108 + 225) [1] 0.3243243 Especificidad (o la tasa de verdaderos negativos): TN / (TN + FP). En este caso, la especificidad mide la proporción de no incumplimientos que se clasificaron correctamente como tales. 9629 / (9629 + 38) [1] 0.9960691 ¿Por qué deberíamos considerar estos errores específicos de clase? Todos los modelos tienen errores, pero no todos los errores del modelo son igualmente importantes. Por ejemplo, un falso negativo  prediciendo incorrectamente que un prestatario no incurrirá en incumplimiento  puede ser un error costoso para un banco, si el incumplimiento se hubiera podido prevenir mediante la intervención. Pero, por otro lado, un falso positivo, que predice incorrectamente que un prestatario incurrirá en incumplimiento, puede desencadenar una advertencia innecesaria que irrita a los clientes. Los errores que comete un modelo se pueden controlar ajustando el umbral de decisión utilizado para asignar probabilidades predichas a las clases. Usamos un umbral de probabilidad de .5 para clasificar los incumplimientos en los pagos. Si el umbral se establece en .1, por el contrario, la precisión general disminuiría, pero también lo haría el número de falsos negativos, lo que podría ser deseable. El modelo luego atraparía a más morosos, lo que ahorraría dinero al banco, pero eso tendría un costo: más falsos positivos (clientes potencialmente irritados). preds &lt;- as.factor(ifelse(fitted(logistic_model2) &gt;= .1, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, Default$default)$table Reference Prediction No Yes No 9105 90 Yes 562 243 La pregunta de cómo establecer el umbral de decisión . 5, .1 o algo más  debe responderse con referencia al contexto empresarial. Una curva de característica operativa del receptor (ROC por sus siglas en ingles) visualiza las compensaciones entre los tipos de errores trazando la especificidad frente a la sensibilidad. El cálculo del área bajo la curva ROC (AUC) nos permite, además, resumir el rendimiento del modelo y comparar modelos. La curva en sí muestra los tipos de errores que cometería el modelo en diferentes umbrales de decisión. Para crear una curva ROC usamos la función roc () del paquete pROC, y mostramos los valores de sensibilidad / especificidad asociados con los umbrales de decisión de .1 y .5: library(pROC) library(plotROC) invisible(plot(roc(factor(ifelse(Default$default == &quot;Yes&quot;, 1, 0)), fitted(logistic_model2)), print.thres = c(.1, .5), col = &quot;red&quot;, print.auc = T)) Un modelo con una precisión del 50%, es decir, un clasificador aleatorio, tendría una curva ROC que siguiera la línea de referencia diagonal. Un modelo con una precisión del 100%, un clasificador perfecto, tendría una curva ROC siguiendo los márgenes del triángulo superior. Cada punto de la curva ROC representa un par de sensibilidad / especificidad correspondiente a un umbral de decisión particular. Cuando establecimos el umbral de decisión en .1, la sensibilidad fue .73 (243 / (243 + 90)) y la especificidad fue .94 (9105 / (9105 + 562)). Ese punto se muestra en la curva. Del mismo modo, cuando establecimos el umbral de decisión en .5, la sensibilidad fue .32 y la especificidad fue .996. Ese punto también está en la curva. ¿Qué umbral de decisión es óptimo? Nuevamente, depende del problema (pensad en cáncer o en este ejemplo de dinero). Las curvas ROC nos permiten elegir los errores específicos de clase que podemos cometer. El AUC es el resumen de cómo funciona el modelo en cada umbral de decisión. En general, los modelos con AUC más altos son mejores. Esta medida nos servirá para comparar métodos de aprendizaje automático que iremos aprendiendo durante el curso. 4.6 Ejemplo de regresión logística: modelización de riesgo diabetes Para este ejemplo usaremos el conjunto de datos Pima, incluido en la librería MASS que contienen esta información: Una población de mujeres que tenían al menos 21 años, de ascendencia indígena Pima y que vivían cerca de Phoenix, Arizona, se sometieron a pruebas de diabetes de acuerdo con los criterios de la Organización Mundial de la Salud. Los datos fueron recopilados por el Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales de EE. UU. Usaremos información para 532 mujeres con datos completos después de eliminar los datos (principalmente faltantes) sobre la insulina sérica. El conjunto de datos incluye las siguientes variables: npreg: número de embarazos glu: concentración de glucosa en plasma a 2 horas en una prueba de tolerancia oral a la glucosa bp: presión arterial diastólica (mm Hg) piel: grosor del pliegue cutáneo del tríceps (mm) bmi: índice de masa corporal (peso en kg / (altura en m) ^ 2) ped: función del pedigrí de la diabetes age: Edad (años) type: Sí o No (diabetes) La variable resultado es type, que indica si una persona tiene diabetes. Los datos están divididos en un conjunto de entrenamiento y otro test que combinaremos para ilustrar este ejemplo. library(arm) library(MASS) data(&quot;Pima.tr&quot;) data(&quot;Pima.te&quot;) d &lt;- rbind(Pima.te, Pima.tr) str(d) &#39;data.frame&#39;: 532 obs. of 8 variables: $ npreg: int 6 1 1 3 2 5 0 1 3 9 ... $ glu : int 148 85 89 78 197 166 118 103 126 119 ... $ bp : int 72 66 66 50 70 72 84 30 88 80 ... $ skin : int 35 29 23 32 45 19 47 38 41 35 ... $ bmi : num 33.6 26.6 28.1 31 30.5 25.8 45.8 43.3 39.3 29 ... $ ped : num 0.627 0.351 0.167 0.248 0.158 0.587 0.551 0.183 0.704 0.263 ... $ age : int 50 31 21 26 53 51 31 33 27 29 ... $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 2 2 1 1 2 ... Todos los predictores son enteros o numéricos. Nuestro objetivo es construir un modelo logístico de diabetes para ilustrar cómo interpretar los coeficientes del modelo. 4.6.1 Modelo simple Comencemos con un modelo simple. bin_model1 &lt;- glm(type ~ bmi + age, data = d, family = binomial) display(bin_model1) glm(formula = type ~ bmi + age, family = binomial, data = d) coef.est coef.se (Intercept) -6.26 0.67 bmi 0.10 0.02 age 0.06 0.01 --- n = 532, k = 3 residual deviance = 577.2, null deviance = 676.8 (difference = 99.6) Intercept: -6.26 es el logaritmo de la probabilidad de tener diabetes cuando bmi = 0 y edad = 0. Dado que ni la edad ni el bmi pueden ser iguales a 0, el intercept no es interpretable en este modelo. Por tanto, tendría sentido centrar las variables para facilitar la interpretación. bmi: .1 es el cambio previsto en el log-odds de diabetes asociado con un aumento de 1 unidad en el bmi, manteniendo la edad constante. Este coeficiente es estadísticamente significativo ya que .1 - 2 x .02 &gt; 0. (Un IC del 95% que no contiene 0 indica significancia estadística) y también porque su p-valor asociacio mediante el test de score es \\(&lt;0.05\\) (usar la función summary () . Podemos traducir este coeficiente en un OR mediante la exponencial: \\(e^.1\\) = 1.11. Un aumento de 1 unidad en el IMC, manteniendo la edad constante, se asocia con un aumento del 11% en la odds (o, más coloquialmente, la probabilidad) de diabetes. edad: .06 es el cambio predicho en el log-oods de diabetes asociado con un aumento de 1 unidad en la edad, manteniendo constante el bmi. Este coeficiente también es estadísticamente significativo ya que .06 - 2 x .01&gt; 0. El OR para la edad es \\(e^.06\\) = 1.06 lo que indica un aumento del 6% en la probabilidad de sufrir diabetes asociada con un aumento de 1 unidad en la edad. 4.6.2 Agregar predictores y evaluar el ajuste Ahora ajustaremos un modelo completo (excluyendo skin, ya que parece medir casi lo mismo que bmi). ¿Mejora el ajuste? bin_model2 &lt;- glm(type ~ bmi + age + ped + glu + npreg + bp , data = d, family = binomial) display(bin_model2) glm(formula = type ~ bmi + age + ped + glu + npreg + bp, family = binomial, data = d) coef.est coef.se (Intercept) -9.59 0.99 bmi 0.09 0.02 age 0.03 0.01 ped 1.31 0.36 glu 0.04 0.00 npreg 0.12 0.04 bp -0.01 0.01 --- n = 532, k = 7 residual deviance = 466.5, null deviance = 676.8 (difference = 210.3) La deviance disminuye de 577 en el modelo anterior a 466.5 en este modelo, muy por encima de los 4 puntos que debería bajar simplemente al incluir 4 predictores adicionales. Además, el LRT nos indica que estas diferencias son estadísticamente significativas: lmtest::lrtest(bin_model1, bin_model2) Likelihood ratio test Model 1: type ~ bmi + age Model 2: type ~ bmi + age + ped + glu + npreg + bp #Df LogLik Df Chisq Pr(&gt;Chisq) 1 3 -288.60 2 7 -233.27 4 110.67 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El segundo modelo es mucho mejor que el primero, lo que también es evidente cuando observamos las matrices de confusión (con el umbral de decisión establecido en .5) preds &lt;- as.factor(ifelse(fitted(bin_model1) &gt; .5, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, d$type)$table Reference Prediction No Yes No 312 112 Yes 43 65 preds &lt;- as.factor(ifelse(fitted(bin_model2) &gt; .5, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, d$type)$table Reference Prediction No Yes No 318 73 Yes 37 104 Como era de esperar, el modelo completo comete menos errores. Podemos formalizar esta impresión calculando y comparando la precisión del modelo: 1 - (112 + 43) / (112 + 43 + 312 + 65) = 0.71 para el primer modelo, en comparación con 1 - (73 + 37) / (73 + 37 + 318 + 104) =r round (1 - (73 + 37) / (73 + 37 + 318 + 104 ), 2) para el segundo modelo más grande. Los números de verdaderos negativos son cercanos, pero el modelo más grande aumenta sustancialmente el número de verdaderos positivos y reduce el número de falsos negativos, prediciendo incorrectamente que una persona no tiene diabetes (aunque esta sigue siendo la clase de error más grande). Deberíamos comprobar para ver que estos modelos mejoran la precisión sobre un modelo que siempre predice la clase mayoritaria. En este conjunto de datos, No es la categoría mayoritaria con 66,7%. Entonces, si siempre predijimos No, estaríamos en lo correcto el 66.7% de las veces, que es una precisión menor que cualquiera de los modelos. Las curvas ROC proporcionan una descripción más sistemática del rendimiento del modelo en términos de errores de clasificación errónea. invisible(plot(roc(d$type, fitted(bin_model1)), col = &quot;red&quot;, main = &quot;ROC curves: logistic model 1 (red) vs. logistic model 2 (blue)&quot;)) invisible(plot(roc(d$type, fitted(bin_model2)), print.auc = T, col = &quot;blue&quot;, add = T)) El modelo más grande es claramente mejor: el AUC es más alto. 4.6.3 Análisis de interacciones Agreguemos una interacción entre dos predictores continuos, ped y glu. Centraremos y escalaremos las entradas para que el coeficiente de la interacción sea interpretable. bin_model3 &lt;- standardize(glm(type ~ bmi + ped * glu + age + npreg + bp , data = d, family = binomial)) display(bin_model3) glm(formula = type ~ z.bmi + z.ped * z.glu + z.age + z.npreg + z.bp, family = binomial, data = d) coef.est coef.se (Intercept) -1.02 0.13 z.bmi 1.29 0.26 z.ped 1.02 0.24 z.glu 2.31 0.27 z.age 0.53 0.30 z.npreg 0.87 0.29 z.bp -0.19 0.26 z.ped:z.glu -1.15 0.41 --- n = 532, k = 8 residual deviance = 460.0, null deviance = 676.8 (difference = 216.7) La interacción mejora el modelo pero no cambia la imagen general del ajuste del modelo en el gráfico de residuos agrupados (no incluido). Intercept : -1.02 es el log-odds de diabetes cuando todos los predictores son promedio (ya que hemos centrado y escalado las entradas). La probabilidad de tener diabetes para la mujer promedio en este conjunto de datos es logit\\(^{- 1}\\) (- 1.02) = 0.27. z.bmi: 1.29 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.bmi, manteniendo constantes los demás predictores. \\(e^{1.29}\\) = 3.63 por lo que un aumento de 1 unidad en z.bmi, manteniendo constantes los otros predictores, se asocia con un aumento del 263% en la probabilidad de sufrir diabetes. z.ped: 1.02 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.ped, cuando z.glu = 0 y manteniendo constantes los otros predictores. \\(e^{1.02}\\) = 2.77 por lo que un aumento de 1 unidad en z.ped, cuando z.glu = 0 y manteniendo los otros predictores constantes, se asocia con un aumento del 177% en la probabilidad de sufrir diabetes. z.glu: 2.31 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.glu, cuando z.ped = 0 y manteniendo constantes los demás predictores. \\(e^{2.31}\\) = 10.07 por lo que un aumento de 1 unidad en z.glu, cuando z.ped = 0 y manteniendo los otros predictores constantes, se asocia con un aumento del 907% en la probabilidad de sufrir diabetes. . el resto de predictores igual z.ped:z.glu : se añade -1.15 al log-odds de diabetes de z.ped cuando z.glu aumenta en 1 unidad, manteniendo constantes los otros predictores. O, alternativamente, se añade -1.15 al log-odds de diabetes de z.glu por cada unidad adicional de z.ped. Calculamos el OR, como en los otros casos, exponenciando: \\(e^ {- 1.15}\\) = 0.32. El OR para z.ped disminuye en un 68% (1 - .32 = .68) cuando z.glu aumenta en 1 unidad, manteniendo constantes los demás predictores. O, alternativamente, el OR para z.glu disminuye en un 68% con cada unidad adicional de z.ped. 4.6.4 Gráfico de la interacción Como siempre, debemos visualizar la interacción para comprenderla. Esto es especialmente necesario cuando las relaciones se expresan en términos de probabilidades logarítmicas y razones de probabilidades. Como hemos hecho anteriormente para fines de visualización, dicotomizaremos los predictores en la interacción y, en este caso, para facilitar la interpretación, presentaremos las relaciones en términos de probabilidades. El propósito de los gráficos es la comprensión y la ilustración, por lo que no nos preocupa demasiado la precisión estadística. Resumiremos las relaciones usando una curva loess (estimación no paramétrica de la regresión) para capturar la no linealidad del efecto del predictor (\\(\\pm \\infty\\)) al rango del resultado binario (0, 1). d$ped_bin &lt;- ifelse(d$ped &gt; mean(d$ped), &quot;above avg&quot;,&quot;below avg&quot;) d$glu_bin &lt;- ifelse(d$glu &gt; mean(d$glu), &quot;above avg&quot;,&quot;below avg&quot;) d$prob &lt;- fitted(bin_model3) d$type_num &lt;- ifelse(d$type == &quot;Yes&quot;, 1, 0) ggplot(d, aes(glu, type_num)) + geom_point() + stat_smooth(aes(glu, prob, col = ped_bin), se = F) + labs(y = &quot;Pr(diabetes)&quot;, title = &quot;Diabetes ~ glu varying by ped_bin&quot;) La relación entre glu y diabetes depende claramente de los niveles de ped. Como en el caso lineal, las líneas no paralelas indican una interacción. El coeficiente de log-odds negativo para la interacción del modelo indica que a medida que ped aumenta, la fuerza de la relación entre glu y type (diabetes) disminuye. Esto es exactamente lo que vemos en este gráfico: ggplot(d, aes(ped, type_num)) + geom_point() + stat_smooth(aes(ped, prob, col = glu_bin), se = F) + labs(y = &quot;Pr(diabetes)&quot;, title = &quot;Diabetes ~ ped varying by glu_bin&quot;) La interacción es más difícil de ver aquí porque la escala de ped está comprimida, con la mayoría de las observaciones cercanas a 0. Sin embargo, podemos ver que a medida que glu aumenta, la fuerza de la relación entre ped y diabetes disminuye. Nuevamente, las líneas no paralelas indican la presencia de una interacción. 4.6.5 Uso del modelo para predecir probabilidades El tamaño del efecto más grande en el modelo anterior con la interacción, con mucho, es z.glu. Por tanto, para comunicar los resultados de este modelo deberíamos concentrarnos en glu. Pero los coeficientes expresados como logaritmos de probabilidades son algo confusos y, lamentablemente, las razones de probabilidades no ayudan a aclarar mucho las cosas. Deberíamos ir al trabajo adicional de traducir los coeficientes del modelo en probabilidades, pero para hacerlo debemos identificar en qué parte de la curva de probabilidad nos gustaría evaluar glu. Escogeremos la región cercana al promedio de z.glu  0  y examinaremos el efecto de aumentar z.glu en 1 unidad (que es igual a 2 desviaciones estándar de glu) cuando los otros predictores son promedio. La forma más sencilla de hacer esto es crear una base de datos con los valores de predicción deseados para usar con la función predict (). basal &lt;- data.frame(z.bmi = 0, z.ped = 0, z.glu = 0, z.age = 0, z.npreg = 0, z.bp = 0) glucosa &lt;- data.frame(z.bmi = 0, z.ped = 0, z.glu = 1, z.age = 0, z.npreg = 0, z.bp = 0) (lo &lt;- as.numeric(invlogit(predict(bin_model3, newdata = basal)))) [1] 0.2652028 (hi &lt;- as.numeric(invlogit(predict(bin_model3, newdata = )))) [1] 0.716146591 0.025911688 0.017983270 0.029013744 0.893018807 0.665405710 0.395033985 0.211066662 0.431695033 0.225375587 0.032840692 [12] 0.443676401 0.022607120 0.418795847 0.194085484 0.411388311 0.122928759 0.771984610 0.760725696 0.088470567 0.953485181 0.785645203 [23] 0.028409690 0.026232726 0.055457646 0.038181372 0.757850071 0.007630220 0.110507313 0.066603557 0.213559028 0.017048355 0.294579371 [34] 0.213686248 0.245378175 0.147939308 0.058529536 0.047092679 0.169073996 0.023421517 0.434375748 0.123923484 0.796680824 0.257623355 [45] 0.236877492 0.032937565 0.075796893 0.459769771 0.014969537 0.214416331 0.409898562 0.024597682 0.736084480 0.071421984 0.863089942 [56] 0.109813203 0.415148971 0.192888308 0.671705524 0.707181753 0.222687051 0.074378276 0.047288218 0.250211485 0.102873418 0.451885105 [67] 0.016412055 0.839500999 0.593725195 0.887724350 0.023636511 0.972427462 0.421875342 0.240818110 0.193220347 0.275433738 0.029285367 [78] 0.858427986 0.792694816 0.428056807 0.241389504 0.359146427 0.159848949 0.126229936 0.089106804 0.965646716 0.021565093 0.504101659 [89] 0.255700597 0.303575110 0.316087819 0.695623695 0.059157940 0.023850831 0.703188586 0.768676985 0.198583192 0.736445668 0.019832290 [100] 0.911158959 0.794437117 0.023391174 0.082335119 0.536540158 0.491636172 0.875915114 0.830550408 0.453101254 0.050024142 0.017749687 [111] 0.151803493 0.175667620 0.900329614 0.125898408 0.410586667 0.586540636 0.832688763 0.035561910 0.252327286 0.110864806 0.054988088 [122] 0.131662542 0.393097130 0.492496368 0.513182966 0.032671230 0.036078371 0.218212971 0.686096496 0.281975363 0.145884637 0.351411775 [133] 0.239547073 0.725330877 0.392954359 0.354585338 0.328110094 0.183872054 0.649535926 0.189835213 0.750660386 0.100154335 0.253081898 [144] 0.306902095 0.138876568 0.112315917 0.602945913 0.033225614 0.021023766 0.126008933 0.074805807 0.593242488 0.276438109 0.030025810 [155] 0.388107478 0.814468072 0.819988723 0.320406177 0.512019175 0.189866084 0.019102564 0.778045038 0.063532324 0.021660839 0.087123658 [166] 0.371697775 0.017831740 0.174573799 0.072586081 0.050795966 0.321374939 0.390300727 0.242283766 0.101281785 0.477098938 0.177832066 [177] 0.851704388 0.421578801 0.059594070 0.801660505 0.263016503 0.158261796 0.629712769 0.680147379 0.627289909 0.143229332 0.371021820 [188] 0.048405335 0.155912833 0.844592024 0.774738072 0.074975116 0.094313006 0.034029316 0.156403796 0.765185930 0.162409946 0.941491860 [199] 0.098203334 0.096782855 0.013696456 0.086137707 0.665930779 0.328426313 0.230933621 0.045364772 0.067599667 0.128835144 0.552086895 [210] 0.192049128 0.391808734 0.634609576 0.138796509 0.034537822 0.402157076 0.473285370 0.915578006 0.109882027 0.060813059 0.078347861 [221] 0.511021110 0.046236210 0.637413042 0.057352160 0.060578365 0.328120324 0.117022441 0.150080125 0.033383541 0.548421342 0.727231653 [232] 0.273012605 0.006665104 0.023959522 0.212622505 0.263913260 0.304803089 0.282446484 0.528587177 0.053303093 0.042787259 0.895978588 [243] 0.951451693 0.260631793 0.057999466 0.077448786 0.039445750 0.078320973 0.521759602 0.120793953 0.109749728 0.115212481 0.063160685 [254] 0.315382778 0.186348273 0.242141093 0.800887574 0.866564275 0.147824356 0.483279794 0.458255040 0.058388003 0.052876258 0.244883311 [265] 0.761115562 0.021394588 0.531937078 0.751371443 0.307274782 0.801133302 0.005240345 0.656336098 0.109766806 0.101435208 0.023548802 [276] 0.056804667 0.081720211 0.513341731 0.014456472 0.127221754 0.744369844 0.498650317 0.182496315 0.605638441 0.021059011 0.094691510 [287] 0.633177077 0.189201907 0.229657632 0.818999683 0.081933947 0.793780642 0.798845854 0.137588419 0.275762056 0.057395933 0.445742740 [298] 0.584089416 0.070998635 0.209775098 0.174014948 0.294145785 0.743857124 0.126042513 0.893349297 0.712229694 0.217225821 0.135596074 [309] 0.343239883 0.187767754 0.233598292 0.822021476 0.082764167 0.102210424 0.113711375 0.122461316 0.828791325 0.107099346 0.054386091 [320] 0.931938668 0.326454054 0.678618259 0.863704140 0.217542759 0.056249096 0.816489720 0.491806299 0.084796292 0.943486215 0.302404878 [331] 0.132747361 0.037305147 0.053261653 0.902996173 0.042527808 0.787771463 0.031525411 0.220902831 0.054775507 0.642266770 0.520264613 [342] 0.707148463 0.902568015 0.695527581 0.848650675 0.466017086 0.291730904 0.027601848 0.133315806 0.771592655 0.383831136 0.090423918 [353] 0.020481869 0.115239515 0.038125985 0.017056834 0.070184592 0.755542001 0.050653582 0.481007350 0.195086963 0.187977925 0.169394579 [364] 0.377212811 0.151859655 0.475032680 0.186324471 0.612248541 0.023804839 0.007054577 0.085213924 0.171343641 0.900710787 0.150595032 [375] 0.257766676 0.013926514 0.035750449 0.185105131 0.253500436 0.369424730 0.640522481 0.841160836 0.060643162 0.051633327 0.731126069 [386] 0.154040714 0.063202801 0.337255163 0.087712142 0.059196122 0.446522479 0.839581017 0.899946965 0.051162921 0.569809764 0.085461942 [397] 0.080315826 0.343678259 0.504516436 0.092545675 0.175360179 0.310167781 0.720652085 0.094860697 0.905656358 0.432122047 0.742494468 [408] 0.704660222 0.065240504 0.044936864 0.086384200 0.460776518 0.058306508 0.538740077 0.360529087 0.413937339 0.248787673 0.063168325 [419] 0.205549345 0.143823024 0.121451790 0.204643255 0.028047843 0.262047997 0.797845942 0.020192814 0.067782381 0.880235920 0.067820578 [430] 0.093376849 0.145058897 0.885115279 0.023833728 0.172919691 0.023186441 0.343149266 0.491733925 0.334336420 0.182785009 0.107602309 [441] 0.159470782 0.480057756 0.718226410 0.110307283 0.282105476 0.504892387 0.023457816 0.246497165 0.288819782 0.406440777 0.333499471 [452] 0.789228958 0.024672867 0.158285199 0.915532293 0.083660243 0.616972917 0.156244026 0.066104438 0.117576505 0.582609228 0.935907656 [463] 0.494078320 0.906231057 0.044752423 0.098120327 0.817658808 0.063067848 0.048103513 0.094446457 0.009656172 0.457728456 0.371287692 [474] 0.732871561 0.239173153 0.048538537 0.427026826 0.838680152 0.005916331 0.639747396 0.192937289 0.044098532 0.307301900 0.771646053 [485] 0.893235289 0.633989608 0.325705459 0.861507987 0.924405484 0.211878609 0.232857992 0.553168378 0.199630964 0.167297598 0.636698113 [496] 0.102745817 0.412260537 0.037681077 0.499347951 0.561847838 0.108622049 0.036902563 0.275679034 0.142289294 0.929821372 0.853111736 [507] 0.052948764 0.186298288 0.034369934 0.468239290 0.021331580 0.617803456 0.083865824 0.751272138 0.115022996 0.496356896 0.458485256 [518] 0.303064449 0.512265744 0.935001789 0.011659745 0.243551235 0.234201771 0.733582682 0.439827183 0.132566837 0.200183137 0.254534757 [529] 0.620459940 0.187218540 0.128750723 0.801532793 round(hi - lo, 2) [1] 0.45 -0.24 -0.25 -0.24 0.63 0.40 0.13 -0.05 0.17 -0.04 -0.23 0.18 -0.24 0.15 -0.07 0.15 -0.14 0.51 0.50 -0.18 0.69 0.52 [23] -0.24 -0.24 -0.21 -0.23 0.49 -0.26 -0.15 -0.20 -0.05 -0.25 0.03 -0.05 -0.02 -0.12 -0.21 -0.22 -0.10 -0.24 0.17 -0.14 0.53 -0.01 [45] -0.03 -0.23 -0.19 0.19 -0.25 -0.05 0.14 -0.24 0.47 -0.19 0.60 -0.16 0.15 -0.07 0.41 0.44 -0.04 -0.19 -0.22 -0.01 -0.16 0.19 [67] -0.25 0.57 0.33 0.62 -0.24 0.71 0.16 -0.02 -0.07 0.01 -0.24 0.59 0.53 0.16 -0.02 0.09 -0.11 -0.14 -0.18 0.70 -0.24 0.24 [89] -0.01 0.04 0.05 0.43 -0.21 -0.24 0.44 0.50 -0.07 0.47 -0.25 0.65 0.53 -0.24 -0.18 0.27 0.23 0.61 0.57 0.19 -0.22 -0.25 [111] -0.11 -0.09 0.64 -0.14 0.15 0.32 0.57 -0.23 -0.01 -0.15 -0.21 -0.13 0.13 0.23 0.25 -0.23 -0.23 -0.05 0.42 0.02 -0.12 0.09 [133] -0.03 0.46 0.13 0.09 0.06 -0.08 0.38 -0.08 0.49 -0.17 -0.01 0.04 -0.13 -0.15 0.34 -0.23 -0.24 -0.14 -0.19 0.33 0.01 -0.24 [155] 0.12 0.55 0.55 0.06 0.25 -0.08 -0.25 0.51 -0.20 -0.24 -0.18 0.11 -0.25 -0.09 -0.19 -0.21 0.06 0.13 -0.02 -0.16 0.21 -0.09 [177] 0.59 0.16 -0.21 0.54 0.00 -0.11 0.36 0.41 0.36 -0.12 0.11 -0.22 -0.11 0.58 0.51 -0.19 -0.17 -0.23 -0.11 0.50 -0.10 0.68 [199] -0.17 -0.17 -0.25 -0.18 0.40 0.06 -0.03 -0.22 -0.20 -0.14 0.29 -0.07 0.13 0.37 -0.13 -0.23 0.14 0.21 0.65 -0.16 -0.20 -0.19 [221] 0.25 -0.22 0.37 -0.21 -0.20 0.06 -0.15 -0.12 -0.23 0.28 0.46 0.01 -0.26 -0.24 -0.05 0.00 0.04 0.02 0.26 -0.21 -0.22 0.63 [243] 0.69 0.00 -0.21 -0.19 -0.23 -0.19 0.26 -0.14 -0.16 -0.15 -0.20 0.05 -0.08 -0.02 0.54 0.60 -0.12 0.22 0.19 -0.21 -0.21 -0.02 [265] 0.50 -0.24 0.27 0.49 0.04 0.54 -0.26 0.39 -0.16 -0.16 -0.24 -0.21 -0.18 0.25 -0.25 -0.14 0.48 0.23 -0.08 0.34 -0.24 -0.17 [287] 0.37 -0.08 -0.04 0.55 -0.18 0.53 0.53 -0.13 0.01 -0.21 0.18 0.32 -0.19 -0.06 -0.09 0.03 0.48 -0.14 0.63 0.45 -0.05 -0.13 [309] 0.08 -0.08 -0.03 0.56 -0.18 -0.16 -0.15 -0.14 0.56 -0.16 -0.21 0.67 0.06 0.41 0.60 -0.05 -0.21 0.55 0.23 -0.18 0.68 0.04 [331] -0.13 -0.23 -0.21 0.64 -0.22 0.52 -0.23 -0.04 -0.21 0.38 0.26 0.44 0.64 0.43 0.58 0.20 0.03 -0.24 -0.13 0.51 0.12 -0.17 [353] -0.24 -0.15 -0.23 -0.25 -0.20 0.49 -0.21 0.22 -0.07 -0.08 -0.10 0.11 -0.11 0.21 -0.08 0.35 -0.24 -0.26 -0.18 -0.09 0.64 -0.11 [375] -0.01 -0.25 -0.23 -0.08 -0.01 0.10 0.38 0.58 -0.20 -0.21 0.47 -0.11 -0.20 0.07 -0.18 -0.21 0.18 0.57 0.63 -0.21 0.30 -0.18 [397] -0.18 0.08 0.24 -0.17 -0.09 0.04 0.46 -0.17 0.64 0.17 0.48 0.44 -0.20 -0.22 -0.18 0.20 -0.21 0.27 0.10 0.15 -0.02 -0.20 [419] -0.06 -0.12 -0.14 -0.06 -0.24 0.00 0.53 -0.25 -0.20 0.62 -0.20 -0.17 -0.12 0.62 -0.24 -0.09 -0.24 0.08 0.23 0.07 -0.08 -0.16 [441] -0.11 0.21 0.45 -0.15 0.02 0.24 -0.24 -0.02 0.02 0.14 0.07 0.52 -0.24 -0.11 0.65 -0.18 0.35 -0.11 -0.20 -0.15 0.32 0.67 [463] 0.23 0.64 -0.22 -0.17 0.55 -0.20 -0.22 -0.17 -0.26 0.19 0.11 0.47 -0.03 -0.22 0.16 0.57 -0.26 0.37 -0.07 -0.22 0.04 0.51 [485] 0.63 0.37 0.06 0.60 0.66 -0.05 -0.03 0.29 -0.07 -0.10 0.37 -0.16 0.15 -0.23 0.23 0.30 -0.16 -0.23 0.01 -0.12 0.66 0.59 [507] -0.21 -0.08 -0.23 0.20 -0.24 0.35 -0.18 0.49 -0.15 0.23 0.19 0.04 0.25 0.67 -0.25 -0.02 -0.03 0.47 0.17 -0.13 -0.07 -0.01 [529] 0.36 -0.08 -0.14 0.54 4.7 Creación de un modelo y validación La creación de un modelo predictivo mediante regresión logística se puede llevar a cabo mediante técnicas de stepwise que ya hemos visto en otros cursos. En R, podemos llevar a cabo dicha metodología usando la función step (que se basa en el test de razón de verosimilitud) o stepAIC (que usa el criterio de Akaike - AIC) de la librería MASS. Lo aprendido anteriormente en el tema de validación cruzada nos servirá oara evaluar la capacidad de un modelo. Veámoslo con un ejemplo. Supongamos que queremos elegir el mejor modelo para predecir el riesgo de diabetes y queremos validarlo con valización cruzada. Todos los pasos y métodos que hemos aprendido en las lecciones anteriores, podemos realizarlos de la siguiente manera. Usaremos los datos train y test que hay por defecto (Pima.tr y Pima.te). Para la validación cruzada usaremos la librería caret que veremos en detalle más adelante. Empecemos seleccionando el mejor modelo en los datos de entrenamiento con un stepwise hacia atrás mod.all &lt;- glm(type ~ ., data=Pima.tr, family=&quot;binomial&quot;) mod &lt;- stats::step(mod.all, trace=FALSE, direction=&quot;backward&quot;) summary(mod) Call: glm(formula = type ~ npreg + glu + bmi + ped + age, family = &quot;binomial&quot;, data = Pima.tr) Deviance Residuals: Min 1Q Median 3Q Max -2.0009 -0.6816 -0.3664 0.6467 2.2898 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -9.938059 1.541571 -6.447 1.14e-10 *** npreg 0.103142 0.064517 1.599 0.10989 glu 0.031809 0.006667 4.771 1.83e-06 *** bmi 0.079672 0.032649 2.440 0.01468 * ped 1.811417 0.661048 2.740 0.00614 ** age 0.039286 0.020967 1.874 0.06097 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 256.41 on 199 degrees of freedom Residual deviance: 178.47 on 194 degrees of freedom AIC: 190.47 Number of Fisher Scoring iterations: 5 Podemos evaluar la capacidad predictiva en la muestra test mediante preds &lt;- predict(mod, newdata = Pima.te, type=&quot;response&quot;) preds &lt;- as.factor(ifelse(preds &gt;= .5, &quot;Yes&quot;, &quot;No&quot;) ) confusionMatrix(preds, Pima.te$type) Confusion Matrix and Statistics Reference Prediction No Yes No 199 42 Yes 24 67 Accuracy : 0.8012 95% CI : (0.7542, 0.8428) No Information Rate : 0.6717 P-Value [Acc &gt; NIR] : 1.116e-07 Kappa : 0.5294 Mcnemar&#39;s Test P-Value : 0.03639 Sensitivity : 0.8924 Specificity : 0.6147 Pos Pred Value : 0.8257 Neg Pred Value : 0.7363 Prevalence : 0.6717 Detection Rate : 0.5994 Detection Prevalence : 0.7259 Balanced Accuracy : 0.7535 &#39;Positive&#39; Class : No y calcular la capacidad predictiva en la muestra train del modelo completo utilizando un método de 5-fold cross-validation mediante: library(caret) mod.train &lt;- train(type ~ ., data=Pima.tr, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.train Generalized Linear Model 200 samples 7 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 160, 160, 159, 160, 161 Resampling results: Accuracy Kappa 0.7553283 0.4381235 la capcidad predictiva para el modelo seleccionado mediante stepwise se calcularía mediante: mod.train.2 &lt;- train(formula(mod), data=Pima.tr, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.train.2 Generalized Linear Model 200 samples 5 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 160, 159, 160, 160, 161 Resampling results: Accuracy Kappa 0.7854503 0.5003265 Comprobamos como el modelo completo tiene mejor accuracy, pero ya hemos visto que esto es un problema de sobreajuste. Validemos los modelos con la muestra test: mod.test &lt;- train(type ~ ., data=Pima.te, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.test Generalized Linear Model 332 samples 7 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 266, 265, 266, 266, 265 Resampling results: Accuracy Kappa 0.7769787 0.4543523 mod.test.2 &lt;- train(formula(mod), data=Pima.te, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.test.2 Generalized Linear Model 332 samples 5 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 266, 266, 265, 265, 266 Resampling results: Accuracy Kappa 0.7949796 0.5130989 Vemos que en este caso el modelo obtenido mediante stepwise tiene una mejor capacidad predictiva en la muestra test, lo que demuestra que es un modelo más adecuado para hacer usar en la práctica. Notemos que la librería caret es muy flexible y nos permite mediante una única función train, estimar el modelo y evaluarlo mediante validación cruzada usando el argumento trControl. A lo largo del curso veremos que cambiando el argumento method podremos usar cualquier método de aprendizaje automático que existe. También veremos otros argumentos que nos permitirán llevar a cabo otros pasos necesarios a la hora de crear un modelo predictivo. 4.8 Nomogramas Una vez que ya tenemos creado y validad un modelo predictivo, nos puede interesar aplicarlo en otros individuos para poder tomar decisiones. Para ello, podemos usar nomogramas. Un nomograma es una representación gráfica que permite realizar con rapidez cálculos numéricos aproximados. Dentro del campo de la medicina, es frecuente que este tipo de gráficos este asociado al calculo de probabilidades de ocurrencia de un evento o una característica asociada a una enfermedad. Es lo que se conoce como Medicina Translacional. Aunque existen otro tipo de herramientas de cálculo vía web para estas probabilidades (Shiny), el uso de nomogramas esta muy extendido en el campo de la biomedicina como por ejemplo en el calculo de probabilidades de recurrencia en distintos tipos de cáncer, la probabilidad de supervivencia a un mes tras un infarto, o el pronóstico tras un diagnóstico de cáncer a cierto tiempo (en ese caso se usan modelos de supervivencia. Así pues, la regresión logística será una de las herramientas con una aplicación más directa y sencilla en el aprendizaje automático, donde el uso de los modelos predictivos suelen tener un aplicabilidad directa en la población. Existen numerosas herramientas para crear nomogramas en R, empecemos con la creación de nomogramas sencillos. Para ello usaremos los datos del ejemplo de diabetes con el modelo que hemos obtenido y validado anteriormente. Para usar la librería rms necesitamos que el modelo esté estimado con la función lrm () library(rms) t.data &lt;- datadist(Pima.tr) options(datadist = &#39;t.data&#39;) mod.lrm &lt;- lrm(type ~ npreg + glu + bmi + ped + age, data=Pima.tr) nom &lt;- nomogram(mod.lrm, fun = plogis, funlabel=&quot;Risk of diabetes&quot;) plot(nom) Supongamos que llega a la consulta una persona con un bmi de 35. Eso sumaría 30 puntos (basta con subir hacia arriba y ver qué valor de Points corresponde a un bmi de 35). Una edad de 50 años (~22 puntos), una función del pedigrí de la diabetes de 1.8 (~62 puntos), una glucosa de 120 (~50 puntos) y 0 embarazos que sumaría 0 puntos. En total, el paciente suma un total de 164 puntos. Si ahora vamos a la línea de Total Points y proyectamos sobre el predictor lineal de aproximadamente ~1.9 que se asocia con un riesgo de diabetes ligeramente superior al 80% (proyectar sobre Risk of diabetes). Obviamente estos cálculos se pueden hacer de forma más directa calculando la predicción sobre este individuo con el objeto de R indiv &lt;- data.frame(bmi=35, age=50, ped=1.8, glu=120, npreg=0) predict(mod.lrm, newdata = indiv, type=&quot;lp&quot;) 1 1.892376 predict(mod.lrm, newdata = indiv, type = &quot;fitted&quot;) 1 0.8690262 Estos cálculos se pueden programar en R y hace una función, o también una aplicación Shiny para aquellos médicos que no sepan usar R (los nomogramas se siguen utilizando). Otra opción es que hagamos uso de una librería para hacer nomogramas dinámicos (crea Shiny app) de forma sencilla con la librería DynNom library(DynNom) DynNom(mod, Pima.tr) Con estas simples instrucciones obtendríamos esta aplicación Shiny (Figura abajo) donde cada intervalo de confianza corresponde a un cálculo obtenido variando alguna de las variables predictoras Nomograma para el modelo de predicción para diabetes GLM significa modelo lineal generalizado. Esta función se ajustará a una variedad de modelos no lineales. Por ejemplo, podríamos especificar una regresión de Poisson con family = poisson. Para obtener más detalles, se puede consultar este texto [How to interpret odds ratio in logistic regression?] (Https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/). "],["caret.html", "5 La librería caret 5.1 Pre-procesado 5.2 Visualización 5.3 Ejemplo completo: creación de modelo diagnóstico para cáncer de mama 5.4 Creación de un modelo predictivo", " 5 La librería caret R es uno de los lenguajes de programación que más se usa en muchas disciplinas como la biomedicina, la economía y por supuesto, la estadísica. Al tratarse de un software libre, innumerables usuarios han podido implementar sus algoritmos, dando lugar a un número muy elevado de librerías donde encontrar prácticamente todas las técnicas de machine learning existentes. Sin embargo, esto tiene un lado negativo, cada paquete tiene una sintaxis, estructura e implementación propia, lo que dificulta su aprendizaje. El paquete caret (Classification And REgression Training ), desarrollado por Max Kuhn, es una interfaz que unifica bajo un único marco cientos de funciones de distintos paquetes, facilitando en gran medida todas las etapas de de preprocesado, entrenamiento, optimización y validación de modelos predictivos. El paquete caret ofrece tal cantidad de posibilidades que, difícilmente, pueden ser mostradas con un único ejemplo. En este capítulo describiremos toda las capacidades de esta libería con un un ejemplo utilizando regresión logística con la que ya estamos familiarizados. El resto de métodos de aprendizaje automático que veremos en el curso también se llevarán a cabo con esta librería y algunas específicas de cada método. Otros proyectos similares a caret son: mlr3, H20 que veremos en este curso y que está orientado a Big Data y tidymodels que forma parte del mundo tydiverse. Hay muchas funciones de modelado diferentes en R. Algunas tienen una sintaxis diferente para el entrenamiento y / o predicción de modelos. El paquete comenzó como una forma de proporcionar una interfaz uniforme para las funciones mismas, así como una forma de estandarizar las tareas comunes (como el ajuste de parámetros y la importancia de las variables). La instalación de caret puede ser muy pesada ya que tiene muchas dependencias (todos las librerías que implementan los métodos de aprendizaje automático que se quieran usar). Es por ello que se recomienda instalar la versión mínima y luego ir instalando aquellas librerías que se requieran en función del método que se pretenda usar. install.packages(&quot;caret&quot;, dependencies = c(&quot;Depends&quot;, &quot;Suggests&quot;)) Luego la librería se carga de la forma usual library(caret) En este capítulo veremos como realizar: Preprocesamiento (con caret y con recipes) Visualización División de datos Selección de variables Ajuste de modelo mediante remuestreo Estimación de la importancia de variables Para ilustrar el uso de esta librería con datos reales usaremos la base de datos Titanic que está en la librería con su mismo nombre. Este conjunto de datos contiene información sobre los pasajeros del Titanic, el transatlántico británico que se hundió en abril de 1912 durante su viaje inaugural desde Southampton a Nueva York. Entre la información almacenada se encuentran la edad, género, características socio-económicas de los pasajeros y si sobrevivieron o no al naufragio (variable respuesta). Este ejemplo puede que no sea muy útil desde un punto de vista científico (en las prácticas trabajaremos con otrods que sí son útiles), pero tienen una serie de características que los hacen idóneos para ser utilizados como ejemplo introductorio al machine learning: Contiene suficientes observaciones para entrenar modelos que consigan un poder predictivo alto. Incluye tanto variables continuas como cualitativas, lo que permite mostrar diferentes análisis exploratorios. La variable respuesta es binaria. Aunque la mayoría de los algoritmos de clasificación mostrados en este capítulo se generalizan para múltiples clases, su interpretación suele ser más sencilla cuando solo hay dos. Contiene valores faltantes (missing data). La forma en que se manejan estos registros (eliminación o imputación) influye en gran medida en el modelo final. Necesitan realizar un pre-procesamiento de datos (eliminación y creación de variables). Además, se trata de unos datos cuyas variables pueden entenderse de forma sencilla. Es intuitivo comprender el impacto que puede tener la edad, el sexo, la localización del camarote en la supervivencia de los pasajeros. Este aspecto es muy importante a al hora de crear buenos modelos predictivos, ya que, comprender a fondo el problema que se pretende modelar es lo más importante para lograr buenos resultados. La librería titanic contiene un conjunto de datos de entrenamiento y otros de test facilitados en la plataforma Kaggle que contiene muchos conjuntos de datos para trabajar con problemas de aprendizaje automático. Los datos test sólo sirven para tener otro conjunto de datos en los que evaluar el modelo, pero no tienen la variable resultado por lo que no se pueden usar para crear el modelo predictivo (Kaggle los usa para ver quién proponer el mejor modelo). Los datos se cargan de forma habitual library(tidyverse) library(titanic) datos &lt;- titanic_train 5.1 Pre-procesado 5.1.1 Creación de variables El primer paso antes de realizar cualquier análisis estadístico, y en particular para entrenar un modelo predictivo, es llevar a cabo una exploración descriptiva de los datos. Este proceso permite entender mejor que información contiene cada variable, así como detectar posibles errores ya que podríamos encontrarnos con los siguientes problemas: Que una columna se haya almacenado con el tipo incorrecto: una variable numérica está siendo reconocida como texto. Que una variable contenga valores que no tienen sentido: para indicar que no se dispone de la altura de una persona se introduce el valor cero o un espacio en blanco. No existe nadie cuya altura sea cero. Que en una variable de tipo numérico se haya introducido una palabra en lugar de un número. Según la información disponible en Kaggle, nuestras variables contienen la siguiente información: PassengerId: identificador único del pasajero. Survived: si el pasajero sobrevivió al naufragio, codificada como 0 (no) y 1 (si). Esta es la variable respuesta que interesa predecir. Pclass: clase a la que pertenecía el pasajero: 1, 2 o 3. Name: nombre del pasajero. Sex: sexo del pasajero. Age: edad del pasajero. SibSp: número de hermanos, hermanas, hermanastros o hermanastras en el barco. Parch: número de padres e hijos en el barco. Ticket: identificador del billete. Fare: precio pagado por el billete. Cabin: identificador del camarote asignado al pasajero. Embarked: puerto en el que embarcó el pasajero. Por ello, empezaremos comprobando que cada variable se ha almacenado con el tipo de valor que le corresponde, es decir, que las variables numéricas sean números y las cualitativas factor, character o booleanas. Recordemos que en R, cuando la variable es cualitativa, conviene convertirla en variable factor para que las funciones que implementan muchos de los métodos estadísticos que queremos usar puedan analizarlas de forma conveniente (i.e. dummy variables). glimpse(datos) Rows: 891 Columns: 12 $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, ~ $ Survived &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0~ $ Pclass &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1~ $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;, &quot;Heikkine~ $ Sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;~ $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, NA, 31, NA, 35, 34, 15, 28~ $ SibSp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 3, 0, 0, 0~ $ Parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 2, 0, 0, 0~ $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;113803&quot;, &quot;373450&quot;, &quot;330877&quot;, &quot;17463&quot;, &quot;349909~ $ Fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625, 21.0750, 11.1333, 30.0708, 16.70~ $ Cabin &lt;chr&gt; &quot;&quot;, &quot;C85&quot;, &quot;&quot;, &quot;C123&quot;, &quot;&quot;, &quot;&quot;, &quot;E46&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;G6&quot;, &quot;C103&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;,~ $ Embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;~ Observamos que el único caso en el que el tipo de valor no se corresponde con la naturaleza de la variable es Survived. Aunque esta variable está codificada como 1 si el pasajero sobrevivió y 0 si murió, no conviene almacenarla en formato numérico, ya que esto puede llevar a errores como el de tratar de calcular su media. Para evitar este tipo de problemas, se recodifica la variable para que sus dos posibles niveles sean Si/No y se convierte a factor. Además, esta re-codificación también ayudará a que los resultados y gráficas se puedan leer fácilmente. datos &lt;- mutate(datos, Survived = as.factor(ifelse(Survived == 1, &quot;Si&quot;, &quot;No&quot;))) table(datos$Survived) No Si 549 342 La variable Pclass es cualitativa ordinal, es decir, toma distintos valores cualitativos ordenados siguiendo una escala establecida, aunque no es necesario que el intervalo entre mediciones sea uniforme. Por ejemplo, se asume que la diferencia entre primera y segunda clase es menor que la diferencia entre primera y tercera, sin embargo, las diferencias entre primera-segunda y segunda-tercera no tiene por qué ser iguales. Es por ello que es preferible guardar esta variable como factor datos &lt;- mutate(datos, Pclass = as.factor(Pclass)) table(datos$Pclass) 1 2 3 216 184 491 Las variables SibSp y Parch son cuantitativas discretas, pueden tomar únicamente determinados valores numéricos. En este caso, al tratarse de número de personas (familiares e hijos), solo pueden ser números enteros. No existe una norma clara sobre como almacenar estas variables. Para este estudio exploratorio, dado que solo toman unos pocos valores, se decide almacenarlas como factor. datos &lt;- mutate(datos, SibSp = as.factor(SibSp)) datos &lt;- mutate(datos, Parch = as.factor(Parch)) Las variables Sex y Embarked también se convierten a tipo factor. datos &lt;- mutate(datos, Sex = as.factor(Sex)) datos &lt;- mutate(datos, Embarked = as.factor(Embarked)) Las variables Cabin y Embarked contienen \"\". Esto consideraría a este valor como una categoría, por lo que habría que hacer datos$Cabin[datos$Cabin==&quot;&quot;] &lt;- NA datos$Embarked[datos$Embarked==&quot;&quot;] &lt;- NA 5.1.2 Predictores con poca variabilidad En algunas situaciones, el mecanismo de generación de datos puede crear predictores que solo tienen un valor único (es decir, un predictor de varianza cero). Para la mayoría de modelos de aprendizaje automático esto puede provocar que el modelo se bloquee o que el ajuste sea inestable. De manera similar, los predictores categóricos pueden tener alguna categoría que ocurren con frecuencias muy bajas. Por ejemplo, esto ocurre con la variable número de hijos en el barco table(datos$Parch) 0 1 2 3 4 5 6 678 118 80 5 4 5 1 La preocupación aquí es que estos predictores pueden convertirse en predictores de varianza cero cuando los datos se dividen en submuestras de validación cruzada / bootstrap o que algunas muestras pueden tener una influencia muy grande en el modelo (valores influyentes). Es posible que estos predictores de varianza cercana a cero deban identificarse y eliminarse antes del modelado. Para identificar estos tipos de predictores, se pueden calcular las siguientes dos métricas: la frecuencia del valor más prevalente sobre el segundo valor más frecuente (llamado índice de frecuencia), que estaría cerca de uno para los predictores con buen comportamiento y muy grande para datos altamente desequilibrados y el porcentaje de valores únicos que es el número de valores únicos dividido por el número total de muestras (multiplicado por 100) que se acerca a cero a medida que aumenta la granularidad de los datos. Si la relación de frecuencia es mayor que un umbral preestablecido y el porcentaje de valor único es menor que un umbral, podríamos considerar que un predictor tiene una varianza cercana a cero. No queremos identificar falsamente los datos que tienen baja granularidad pero están distribuidos de manera uniforme, como los datos de una distribución uniforme discreta. El uso de ambos criterios no debería detectar falsamente tales predictores. Para realizar estos cálculos podemos usar la función nearZeroVar () (el argumento saveMetrics se puede usar para mostrar los detalles y - por defecto el valor predeterminado es FALSE): nzv &lt;- nearZeroVar(datos, saveMetrics= TRUE) nzv freqRatio percentUnique zeroVar nzv PassengerId 1.000000 100.0000000 FALSE FALSE Survived 1.605263 0.2244669 FALSE FALSE Pclass 2.273148 0.3367003 FALSE FALSE Name 1.000000 100.0000000 FALSE FALSE Sex 1.837580 0.2244669 FALSE FALSE Age 1.111111 9.8765432 FALSE FALSE SibSp 2.909091 0.7856341 FALSE FALSE Parch 5.745763 0.7856341 FALSE FALSE Ticket 1.000000 76.4309764 FALSE FALSE Fare 1.023810 27.8338945 FALSE FALSE Cabin 1.000000 16.4983165 FALSE FALSE Embarked 3.833333 0.3367003 FALSE FALSE La función nearZeroVar devuelve las posiciones con las variables marcadas como problemáticas. Podríamos crear una base de datos sin variables problemáticas de las siguiente forma: nzv &lt;- nearZeroVar(datos) nzv integer(0) filteredDatos &lt;- dplyr::select(datos, -all_of(nzv)) dim(datos) [1] 891 12 dim(filteredDatos) [1] 891 12 En este caso no hemos filtrado ninguna porque ninguna ha devuelto TRUE en las columnas zeroVar y nzv del objeto nzv cuando indicamos saveMetrics= TRUE. 5.1.3 Identificación de predictores correlacionados Algunos de los modelos que vamos a estudiar pueden tratar con predictores correlacionados (como pls). Sin embargo, otros modelos pueden beneficiarse al reducir el nivel de correlación entre los predictores. Dada una matriz de correlación, la función findCorrelation() es útil para marcar los predictores que deben ser eliminados por su alta correlación con otros: descrCor &lt;- cor(filteredDatos) highCorr &lt;- sum(abs(descrCor[upper.tri(descrCor)]) &gt; .99) Dado que en nuestro caso la mayoría de variables son categóricas, no tiene mucho sentido llevar a cabo este pre-proceso. Además, la función cor() sólo puede aplicarse a datos continuos. Existen otras funciones para calcular las correlaciones entre variables categóricas, que deberían implementarse de forma manual. 5.1.4 Centrado y escalado En ocasiones es recomendable centrar y escalar las variables continuas para evitar que aquellas vairables con rangos mayores tengan más importancia. Esto se puede hacer de forma sencilla con la función preProcess de la siguiente manera datosTransf &lt;- preProcess(datos, method=c(&quot;center&quot;, &quot;scale&quot;)) datosTransf Created from 183 samples and 12 variables Pre-processing: - centered (3) - ignored (9) - scaled (3) Vemos que este escalado y centrado no se ha aplicado a 7 de las 12 variables ya que no variables categóricas o carácter. También podemos transformar nuestros datos para garantizar normalidad utilizando transformacines exponenciales (expoTrans), Box-Cox (BoxCox) o Yeo-Johnson (YeoJohnson). Esto se puede hacer de forma sencilla mediante preProc &lt;- preProcess(datos, method=c(&quot;center&quot;, &quot;scale&quot;, &quot;YeoJohnson&quot;)) preProc Created from 183 samples and 12 variables Pre-processing: - centered (3) - ignored (9) - scaled (3) - Yeo-Johnson transformation (3) Lambda estimates for Yeo-Johnson transformation: 0.71, 0.76, -0.1 Los valores Lambda corresponden a las diferentes transformaciones usadas para cada unos de las variables según la definición que se puede encontrar aquí. Este objeto tiene toda la información para centrar, escalar, transformar, e incluso eliminar o no variables. 5.1.5 Imputación En el capítulo de modelización ya hablamos de este tema. La librería caret facilita la imputación mediante la función preProcess(). Basta con usar en el argumento method cualquiera de estas approximaciones: knnImpute, bagImpute, medianImpute. NOTA: también se podría usar zv y nzv para eliminar aquellos predictores con poca variabilidad, así que todos los pasos se pueden hacer con la función preProcess() [ver ayuda de la función para más detalles] 5.1.6 Pre-procesado con la librería recipes El preprocesado de datos engloba aquellas transformaciones de los datos hechas con la finalidad de que puedan ser aceptados por el algoritmo de machine learning o que mejoren sus resultados. Todo preprocesado de datos debe aprenderse de las observaciones de entrenamiento y luego aplicarse al conjunto de entrenamiento y al de test. Esto es muy importante para no violar la condición de que ninguna información procedente de las observaciones de test puede participar o influir en el ajuste del modelo. Aunque no es posible crear un único listado, algunos pasos de preprocesado que más suelen aplicarse en la práctica son: Imputación de valores ausentes Exclusión de variables con varianza próxima a cero Reducción de dimensionalidad Estandarización de las variables numéricas Binarización de las variables cualitativas El paquete caret incorpora muchas funciones para preprocesar los datos. Sin embargo, para facilitar todavía más el aprendizaje de las transformaciones, únicamente con las observaciones de entrenamiento, y poder aplicarlas después a cualquier conjunto de datos, el mismo autor ha creado el paquete recipes. La idea detrás de este paquete es la siguiente: Definir cuál es la variable respuesta, los predictores y el set de datos de entrenamiento, recipe(). Definir todas las transformaciones (escalado, selección, filtrado) que se desea aplicar, step_(). Aprender los parámetros necesarios para dichas transformaciones con las observaciones de entrenamiento rep(). Aplicar las trasformaciones aprendidas a cualquier conjunto de datos bake(). En los siguientes apartados, se almacenan en un objeto recipe todos los pasos de preprocesado y, finalmente, se aplican a los datos. 5.1.6.1 Imputación de valores ausentes Para ver qué cantidad de datos faltantes hay podemos crear la siguiente figura: datos_long &lt;- datos %&gt;% gather(key = &quot;variable&quot;, value = &quot;valor&quot;, -PassengerId) datos_long %&gt;% group_by(variable) %&gt;% dplyr::summarize(porcentaje_NA = 100 * sum(is.na(valor)) / length(valor)) %&gt;% ggplot(aes(x = reorder(variable, desc(porcentaje_NA)), y = porcentaje_NA)) + geom_col() + labs(title = &quot;Porcentaje valores ausentes por variable&quot;, x = &quot;Variable&quot;, y = &quot;Porcentaje NAs&quot;) + theme_bw() Podemos observar que las variables Cabin, Age y Embarked contienen valores ausentes. La gran mayoría de algoritmos no aceptan observaciones incompletas, por lo que, cuando el set de datos contiene valores ausentes, se puede: Eliminar aquellas observaciones que estén incompletas. Eliminar aquellas variables que contengan valores ausentes. Tratar de estimar los valores ausentes empleando el resto de información disponible (imputación). Las primeras dos opciones, aunque sencillas, suponen perder información. La eliminación de observaciones solo puede aplicarse cuando se dispone de muchas y el porcentaje de registros incompletos es muy bajo. En el caso de eliminar variables, el impacto dependerá de cuanta información aporten dichas variables al modelo. Cuando se emplea imputación, es muy importante tener en cuenta el riesgo que se corre al introducir valores en predictores que tengan mucha influencia en el modelo. Supóngase un estudio médico en el que, cuando uno de los predictores es positivo, el modelo predice casi siempre que el paciente está sano. Para un paciente cuyo valor de este predictor se desconoce, el riesgo de que la imputación sea errónea es muy alto, por lo que es preferible obtener una predicción basada únicamente en la información disponible. Esta es otra muestra de la importancia que tiene que el analista conozca el problema al que se enfrenta y pueda así tomar la mejor decisión. En el conjunto de datos Titanic, si se eliminan las observaciones incompletas, se pasa de 891 observaciones a 714, por lo que esta no es una opción. nrow(datos) [1] 891 nrow(datos[complete.cases(datos),]) [1] 183 La variable Cabin está ausente para casi un 80% de las observaciones, con un porcentaje tan alto de valores ausentes, no es conveniente imputarla, se excluye directamente del modelo. Esto deja al modelo con dos variables que requieren de imputación: Age y Embarked. La imputación es un proceso complejo que debe de realizarse con detenimiento, identificando cuidadosamente qué variables son las adecuadas para cada imputación. La librería recipes permite 4 métodos de imputación distintos: step_bagimpute(): imputación vía Bagged Trees. step_knnimpute(): imputación vía K-Nearest Neighbors. step_meanimpute(): imputación vía media del predictor (predictores continuos). step_modeimpute(): imputación vía moda del predictor (predictores cualitativos). Como ya vimos en su momento, las librerías Hmisc, missForest y MICE también permiten aplicar otros métodos. Se imputa la variable Embarked con el valor C (más frecuene). Como no existe una función step() que haga sustituciones por valores concretos, se realiza una sustitución de forma externa al recipe. datos &lt;- datos %&gt;% mutate(Embarked = replace(Embarked, is.na(Embarked), &quot;C&quot;)) La variable Age se imputa con el método bagging empleando todos los otros predictores. library(recipes) # Se crea un objeto recipe() con la variable respuesta y los predictores. # Las variables *PassengerId*, *Name*, *Ticket* no parecen aportar información # relevante sobre la supervivencia de los pasajeros. Excluyendo todas estas # variables, se propone como modelo inicial el formado por los predictores: # Pclass + Sex + SibSp + Parch + Fare + Embarked + Age. objeto_recipe &lt;- recipe(formula = Survived ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Age, data = datos) objeto_recipe Data Recipe Inputs: role #variables outcome 1 predictor 7 objeto_recipe &lt;- objeto_recipe %&gt;% step_bagimpute(Age) objeto_recipe Data Recipe Inputs: role #variables outcome 1 predictor 7 Operations: Bagged tree imputation for Age NOTA: este objeto no es la imputación, es la recipe para realizarla. La idea es seguir haciendo pasos de pre-proceso y al final aplicarlos a nuestros datos 5.1.6.2 Variables con varianza próxima a cero No se deben incluir en el modelo predictores que contengan un único valor (cero-varianza) ya que no aportan información. Tampoco es conveniente incluir predictores que tengan una varianza próxima a cero, es decir, predictores que toman solo unos pocos valores, de los cuales, algunos aparecen con muy poca frecuencia. El problema con estos últimos es que pueden convertirse en predictores con varianza cero cuando se dividen las observaciones por validación cruzada o bootstrap. La función nearZeroVar() del paquete caret y step_nzv() del paquete recipe identifican como predictores potencialmente problemáticos aquellos que tienen un único valor (cero varianza) o que cumplen las dos siguientes condiciones: Ratio de frecuencias: ratio entre la frecuencia del valor más común y la frecuencia del segundo valor más común. Este ratio tiende a 1 si las frecuencias están equidistribuidas y a valores grandes cuando la frecuencia del valor mayoritario supera por mucho al resto (el denominador es un número decimal pequeño). Valor por defecto freqCut = 95/5. Porcentaje de valores únicos: número de valores únicos dividido entre el total de muestras (multiplicado por 100). Este porcentaje se aproxima a cero cuanto mayor es la variedad de valores. Valor por defecto uniqueCut = 10. datos %&gt;% dplyr::select(Pclass, Sex, SibSp, Parch, Fare, Embarked, Age) %&gt;% nearZeroVar(saveMetrics = TRUE) freqRatio percentUnique zeroVar nzv Pclass 2.273148 0.3367003 FALSE FALSE Sex 1.837580 0.2244669 FALSE FALSE SibSp 2.909091 0.7856341 FALSE FALSE Parch 5.745763 0.7856341 FALSE FALSE Fare 1.023810 27.8338945 FALSE FALSE Embarked 3.788235 0.3367003 FALSE FALSE Age 1.111111 9.8765432 FALSE FALSE Entre los predictores incluidos en el modelo, no se detecta ninguno con varianza cero o próxima a cero. Actualizamos nuestro objeto recipe objeto_recipe &lt;- objeto_recipe %&gt;% step_nzv(all_predictors()) Si bien la eliminación de predictores no informativos podría considerarse un paso propio del proceso de selección de predictores, dado que consiste en un filtrado por varianza, tiene que realizarse antes de estandarizar los datos, ya que después, todos los predictores tienen varianza 1. 5.1.6.3 Normalización de datos Como ya hemos mencionado anteriormente, también podemos transformar nuestros datos para garantizar normalidad utilizando transformaciones Box-Cox (BoxCox) o Yeo-Johnson (YeoJohnson). Esto se puede hacer de forma sencilla mediante objeto_recipe &lt;- objeto_recipe %&gt;% step_BoxCox(all_numeric()) ó objeto_recipe &lt;- objeto_recipe %&gt;% step_YeoJohnson(all_numeric()) respectivamente. 5.1.6.4 Estandarización y escalado Cuando los predictores son numéricos, la escala en la que se miden, así como la magnitud de su varianza pueden influir en gran medida en el modelo. Muchos algoritmos de machine learning (SVM, redes neuronales, lasso) son sensibles a esto, de forma que, si no se igualan de alguna forma los predictores, aquellos que se midan en una escala mayor o que tengan más varianza, dominarán el modelo aunque no sean los que más relación tienen con la variable respuesta. Existen principalmente 2 estrategias para evitarlo: Centrado: consiste en restarle a cada valor la media del predictor al que pertenece. Si los datos están almacenados en un dataframe, el centrado se consigue restándole a cada valor la media de la columna en la que se encuentra. Como resultado de esta transformación, todos los predictores pasan a tener una media de cero, es decir, los valores se centran en torno al origen. Normalización (estandarización): consiste en transformar los datos de forma que todos los predictores estén aproximadamente en la misma escala (centrado + escalado). Con este código se normalizan todas las variables numéricas. objeto_recipe &lt;- objeto_recipe %&gt;% step_center(all_numeric()) objeto_recipe &lt;- objeto_recipe %&gt;% step_scale(all_numeric()) Nunca se debe estandarizar las variables después de ser binarizadas (ver siguiente sección). 5.1.6.5 Binarización de variables cuantitativas La binarización consiste en crear nuevas variables dummy con cada uno de los niveles de las variables cualitativas. Por ejemplo, una variable llamada color que contenga los niveles rojo, verde y azul, se convertirá en tres nuevas variables (color_rojo, color_verde, color_azul), todas con el valor 0 excepto la que coincide con la observación, que toma el valor 1. Por defecto, la función step_dummy(all_nominal()) binariza todas las variables almacenadas como tipo factor o character. Además, elimina uno de los niveles para evitar redundancias. Volviendo al ejemplo anterior, no es necesario almacenar las tres variables, ya que, si color_rojo y color_verde toman el valor 0, la variable color_azul toma necesariamente el valor 1. Si color_rojo o color_verde toman el valor 1, entonces color_azul es necesariamente 0. objeto_recipe &lt;- objeto_recipe %&gt;% step_dummy(all_nominal(), -all_outcomes()) Finalmente, esto es lo que hemos preparado para pre-procesar nuestros datos: objeto_recipe Data Recipe Inputs: role #variables outcome 1 predictor 7 Operations: Bagged tree imputation for Age Sparse, unbalanced variable filter on all_predictors() Centering for all_numeric() Scaling for all_numeric() Dummy variables from all_nominal(), -all_outcomes() 5.1.6.6 Obtención de datos para entrenar Una vez que se ha creado el objeto recipe con todas las transformaciones de preprocesado, se aprende con los datos de entrenamiento y se aplican a los dos conjuntos de datos. IMPORTANTE Para nosotros el objeto datos es nuestro train, y aplicamos estas recipe a estos datos y luego también tenemos que aplicarlo a los datos train para que las variables con las que hagamos predicciones estén en las mismas escalas, sin valores faltantes, cero-varianza, etc. Paso 1: Se entrena el objeto recipe trained_recipe &lt;- prep(objeto_recipe, training = datos) trained_recipe Data Recipe Inputs: role #variables outcome 1 predictor 7 Training data contained 891 data points and 177 incomplete rows. Operations: Bagged tree imputation for Age [trained] Sparse, unbalanced variable filter removed no terms [trained] Centering for Fare, Age [trained] Scaling for Fare, Age [trained] Dummy variables from Pclass, Sex, SibSp, Parch, Embarked [trained] Paso 2: Se aplican las transformaciones al conjunto de entrenamiento y de test (en nuestro caso sólo tenemos train) datos_train_transf &lt;- bake(trained_recipe, new_data = datos) # datos_test_prep &lt;- bake(trained_recipe, new_data = datos_test) glimpse(datos_train_transf) Rows: 891 Columns: 21 $ Fare &lt;dbl&gt; -0.50216314, 0.78640362, -0.48857985, 0.42049407, -0.48606443, -0.47784805, 0.39559138, -0.2~ $ Age &lt;dbl&gt; -0.56594935, 0.62664093, -0.26780178, 0.40303025, 0.40303025, -0.08762746, 1.81923121, -2.05~ $ Survived &lt;fct&gt; No, Si, Si, Si, No, No, No, No, Si, Si, Si, Si, No, No, No, Si, No, Si, No, Si, No, Si, Si, ~ $ Pclass_X2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ $ Pclass_X3 &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,~ $ Sex_male &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,~ $ SibSp_X1 &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,~ $ SibSp_X2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ $ SibSp_X3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,~ $ SibSp_X4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ $ SibSp_X5 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ $ SibSp_X8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ $ Parch_X1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,~ $ Parch_X2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,~ $ Parch_X3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ $ Parch_X4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ $ Parch_X5 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,~ $ Parch_X6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ $ Embarked_C &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,~ $ Embarked_Q &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,~ $ Embarked_S &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,~ 5.2 Visualización Podemos empezar con unos gráficos para las variables continuas de la siguiente forma: library(caret) library(AppliedPredictiveModeling) transparentTheme(trans = .4) featurePlot(x = select(datos_train_transf, c(&quot;Age&quot;, &quot;Fare&quot;)), y = datos_train_transf$Survived, plot = &quot;pairs&quot;, ## Add a key at the top auto.key = list(columns = 2)) ó transparentTheme(trans = .9) featurePlot(x = select(datos_train_transf, c(&quot;Age&quot;, &quot;Fare&quot;)), y = datos_train_transf$Survived, plot = &quot;density&quot;, ## Pass in options to xyplot() to ## make it prettier scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;)), adjust = 1.5, pch = &quot;|&quot;, layout = c(2, 1), auto.key = list(columns = 2)) ó featurePlot(x = select(datos_train_transf, c(&quot;Age&quot;, &quot;Fare&quot;)), y = datos_train_transf$Survived, plot = &quot;box&quot;, ## Pass in options to bwplot() scales = list(y = list(relation=&quot;free&quot;), x = list(rot = 90)), layout = c(2,1), auto.key = list(columns = 2)) 5.3 Ejemplo completo: creación de modelo diagnóstico para cáncer de mama Para illustrar el uso de esta librería con datos reales, usaremos una base de datos que contiene datos de un estudio sobre diagnóstico del cáncer de mama por imagen. Mediante una punción con aguja fina se extrae una muestra del tejido sospechoso de la paciente. La muestra se tiñe para resaltar los núcleos de las células y se determinan los límites exactos de los núcleos. Las variables consideradas corresponden a distintos aspectos de la forma del núcleo. El fichero breast.csv accesible en el Moodle contiene 30 variables explicativas medidas en pacientes cuyos tumores fueron diagnosticados posteriormente como benignos o malignos (variable diagnosis considerada como la variable resultado). Hay 569 observaciones, de las que 357 corresponden a tumores benignos y 212 a tumores malignos. La primera variable (id) corresponde al identificador de paciente. Información adicional sobre estos datos se pueden encontrar aquí. breast &lt;- readr::read_delim(&quot;data/breast.csv&quot;, delim=&quot;,&quot;) Podemos hacer servir caret o recipes para llevar a cabo todos estos pasos. Lo haremos con recipes que controlamos un poco más qué estamos llevando a cabo en cada momento. Paso 1: Visualizar la información de la que disponemos y ver si las variables están en el formato que corresponde dplyr::glimpse(breast) Rows: 569 Columns: 32 $ id &lt;dbl&gt; 842302, 842517, 84300903, 84348301, 84358402, 843786, 844359, 84458202, 844981,~ $ diagnosis &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;,~ $ radius_mean &lt;dbl&gt; 17.990, 20.570, 19.690, 11.420, 20.290, 12.450, 18.250, 13.710, 13.000, 12.460,~ $ texture_mean &lt;dbl&gt; 10.38, 17.77, 21.25, 20.38, 14.34, 15.70, 19.98, 20.83, 21.82, 24.04, 23.24, 17~ $ perimeter_mean &lt;dbl&gt; 122.80, 132.90, 130.00, 77.58, 135.10, 82.57, 119.60, 90.20, 87.50, 83.97, 102.~ $ area_mean &lt;dbl&gt; 1001.0, 1326.0, 1203.0, 386.1, 1297.0, 477.1, 1040.0, 577.9, 519.8, 475.9, 797.~ $ smoothness_mean &lt;dbl&gt; 0.11840, 0.08474, 0.10960, 0.14250, 0.10030, 0.12780, 0.09463, 0.11890, 0.12730~ $ compactness_mean &lt;dbl&gt; 0.27760, 0.07864, 0.15990, 0.28390, 0.13280, 0.17000, 0.10900, 0.16450, 0.19320~ $ concavity_mean &lt;dbl&gt; 0.30010, 0.08690, 0.19740, 0.24140, 0.19800, 0.15780, 0.11270, 0.09366, 0.18590~ $ `concave points_mean` &lt;dbl&gt; 0.14710, 0.07017, 0.12790, 0.10520, 0.10430, 0.08089, 0.07400, 0.05985, 0.09353~ $ symmetry_mean &lt;dbl&gt; 0.2419, 0.1812, 0.2069, 0.2597, 0.1809, 0.2087, 0.1794, 0.2196, 0.2350, 0.2030,~ $ fractal_dimension_mean &lt;dbl&gt; 0.07871, 0.05667, 0.05999, 0.09744, 0.05883, 0.07613, 0.05742, 0.07451, 0.07389~ $ radius_se &lt;dbl&gt; 1.0950, 0.5435, 0.7456, 0.4956, 0.7572, 0.3345, 0.4467, 0.5835, 0.3063, 0.2976,~ $ texture_se &lt;dbl&gt; 0.9053, 0.7339, 0.7869, 1.1560, 0.7813, 0.8902, 0.7732, 1.3770, 1.0020, 1.5990,~ $ perimeter_se &lt;dbl&gt; 8.589, 3.398, 4.585, 3.445, 5.438, 2.217, 3.180, 3.856, 2.406, 2.039, 2.466, 3.~ $ area_se &lt;dbl&gt; 153.40, 74.08, 94.03, 27.23, 94.44, 27.19, 53.91, 50.96, 24.32, 23.94, 40.51, 5~ $ smoothness_se &lt;dbl&gt; 0.006399, 0.005225, 0.006150, 0.009110, 0.011490, 0.007510, 0.004314, 0.008805,~ $ compactness_se &lt;dbl&gt; 0.049040, 0.013080, 0.040060, 0.074580, 0.024610, 0.033450, 0.013820, 0.030290,~ $ concavity_se &lt;dbl&gt; 0.05373, 0.01860, 0.03832, 0.05661, 0.05688, 0.03672, 0.02254, 0.02488, 0.03553~ $ `concave points_se` &lt;dbl&gt; 0.015870, 0.013400, 0.020580, 0.018670, 0.018850, 0.011370, 0.010390, 0.014480,~ $ symmetry_se &lt;dbl&gt; 0.03003, 0.01389, 0.02250, 0.05963, 0.01756, 0.02165, 0.01369, 0.01486, 0.02143~ $ fractal_dimension_se &lt;dbl&gt; 0.006193, 0.003532, 0.004571, 0.009208, 0.005115, 0.005082, 0.002179, 0.005412,~ $ radius_worst &lt;dbl&gt; 25.38, 24.99, 23.57, 14.91, 22.54, 15.47, 22.88, 17.06, 15.49, 15.09, 19.19, 20~ $ texture_worst &lt;dbl&gt; 17.33, 23.41, 25.53, 26.50, 16.67, 23.75, 27.66, 28.14, 30.73, 40.68, 33.88, 27~ $ perimeter_worst &lt;dbl&gt; 184.60, 158.80, 152.50, 98.87, 152.20, 103.40, 153.20, 110.60, 106.20, 97.65, 1~ $ area_worst &lt;dbl&gt; 2019.0, 1956.0, 1709.0, 567.7, 1575.0, 741.6, 1606.0, 897.0, 739.3, 711.4, 1150~ $ smoothness_worst &lt;dbl&gt; 0.1622, 0.1238, 0.1444, 0.2098, 0.1374, 0.1791, 0.1442, 0.1654, 0.1703, 0.1853,~ $ compactness_worst &lt;dbl&gt; 0.6656, 0.1866, 0.4245, 0.8663, 0.2050, 0.5249, 0.2576, 0.3682, 0.5401, 1.0580,~ $ concavity_worst &lt;dbl&gt; 0.71190, 0.24160, 0.45040, 0.68690, 0.40000, 0.53550, 0.37840, 0.26780, 0.53900~ $ `concave points_worst` &lt;dbl&gt; 0.26540, 0.18600, 0.24300, 0.25750, 0.16250, 0.17410, 0.19320, 0.15560, 0.20600~ $ symmetry_worst &lt;dbl&gt; 0.4601, 0.2750, 0.3613, 0.6638, 0.2364, 0.3985, 0.3063, 0.3196, 0.4378, 0.4366,~ $ fractal_dimension_worst &lt;dbl&gt; 0.11890, 0.08902, 0.08758, 0.17300, 0.07678, 0.12440, 0.08368, 0.11510, 0.10720~ Vemos que todas las variables excepto nuestro resultado (diagnosis: tipo de tumor) son continuas. Como la variable resultado es character, es recomendable pasarla a factor breast &lt;- mutate(breast, diagnosis=as.factor(diagnosis)) Paso 2: Puesto que no tenemos datos de entrenamiento y test, lo crearemos nosotros library(caret) set.seed(123) train &lt;- createDataPartition(y = breast$diagnosis, p = 0.7, list = FALSE, times = 1) breast_train &lt;- breast[train, ] breast_test &lt;- breast[-train, ] Paso 3: Crear un objeto recipe con la variable respuesta y los predictores library(recipes) objeto_recipe &lt;- recipe(formula = diagnosis ~ . , data = breast_train) # debemos eliminar la variable id que irrelevante para predecir objeto_recipe &lt;- objeto_recipe %&gt;% step_rm(id) objeto_recipe Data Recipe Inputs: role #variables outcome 1 predictor 31 Operations: Delete terms id Paso 4: Veamos si tenemos datos faltantes any(is.na(breast)) [1] FALSE Paso 5: Puesto que no hay, podemos pasar al siguiente paso que sería ver si hay que eliminar variablaes con varianza próxima a cero. Empecemos viendo si hay alguna. Puesto que todas son continuas, esto nos facilitará la escritura en R no teniendo que usar la función select () para seleccionar las varaibles continuas breast_train %&gt;% nearZeroVar(saveMetrics = TRUE) freqRatio percentUnique zeroVar nzv id 1.000000 100.0000000 FALSE FALSE diagnosis 1.677852 0.5012531 FALSE FALSE radius_mean 1.000000 86.2155388 FALSE FALSE texture_mean 1.500000 88.9724311 FALSE FALSE perimeter_mean 1.000000 93.9849624 FALSE FALSE area_mean 1.000000 95.2380952 FALSE FALSE smoothness_mean 1.000000 85.7142857 FALSE FALSE compactness_mean 1.000000 96.4912281 FALSE FALSE concavity_mean 3.000000 95.7393484 FALSE FALSE concave points_mean 3.000000 96.7418546 FALSE FALSE symmetry_mean 1.333333 81.2030075 FALSE FALSE fractal_dimension_mean 1.500000 92.2305764 FALSE FALSE radius_se 1.000000 95.4887218 FALSE FALSE texture_se 1.000000 93.4837093 FALSE FALSE perimeter_se 2.000000 94.9874687 FALSE FALSE area_se 1.500000 95.4887218 FALSE FALSE smoothness_se 1.000000 97.2431078 FALSE FALSE compactness_se 1.500000 96.2406015 FALSE FALSE concavity_se 3.000000 95.2380952 FALSE FALSE concave points_se 2.000000 91.7293233 FALSE FALSE symmetry_se 1.000000 90.9774436 FALSE FALSE fractal_dimension_se 1.000000 97.2431078 FALSE FALSE radius_worst 1.333333 83.2080201 FALSE FALSE texture_worst 1.000000 92.2305764 FALSE FALSE perimeter_worst 1.500000 93.4837093 FALSE FALSE area_worst 1.000000 96.4912281 FALSE FALSE smoothness_worst 1.000000 78.9473684 FALSE FALSE compactness_worst 1.500000 95.7393484 FALSE FALSE concavity_worst 2.000000 96.7418546 FALSE FALSE concave points_worst 2.000000 89.9749373 FALSE FALSE symmetry_worst 1.500000 90.7268170 FALSE FALSE fractal_dimension_worst 1.000000 96.4912281 FALSE FALSE Vemos que tampoco hay ninguna variable que tenga que ser eliminada por este motivo. Este paso no sería necesario realizarlo, pero lo dejamos para que sirva como ejemplo para otros casos objeto_recipe &lt;- objeto_recipe %&gt;% step_nzv(all_predictors()) Paso 6: El siguiente paso sería eliminar aquellas variables con correlación elevada. El argumento threshold nos permite elegir el grado de correlación que por defecto es 0.9. objeto_recipe &lt;- objeto_recipe %&gt;% step_corr(all_predictors()) Paso 7: Ahora centraremos y normalizaremos los datos (esto último no tiene porqué sere necesario). Recordamos que también se pueden transformar los datos para garantizar normalidad usando preProcess() con el métodos Box-Cox o Yeo-Johnson de caret(). objeto_recipe &lt;- objeto_recipe %&gt;% step_center(all_numeric()) objeto_recipe &lt;- objeto_recipe %&gt;% step_scale(all_numeric()) Paso 8: La binarización de variables no es necesario Paso 9: Aprendizaje de las transformaciones de pre-procesado y aplicación a nuestros conjuntos de datos. Empezamos con el entrenamiento trained_recipe &lt;- prep(objeto_recipe, training = breast_train) trained_recipe Data Recipe Inputs: role #variables outcome 1 predictor 31 Training data contained 399 data points and no missing data. Operations: Variables removed id [trained] Sparse, unbalanced variable filter removed no terms [trained] Correlation filter removed perimeter_mean, perimeter_se, radius_worst, texture_worst, ... [trained] Centering for texture_mean, area_mean, smoothness_mean, compactness_mean, ... [trained] Scaling for texture_mean, area_mean, smoothness_mean, compactness_mean, ... [trained] y continuamos con la aplicación a nuestros datos test y train breast_train_prep &lt;- bake(trained_recipe, new_data = breast_train) breast_test_prep &lt;- bake(trained_recipe, new_data = breast_test) glimpse(breast_train_prep) Rows: 399 Columns: 20 $ texture_mean &lt;dbl&gt; 0.4958449, 0.2881680, -1.1536351, -0.8289907, 0.3955871, 0.6319091, 1.1618434, ~ $ area_mean &lt;dbl&gt; 1.57356630, -0.76566930, 1.84274018, -0.50508607, -0.21644003, -0.38281240, -0.~ $ smoothness_mean &lt;dbl&gt; 0.86440333, 3.11247741, 0.22892950, 2.10801878, 1.49987716, 2.07385352, 1.47937~ $ compactness_mean &lt;dbl&gt; 1.048709540, 3.387816481, 0.537501491, 1.239233573, 1.135482862, 1.676872936, 2~ $ symmetry_mean &lt;dbl&gt; 0.917687339, 2.799401020, -0.008914095, 0.981836669, 1.370296501, 1.919129657, ~ $ fractal_dimension_mean &lt;dbl&gt; -0.42639724, 4.98925612, -0.59414511, 1.90761198, 1.67334340, 1.58368505, 2.818~ $ texture_se &lt;dbl&gt; -0.80849714, -0.08080732, -0.81953767, -0.60483862, 0.35489967, -0.38442214, 0.~ $ area_se &lt;dbl&gt; 1.31855202, -0.28815366, 1.32841354, -0.28911575, 0.28261170, -0.35814637, -0.3~ $ smoothness_se &lt;dbl&gt; -0.30223259, 0.65056583, 1.41666726, 0.13553966, 0.55238896, -0.43710507, 0.019~ $ compactness_se &lt;dbl&gt; 0.85265446, 2.83595905, -0.03500649, 0.47288495, 0.29133100, 0.56308739, 2.6974~ $ concavity_se &lt;dbl&gt; 0.30900888, 1.07030125, 1.08153957, 0.24241140, -0.25040990, 0.19287953, 1.9369~ $ `concave points_se` &lt;dbl&gt; 1.630026033, 1.284095782, 1.316696538, -0.038046018, 0.525222612, 0.123146612, ~ $ symmetry_se &lt;dbl&gt; 0.24097383, 4.68326185, -0.35005507, 0.13927857, -0.67308705, 0.11295745, -0.31~ $ fractal_dimension_se &lt;dbl&gt; 0.392387187, 2.560158937, 0.646704166, 0.631276850, 0.785550017, 0.008106752, 2~ $ smoothness_worst &lt;dbl&gt; 0.47944015, 3.24454482, 0.18348094, 1.94655226, 1.36731780, 1.57448925, 2.20868~ $ compactness_worst &lt;dbl&gt; 1.05036897, 3.78488958, -0.30822603, 1.67179466, 0.70190019, 1.76587505, 4.9714~ $ `concave points_worst` &lt;dbl&gt; 1.96343178, 2.18407810, 0.73846430, 0.91498136, 0.63346709, 1.40040325, 1.62865~ $ symmetry_worst &lt;dbl&gt; 1.09719973, 5.76429394, -0.82980875, 1.67113661, 0.45383501, 2.27747315, 2.2589~ $ fractal_dimension_worst &lt;dbl&gt; 0.17926376, 4.82187666, -0.40772078, 2.18044624, 1.67498733, 1.24561901, 6.6969~ $ diagnosis &lt;fct&gt; M, M, M, M, M, M, M, M, M, M, M, M, B, B, M, M, M, M, M, M, M, M, M, M, M, M, B~ Paso 10: Visualización Hagamos algunos de los gráficos que hemos visto library(AppliedPredictiveModeling) transparentTheme(trans = .4) featurePlot(x = select(breast_train_prep, 1:6), y = breast_train_prep$diagnosis, plot = &quot;pairs&quot;, ## Add a key at the top auto.key = list(columns = 2)) 5.4 Creación de un modelo predictivo Una vez que ya hemos preprocesado nuestros datos, podemos pasar a la parte de creación de un modelo. La librería caret tiene varias funciones que intenta reproducir lo que hasta ahora hemos realizado más o menos de forma manual. La función train se puede usar para: evaluar, mediante remuestreo, el efecto de los parámetros de ajuste del modelo - elegir el modelo óptimo a través de estos parámetros estimar el rendimiento del modelo a partir de un conjunto de entrenamiento Primero, se debe elegir un modelo específico. Actualmente, hay 238 disponibles que pueden consultarse aquí. Estos modelos los iremos viendo de forma individual a lo largo del curso y estudiaremos los parámetros que potencialmente pueden optimizarse. También se pueden crear modelos definidos por el usuario. Algoritmo de entrenamiento El primer paso para ajustar el modelo (línea 1 en el algoritmo a continuación) es elegir un conjunto de parámetros para evaluar. Por ejemplo, si se ajusta a un modelo de mínimos cuadrados parciales (PLS), se debe especificar el número de componentes PLS a evaluar. Una vez que se han definido el modelo y los valores de los parámetros de ajuste, también se debe especificar el tipo de remuestreo. Actualmente, tiene implementado LOOCV, K-fold CV y Bootstrap. Después del remuestreo, se obtiene una medida de ajuste para cada remuestra que permite guiar al usuario sobre qué valores de parámetros de ajuste deben elegirse. De forma predeterminada, la función elige automáticamente los parámetros de ajuste asociados con el mejor valor, aunque se pueden utilizar diferentes métricas. Veamos cómo funcionaría en nuestro caso utilizando la regresión logística (dado que nuestro outcome es binario) como método de aprendizaje para la creación de un modelo predictivo. Partimos del hecho que ya hemos hecho un pre-procesado de datos tal y como hemos mostrado anteriormente y que nuestros datos de entrenamiento y validación se llaman breast_train_prep y breast_test_prep, respectivamente. Por defecto el método utiliza boostrap para evaluar la capacidad predictiva del modelo. La función trainControl() se puede utilizar para especificar el tipo de remuestreo. Podéis encontrar más información sobre esta función aquí: fitControl &lt;- trainControl(## 10-fold CV method = &quot;repeatedcv&quot;, number = 10, ## repeated ten times repeats = 10) Estamos haciendo una estimación de la capacidad predictiva del modelo con un 10-fold CV (argumento number) y lo repetimos 10 veces para garantizar aleatoriedad. Despues usamos la función train() set.seed(123) suppressMessages(fit &lt;- train(diagnosis ~ ., data = breast_train_prep, method = &quot;glm&quot;, trControl = fitControl)) fit Generalized Linear Model 399 samples 19 predictor 2 classes: &#39;B&#39;, &#39;M&#39; No pre-processing Resampling: Cross-Validated (10 fold, repeated 10 times) Summary of sample sizes: 360, 359, 359, 359, 359, 359, ... Resampling results: Accuracy Kappa 0.9506731 0.8948839 "],["árboles-de-decisión.html", "6 Árboles de decisión 6.1 Árboles de clasificación 6.2 Área bajo la curva ROC 6.3 Árboles de regresión 6.4 Bagged trees 6.5 Random Forest 6.6 Random Forest p&gt;&gt;n", " 6 Árboles de decisión Los árboles de decisión, también conocidos como modelos de árbol de clasificación y regresión (CART), son métodos basados en árboles para el aprendizaje automático supervisado. Los árboles de clasificación y de regresión simples son fáciles de usar e interpretar, pero no son competitivos con los mejores métodos de aprendizaje automático. Sin embargo, forman la base para el conjunto de modelos de ensamblaje como bagged trees, random forest y boosted trees, que aunque son menos interpretables, son muy precisos. Los modelos CART se puede definir en dos tipos de problemas Árboles de clasificación: la variable resultado es categórica y el métodos se utiliza para identificar la clase dentro de la cual es más probable que caiga nuestra variable resultado. Un ejemplo de un problema de tipo clasificación sería determinar quién se suscribirá o no a una plataforma digital; o quién se graduará o no de la escuela secundaria; o si una persona tiene cáncer o no. Árboles de regressión: la variable resultado es continua y el métodos se utiliza para predecir su valor. Un ejemplo de un problema de tipo regresión sería predecir los precios de venta de una casa residencial o el nivel de colesterol de una persona. Los modelos CART segmentan el espacio predictor en \\(K\\) nodos terminales no superpuestos (hojas). Cada nodo se describe mediante un conjunto de reglas que se pueden utilizar para predecir nuevas respuestas. El valor predicho \\(\\hat{y}\\) para cada nodo es la moda (clasificación) o la media (regresión). Los modelos CART definen los nodos a través de un proceso top-down greedy llamado división binaria recursiva (recursive binary splitting). El proceso es de arriba hacia abajo porque comienza en la parte superior del árbol con todas las observaciones en una sola región y divide sucesivamente el espacio de predicción. Es greedy porque en cada paso de división, la mejor división se realiza en ese paso en particular sin tener en cuenta las divisiones posteriores. La siguiente figura muestra la idea general de esta metodología: Diagrama árboles de decisión Como vemos en el ejemplo una de las ventajas de los modelos CART es que consideran interacciones. En este curso no vamos a ver la regresión lógica pero es una metodología muy interesante que extiende CART cuando las variables predictoras son binarias y las interacciones que buscamos son del tipo AND y OR. Esta metodología se ha empleado con éxito para analizar datos genéticos donde el interés radica en saber cuál es el riesgo de desarrolar una enfermeda si te tiene por ejemplo: \"una mutación en un punto A del genoma (SNP) y otra mutación en el punto B ó si se tiene una mutación en el punto C pero no se tiene en el punto D. También son interesantes porque permiten valores faltantes sin la necesidad de hacer imputaciones previas. La mejor división es la variable predictora y el punto de corte que minimiza una función de costo. La función de costo más común para los árboles de regresión es la suma de los residuos al cuadrado, \\[RSS = \\sum_{k=1}^K\\sum_{i \\in A_k}{\\left(y_i - \\hat{y}_{A_k} \\right)^2}.\\] Para árboles de clasificación, es el índice de Gini, \\[G = \\sum_{c=1}^C{\\hat{p}_{kc}(1 - \\hat{p}_{kc})},\\] y la entropía (aka información estadística) \\[D = - \\sum_{c=1}^C{\\hat{p}_{kc} \\log \\hat{p}_{kc}}\\] dónde \\(\\hat{p}_{kc}\\) es la proporción de observaciones de entrenamiento en el nodo \\(k\\) que son de clase \\(c\\). Un nodo completamente puro en un árbol binario tendría \\(\\hat{p} \\in \\{ 0, 1 \\}\\) y \\(G=D=0\\). Un nodo completamente impuro en un árbol binario tendría \\(\\hat{p}=0.5\\) y \\(G=0.5^2 \\cdot 2 = 0.25\\) y \\(D = -(0.5 \\log(0.5)) \\cdot 2 = 0.69\\). CART repite el proceso de división para cada nodo hijo hasta que se satisface un criterio de detención, generalmente cuando ningún tamaño de nodo supera un máximo predefinido o la división no mejora el modelo de manera significativa. CART también puede imponer un número mínimo de observaciones en cada nodo. Es probable que el árbol resultante esté sobre-entrenado (over-fitting) y, por lo tanto, no se generalice bien para los datos de prueba. Para evitar este problema CART poda el árbol, minimizando el error de predicción de validación cruzada. En este caso, el hiperparámetro que debermos seleccionar en este modelo es la profundidad del arbol (e.g. número de nodos). En lugar de realizar una validación cruzada de todos los subárboles posibles para encontrar el que tenga el mínimo de error, CART utiliza la poda de complejidad de costos (cost-complexity pruning). Costo-complejidad es la compensación entre error (costo) y tamaño del árbol (complejidad) donde la compensación se cuantifica con el parámetro costo-complejidad \\(c_p\\). El costo-complejidad del árbol, \\(R_{c_p}(T)\\), es la suma de su riesgo (error) más un factor de complejidad de costos \\(c_p\\) multiplicado pro el tamaño del arbol \\(|T|\\). \\[R_{c_p}(T) = R(T) + c_p|T|\\] \\(c_p\\) puede tomar cualquier valor de \\([0..\\infty]\\), pero resulta que hay un árbol óptimo para rangos de \\(c_p\\), por lo que solo hay un conjunto finito de valores interesantes para \\(c_p\\) (ver Therneau y Atkinson 2019. CART utiliza validación cruzada para determinar qué \\(c_p\\) es óptimo. 6.1 Árboles de clasificación Veamos cómo crear árboles de clasificación usando el conjunto de datos ISLR::OJ que se usaron para predecir qué marca de zumo de naranja, Citrus Hill (CH) o Minute Maid = (MM) toman los clientes (variable `Purchase) a partir de 17 variables predictoras. Vamos a introducir la librería skimr que es interesante para hacer descriptivas. Con ella podremos saber, por ejemplo, cuántos tipos de variables tenemos o ver qué distribuciones tienen las variables continuas library(tidyverse) library(caret) library(rpart) # classification and regression trees library(rpart.plot) # better formatted plots than the ones in rpart oj_dat &lt;- ISLR::OJ skimr::skim(oj_dat) Table 6.1: Data summary Name oj_dat Number of rows 1070 Number of columns 18 _______________________ Column type frequency: factor 2 numeric 16 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Purchase 0 1 FALSE 2 CH: 653, MM: 417 Store7 0 1 FALSE 2 No: 714, Yes: 356 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist WeekofPurchase 0 1 254.38 15.56 227.00 240.00 257.00 268.00 278.00  StoreID 0 1 3.96 2.31 1.00 2.00 3.00 7.00 7.00  PriceCH 0 1 1.87 0.10 1.69 1.79 1.86 1.99 2.09  PriceMM 0 1 2.09 0.13 1.69 1.99 2.09 2.18 2.29  DiscCH 0 1 0.05 0.12 0.00 0.00 0.00 0.00 0.50  DiscMM 0 1 0.12 0.21 0.00 0.00 0.00 0.23 0.80  SpecialCH 0 1 0.15 0.35 0.00 0.00 0.00 0.00 1.00  SpecialMM 0 1 0.16 0.37 0.00 0.00 0.00 0.00 1.00  LoyalCH 0 1 0.57 0.31 0.00 0.33 0.60 0.85 1.00  SalePriceMM 0 1 1.96 0.25 1.19 1.69 2.09 2.13 2.29  SalePriceCH 0 1 1.82 0.14 1.39 1.75 1.86 1.89 2.09  PriceDiff 0 1 0.15 0.27 -0.67 0.00 0.23 0.32 0.64  PctDiscMM 0 1 0.06 0.10 0.00 0.00 0.00 0.11 0.40  PctDiscCH 0 1 0.03 0.06 0.00 0.00 0.00 0.00 0.25  ListPriceDiff 0 1 0.22 0.11 0.00 0.14 0.24 0.30 0.44  STORE 0 1 1.63 1.43 0.00 0.00 2.00 3.00 4.00  Dividiremos nuestra base de datos oj_dat (n = 1070) en oj_train (80%, n = 857) para estimar varios modelos, y oj_test (20%, n = 213) para comparar su rendimiento con datos nuevos. set.seed(12345) partition &lt;- createDataPartition(y = oj_dat$Purchase, p = 0.8, list = FALSE) oj_train &lt;- oj_dat[partition, ] oj_test &lt;- oj_dat[-partition, ] La función rpart::rpart() construye un árbol completo, minimizando el índice de Gini \\(G\\) por defecto (parms = list (split = \"gini\")), hasta que se cumpla el criterio de parada. El criterio de parada predeterminado es: solo intenta una división si el nodo actual tiene al menos minsplit = 20 observaciones, y solo acepta una división si los nodos resultantes tienen al menos minbucket = round (minsplit / 3) observaciones, y el ajuste general resultante mejora en cp = 0.01 (es decir, \\(\\Delta G &lt;= 0.01\\)). # Usar method = &quot;class&quot; para clasificación y method = &quot;anova&quot; para regresión set.seed(123) oj_mdl_cart_full &lt;- rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) oj_mdl_cart_full n= 857 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 857 334 CH (0.61026838 0.38973162) 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) * 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) * 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) * 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) * La salida comienza con el nodo raíz. La clase predicha en la raíz es CH y esta predicción produce 334 errores en las 857 observaciones para una tasa de éxito (precisión) del 61% y una tasa de error del 39%. Los nodos secundarios del nodo x están etiquetados como 2x) y 2x + 1), por lo que los nodos secundarios de 1) son 2) y 3), y los nodos secundarios de 2) son 4) y 5). Los nodos terminales están etiquetados con un asterisco (*). Sorprendentemente, solo 3 de las 17 variables se utilizaron en el árbol completo: LoyalCH (lealtad de marca del cliente para CH), PriceDiff (precio relativo de MM sobre CH) y SalePriceMM (precio absoluto de MM). La primera división está en LoyalCH = 0.48285. Aquí hay un diagrama del árbol completo (sin podar). rpart.plot(oj_mdl_cart_full, yesno = TRUE) Las cajas muestran la clasificación del nodo (según la moda), la proporción de observaciones que no son CH y la proporción de observaciones incluidas en el nodo. rpart () no solo hizo crecer el árbol completo, sino que identificó el conjunto de parámetros de complejidad de costos y midió el rendimiento del modelo de cada árbol correspondiente mediante validación cruzada. printcp () muestra los posibles valores de \\(c_p\\). La siguiente tabla se puede utilizar esta tabla para decidir cómo podar el árbol. printcp(oj_mdl_cart_full) Classification tree: rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;) Variables actually used in tree construction: [1] LoyalCH PriceDiff SalePriceMM Root node error: 334/857 = 0.38973 n= 857 CP nsplit rel error xerror xstd 1 0.479042 0 1.00000 1.00000 0.042745 2 0.032934 1 0.52096 0.54192 0.035775 3 0.013473 3 0.45509 0.47006 0.033905 4 0.010000 5 0.42814 0.46407 0.033736 Hay 4 valores de \\(c_p\\) en este modelo. El modelo con el parámetro de complejidad más pequeño permite la mayoría de las divisiones (nsplit). El parámetro de mayor complejidad corresponde a un árbol con solo un nodo raíz. rel error es la tasa de error relativa al nodo raíz. El error absoluto del nodo raíz es 0.38973162 (la proporción de MM), por lo que su rel error es 0.38973162 / 0.38973162 = 1.0. Eso significa que el error absoluto del árbol completo (en CP = 0.01) es 0.42814 * 0.38973162 = 0.1669. Podemos verificarlo calculando la tasa de error de los valores predichos: pred &lt;- predict(oj_mdl_cart_full, newdata = oj_train, type = &quot;class&quot;) mean(oj_train$Purchase != pred) [1] 0.1668611 Para acaber de explicar toda la salida de la table CP, xerror es la tasa de error relativa con validación cruzada y xstd es su error estándar. Si se desea el error más bajo posible, deberíamos podar el árbol con el error de CV relativo más pequeño, \\(c_p=0.01\\). Si deseamos equilibrar el poder predictivo con la simplicidad, podaremos al árbol más pequeño dentro de 1 SE del que tiene el error relativo más pequeño. La tabla CP no es muy útil para encontrar ese árbol, así que añadiremos una columna para encontrarlo. oj_mdl_cart_full$cptable %&gt;% data.frame() %&gt;% mutate( min_idx = which.min(oj_mdl_cart_full$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = oj_mdl_cart_full$cptable[min_idx, &quot;xerror&quot;] + oj_mdl_cart_full$cptable[min_idx, &quot;xstd&quot;], eval = case_when(rownum == min_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;) ) %&gt;% dplyr::select(-rownum, -min_idx) CP nsplit rel.error xerror xstd xerror_cap eval 1 0.47904192 0 1.0000000 1.0000000 0.04274518 0.4978082 2 0.03293413 1 0.5209581 0.5419162 0.03577468 0.4978082 3 0.01347305 3 0.4550898 0.4700599 0.03390486 0.4978082 under cap 4 0.01000000 5 0.4281437 0.4640719 0.03373631 0.4978082 min xerror El árbol más simple que usa la regla 1-SE es \\(c_p = 0.01347305\\) (error CV = 0.18). Afortunadamente, plotcp () nos da una representación gráfica de la relación entre xerror y cp. plotcp(oj_mdl_cart_full, upper = &quot;splits&quot;) La línea discontinua se establece en el mínimo xerror + xstd. El eje superior muestra el número de divisiones en el árbol. NOTA: No estoy seguro de por qué los valores de CP no son los mismos que en la tabla (están cerca, pero no son los mismos). La figura sugiere que debería podar a 5 o 3 divisiones. Vemos que esta curva nunca llega al mínimo, sigue disminuyendo en 5 divisiones. El valor del parámetro de ajuste predeterminado cp = 0.01 puede ser demasiado grande, así que lo cambiaremos a cp = 0.001 y empezaremos de nuevo. set.seed(123) oj_mdl_cart_full &lt;- rpart( formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, cp = 0.001 ) print(oj_mdl_cart_full) n= 857 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 857 334 CH (0.61026838 0.38973162) 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) 20) ListPriceDiff&gt;=0.255 115 11 CH (0.90434783 0.09565217) * 21) ListPriceDiff&lt; 0.255 111 39 CH (0.64864865 0.35135135) 42) PriceMM&gt;=2.155 19 2 CH (0.89473684 0.10526316) * 43) PriceMM&lt; 2.155 92 37 CH (0.59782609 0.40217391) 86) DiscCH&gt;=0.115 7 0 CH (1.00000000 0.00000000) * 87) DiscCH&lt; 0.115 85 37 CH (0.56470588 0.43529412) 174) ListPriceDiff&gt;=0.215 45 15 CH (0.66666667 0.33333333) * 175) ListPriceDiff&lt; 0.215 40 18 MM (0.45000000 0.55000000) 350) LoyalCH&gt;=0.527571 28 13 CH (0.53571429 0.46428571) 700) WeekofPurchase&lt; 266.5 21 8 CH (0.61904762 0.38095238) * 701) WeekofPurchase&gt;=266.5 7 2 MM (0.28571429 0.71428571) * 351) LoyalCH&lt; 0.527571 12 3 MM (0.25000000 0.75000000) * 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) 24) LoyalCH&lt; 0.303104 7 0 CH (1.00000000 0.00000000) * 25) LoyalCH&gt;=0.303104 64 31 CH (0.51562500 0.48437500) 50) WeekofPurchase&gt;=246.5 52 22 CH (0.57692308 0.42307692) 100) PriceCH&lt; 1.94 35 11 CH (0.68571429 0.31428571) 200) StoreID&lt; 1.5 9 1 CH (0.88888889 0.11111111) * 201) StoreID&gt;=1.5 26 10 CH (0.61538462 0.38461538) 402) LoyalCH&lt; 0.410969 17 4 CH (0.76470588 0.23529412) * 403) LoyalCH&gt;=0.410969 9 3 MM (0.33333333 0.66666667) * 101) PriceCH&gt;=1.94 17 6 MM (0.35294118 0.64705882) * 51) WeekofPurchase&lt; 246.5 12 3 MM (0.25000000 0.75000000) * 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) 26) SpecialCH&gt;=0.5 14 6 CH (0.57142857 0.42857143) * 27) SpecialCH&lt; 0.5 61 10 MM (0.16393443 0.83606557) * 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) 14) LoyalCH&gt;=0.035047 117 21 MM (0.17948718 0.82051282) 28) WeekofPurchase&lt; 273.5 104 21 MM (0.20192308 0.79807692) 56) PriceCH&gt;=1.875 20 9 MM (0.45000000 0.55000000) 112) WeekofPurchase&gt;=252.5 12 5 CH (0.58333333 0.41666667) * 113) WeekofPurchase&lt; 252.5 8 2 MM (0.25000000 0.75000000) * 57) PriceCH&lt; 1.875 84 12 MM (0.14285714 0.85714286) * 29) WeekofPurchase&gt;=273.5 13 0 MM (0.00000000 1.00000000) * 15) LoyalCH&lt; 0.035047 57 1 MM (0.01754386 0.98245614) * Este es un árbol mucho más grande. ¿Encontramo un valor cp que produce un mínimo? plotcp(oj_mdl_cart_full, upper = &quot;splits&quot;) Sí, el mínimo está en CP = 0.011 con 5 divisiones. El mínimo + 1 SE está en CP = 0.021 con 3 divisiones. Podaremos entonces el árbol en 3. oj_mdl_cart &lt;- prune( oj_mdl_cart_full, cp = oj_mdl_cart_full$cptable[oj_mdl_cart_full$cptable[, 2] == 3, &quot;CP&quot;] ) rpart.plot(oj_mdl_cart, yesno = TRUE) El indicador de compra más importante parece ser LoyalCH. De la vignette de rpart (página 12) tenemos que: An overall measure of variable importance is the sum of the goodness of split measures for each split for which it was the primary variable, plus goodness (adjusted agreement) for all splits in which it was a surrogate. Surrogate (variable subrogada[^La FDA define una variable subrogada como una medida de laboratorio o signo físico que se usa en ensayos terapéuticos como sustituto de una variable clínicamente significativa que es una medida directa sobre lo que siente un paciente, sus funciones o su supervivencia y que se espera que prediga el efecto de la terapia]) se refieren a características alternativas para que un nodo maneje los datos faltantes. Para cada división, CART evalúa una variedad de divisiones alternativassustitutas\" para usar cuando el valor de la característica para la división principal es NA. Las divisiones sustitutas son divisiones que producen resultados similares a la división original. La importancia de una variable es la suma de la mejora en la medida general de Gini (o RMSE) producida por los nodos en los que aparece. En el siguiente gráfico podemos ver la importancia de cada variable para este modelo. oj_mdl_cart$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Importancia mediante clasificación simple&quot;) LoyalCH es, con mucho, la variable más importante, como se esperaba de su posición en la parte superior del árbol así como en el siguiente nivel abajo. Podemos ver cómo aparecen los variables subrogadas en el modelo con la función summary(). summary(oj_mdl_cart) Call: rpart(formula = Purchase ~ ., data = oj_train, method = &quot;class&quot;, cp = 0.001) n= 857 CP nsplit rel error xerror xstd 1 0.47904192 0 1.0000000 1.0000000 0.04274518 2 0.03293413 1 0.5209581 0.5419162 0.03577468 3 0.01347305 3 0.4550898 0.4700599 0.03390486 Variable importance LoyalCH PriceDiff SalePriceMM StoreID WeekofPurchase DiscMM PriceMM 67 9 5 4 4 3 3 PctDiscMM PriceCH 3 1 Node number 1: 857 observations, complexity param=0.4790419 predicted class=CH expected loss=0.3897316 P(node) =1 class counts: 523 334 probabilities: 0.610 0.390 left son=2 (537 obs) right son=3 (320 obs) Primary splits: LoyalCH &lt; 0.48285 to the right, improve=132.56800, (0 missing) StoreID &lt; 3.5 to the right, improve= 40.12097, (0 missing) PriceDiff &lt; 0.015 to the right, improve= 24.26552, (0 missing) ListPriceDiff &lt; 0.255 to the right, improve= 22.79117, (0 missing) SalePriceMM &lt; 1.84 to the right, improve= 20.16447, (0 missing) Surrogate splits: StoreID &lt; 3.5 to the right, agree=0.646, adj=0.053, (0 split) PriceMM &lt; 1.89 to the right, agree=0.638, adj=0.031, (0 split) WeekofPurchase &lt; 229.5 to the right, agree=0.632, adj=0.016, (0 split) DiscMM &lt; 0.77 to the left, agree=0.629, adj=0.006, (0 split) SalePriceMM &lt; 1.385 to the right, agree=0.629, adj=0.006, (0 split) Node number 2: 537 observations, complexity param=0.03293413 predicted class=CH expected loss=0.1750466 P(node) =0.6266044 class counts: 443 94 probabilities: 0.825 0.175 left son=4 (271 obs) right son=5 (266 obs) Primary splits: LoyalCH &lt; 0.7648795 to the right, improve=17.669310, (0 missing) PriceDiff &lt; 0.015 to the right, improve=15.475200, (0 missing) SalePriceMM &lt; 1.84 to the right, improve=13.951730, (0 missing) ListPriceDiff &lt; 0.255 to the right, improve=11.407560, (0 missing) DiscMM &lt; 0.15 to the left, improve= 7.795122, (0 missing) Surrogate splits: WeekofPurchase &lt; 257.5 to the right, agree=0.594, adj=0.180, (0 split) PriceCH &lt; 1.775 to the right, agree=0.590, adj=0.173, (0 split) StoreID &lt; 3.5 to the right, agree=0.587, adj=0.165, (0 split) PriceMM &lt; 2.04 to the right, agree=0.587, adj=0.165, (0 split) SalePriceMM &lt; 2.04 to the right, agree=0.587, adj=0.165, (0 split) Node number 3: 320 observations predicted class=MM expected loss=0.25 P(node) =0.3733956 class counts: 80 240 probabilities: 0.250 0.750 Node number 4: 271 observations predicted class=CH expected loss=0.04797048 P(node) =0.3162194 class counts: 258 13 probabilities: 0.952 0.048 Node number 5: 266 observations, complexity param=0.03293413 predicted class=CH expected loss=0.3045113 P(node) =0.3103851 class counts: 185 81 probabilities: 0.695 0.305 left son=10 (226 obs) right son=11 (40 obs) Primary splits: PriceDiff &lt; -0.165 to the right, improve=20.84307, (0 missing) ListPriceDiff &lt; 0.235 to the right, improve=20.82404, (0 missing) SalePriceMM &lt; 1.84 to the right, improve=16.80587, (0 missing) DiscMM &lt; 0.15 to the left, improve=10.05120, (0 missing) PctDiscMM &lt; 0.0729725 to the left, improve=10.05120, (0 missing) Surrogate splits: SalePriceMM &lt; 1.585 to the right, agree=0.906, adj=0.375, (0 split) DiscMM &lt; 0.57 to the left, agree=0.895, adj=0.300, (0 split) PctDiscMM &lt; 0.264375 to the left, agree=0.895, adj=0.300, (0 split) WeekofPurchase &lt; 274.5 to the left, agree=0.872, adj=0.150, (0 split) SalePriceCH &lt; 2.075 to the left, agree=0.857, adj=0.050, (0 split) Node number 10: 226 observations predicted class=CH expected loss=0.2212389 P(node) =0.2637106 class counts: 176 50 probabilities: 0.779 0.221 Node number 11: 40 observations predicted class=MM expected loss=0.225 P(node) =0.04667445 class counts: 9 31 probabilities: 0.225 0.775 Una vez tenemos un modelo (o varios) los podemos evaluar en la muestra test con las medidas estándard pred &lt;- predict(oj_mdl_cart, newdata = oj_test, type = &quot;class&quot;) oj_cm_cart &lt;- confusionMatrix(pred, oj_test$Purchase) oj_cm_cart Confusion Matrix and Statistics Reference Prediction CH MM CH 113 13 MM 17 70 Accuracy : 0.8592 95% CI : (0.8051, 0.9029) No Information Rate : 0.6103 P-Value [Acc &gt; NIR] : 1.265e-15 Kappa : 0.7064 Mcnemar&#39;s Test P-Value : 0.5839 Sensitivity : 0.8692 Specificity : 0.8434 Pos Pred Value : 0.8968 Neg Pred Value : 0.8046 Prevalence : 0.6103 Detection Rate : 0.5305 Detection Prevalence : 0.5915 Balanced Accuracy : 0.8563 &#39;Positive&#39; Class : CH También podemos representar gráficamente la tabla de confusión plot(oj_test$Purchase, pred, main = &quot;Clasificación: Predicho vs. Observado&quot;, xlab = &quot;Observado&quot;, ylab = &quot;Predicho&quot;) 6.2 Área bajo la curva ROC También podemos calcular el área bajo la curva ROC. La curva ROC (características operativas del receptor) es otra medida de precisión. Corresponde a un gráfico de la tasa de verdaderos positivos (TPR, sensibilidad) versus la tasa de falsos positivos (FPR, 1 - especificidad) para un conjunto de umbrales. De forma predeterminada, el umbral para predecir la clasificación predeterminada es 0.50, pero podría ser cualquier umbral. La función precrec::evalmod () calcula los valores de la matriz de confusión del modelo usando el conjunto de datos test. El AUC en el conjunto de datos test es 0.8848 y podemos calcularlo con varias funciones: pROC::plot.roc (), plotROC::geom_roc (), yardstick::roc_curve () y plotROC para usar ggplot() [geometría geom_roc ()]. Nosotros usaremos pROC. Para ello necesitamos tener las predicciones como probabilidades para la categoría de referencia. NOTA: El AUC es, pues, una medida útil para casos donde el predictor es binario. library(pROC) pred2 &lt;- predict(oj_mdl_cart, newdata = oj_test, type = &quot;prob&quot;)[,&quot;CH&quot;] roc.car &lt;- roc(oj_test$Purchase, pred2, print.auc=TRUE, ci=TRUE, plot=TRUE) 6.2.1 Entrenamiento con caret También podemos ajustar el modelo con la función caret::train (). Recordemo que hay dos formas de ajustar los hiperparámetros cuando usamos train (): establecer el número de valores de parámetros de ajuste a considerar utilizando tuneLength, o establecer ciertos valores para cada parámetro utilizando tuneGrid. ESTRATEGIA: Construiremos el modelo usando una validación cruzada de 10 veces para optimizar el hiperparámetro CP. Si no tenemos idea de cuál es el parámetro de ajuste óptimo, empezaremos con tuneLength para aproximarnos al valor óptimo y luego ajustaremos el valor con tuneGrid. Crearemos un objeto de control de entrenamiento que puedo reutilizar en otras compilaciones de modelos. oj_trControl = trainControl (method = &quot;cv&quot;, number = 10, savePredictions = &quot;final&quot;, # guardaremos preds para el valor óptimo del parámetro a tunear classProbs = TRUE, # probs para las clases además de preds summaryFunction = twoClassSummary ) Ahora estimamos el modelo con set.seed(1234) oj_mdl_cart2 &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;ROC&quot;, trControl = oj_trControl ) caret construye un árbol completo usando los parámetros predeterminados de rpart que son: índice de división de Gini, al menos 20 observaciones en un nodo para considerar dividirlo, y al menos 6 observaciones en cada nodo. Luego, caret calcula la precisión para cada valor candidato del hiperparámetro (CP). Estos son los resultados: oj_mdl_cart2 CART 857 samples 17 predictor 2 classes: &#39;CH&#39;, &#39;MM&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... Resampling results across tuning parameters: cp ROC Sens Spec 0.005988024 0.8539885 0.8605225 0.7274510 0.008982036 0.8502309 0.8568578 0.7334225 0.013473054 0.8459290 0.8473149 0.7397504 0.032934132 0.7776483 0.8509071 0.6796791 0.479041916 0.5878764 0.9201379 0.2556150 ROC was used to select the optimal model using the largest value. The final value used for the model was cp = 0.005988024. El segundo CP (0.008982036) produce la mayor precisión. Podemos profundizar en el mejor valor de CP usando un tuning grid. set.seed(1234) oj_mdl_cart2 &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0.001, to = 0.010, length = 11)), metric = &quot;ROC&quot;, trControl = oj_trControl ) print(oj_mdl_cart2) CART 857 samples 17 predictor 2 classes: &#39;CH&#39;, &#39;MM&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... Resampling results across tuning parameters: cp ROC Sens Spec 0.0010 0.8513056 0.8529390 0.7182709 0.0019 0.8528471 0.8529753 0.7213012 0.0028 0.8524435 0.8510522 0.7302139 0.0037 0.8533529 0.8510522 0.7421569 0.0046 0.8540042 0.8491292 0.7333333 0.0055 0.8543820 0.8567126 0.7334225 0.0064 0.8539885 0.8605225 0.7274510 0.0073 0.8521076 0.8625181 0.7335116 0.0082 0.8521076 0.8625181 0.7335116 0.0091 0.8502309 0.8568578 0.7334225 0.0100 0.8507262 0.8510885 0.7424242 ROC was used to select the optimal model using the largest value. The final value used for the model was cp = 0.0055. El mejor modelo se consigue con CP = 0.0082. A continuación podemos ver las precisiones de validación cruzada para los valores de CP candidatos. plot(oj_mdl_cart2) Estos son los resultados para el modelo final: oj_mdl_cart2$finalModel n= 857 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 857 334 CH (0.61026838 0.38973162) 2) LoyalCH&gt;=0.48285 537 94 CH (0.82495345 0.17504655) 4) LoyalCH&gt;=0.7648795 271 13 CH (0.95202952 0.04797048) * 5) LoyalCH&lt; 0.7648795 266 81 CH (0.69548872 0.30451128) 10) PriceDiff&gt;=-0.165 226 50 CH (0.77876106 0.22123894) * 11) PriceDiff&lt; -0.165 40 9 MM (0.22500000 0.77500000) * 3) LoyalCH&lt; 0.48285 320 80 MM (0.25000000 0.75000000) 6) LoyalCH&gt;=0.2761415 146 58 MM (0.39726027 0.60273973) 12) SalePriceMM&gt;=2.04 71 31 CH (0.56338028 0.43661972) 24) LoyalCH&lt; 0.303104 7 0 CH (1.00000000 0.00000000) * 25) LoyalCH&gt;=0.303104 64 31 CH (0.51562500 0.48437500) 50) WeekofPurchase&gt;=246.5 52 22 CH (0.57692308 0.42307692) 100) PriceCH&lt; 1.94 35 11 CH (0.68571429 0.31428571) * 101) PriceCH&gt;=1.94 17 6 MM (0.35294118 0.64705882) * 51) WeekofPurchase&lt; 246.5 12 3 MM (0.25000000 0.75000000) * 13) SalePriceMM&lt; 2.04 75 18 MM (0.24000000 0.76000000) 26) SpecialCH&gt;=0.5 14 6 CH (0.57142857 0.42857143) * 27) SpecialCH&lt; 0.5 61 10 MM (0.16393443 0.83606557) * 7) LoyalCH&lt; 0.2761415 174 22 MM (0.12643678 0.87356322) * rpart.plot(oj_mdl_cart2$finalModel) Veamos el rendimiento en la muestra test: pred3 &lt;- predict(oj_mdl_cart2, newdata = oj_test, type = &quot;raw&quot;) oj_cm_cart2 &lt;- confusionMatrix(pred3, oj_test$Purchase) oj_cm_cart2 Confusion Matrix and Statistics Reference Prediction CH MM CH 117 18 MM 13 65 Accuracy : 0.8545 95% CI : (0.7998, 0.8989) No Information Rate : 0.6103 P-Value [Acc &gt; NIR] : 4.83e-15 Kappa : 0.6907 Mcnemar&#39;s Test P-Value : 0.4725 Sensitivity : 0.9000 Specificity : 0.7831 Pos Pred Value : 0.8667 Neg Pred Value : 0.8333 Prevalence : 0.6103 Detection Rate : 0.5493 Detection Prevalence : 0.6338 Balanced Accuracy : 0.8416 &#39;Positive&#39; Class : CH La precisión es 0.8545, un poco peor que la 0.8592 del método directo. El AUC es 0.916 que es mejor que el obtenido con el método directo. pred4 &lt;- predict(oj_mdl_cart2, newdata = oj_test, type = &quot;prob&quot;)[,&quot;CH&quot;] roc.car2 &lt;- roc(oj_test$Purchase, pred4, print.auc=TRUE, ci=TRUE, plot=TRUE) Podemos comparar ambas curvas ROC mediante el test de DeLong roc.test(roc.car, roc.car2) DeLong&#39;s test for two correlated ROC curves data: roc.car and roc.car2 Z = -2.4259, p-value = 0.01527 alternative hypothesis: true difference in AUC is not equal to 0 sample estimates: AUC of roc1 AUC of roc2 0.8848471 0.9162651 Finalmente, podemos crear fácilmente la gráfica de importancia de variables con la función varImp (). La lealtad a la marca es lo más importante, seguida de la diferencia de precio. plot(varImp(oj_mdl_cart2), main=&quot;Importancia de variables con CART (caret)&quot;) Parece que con la estrategia de caret hemos conseguido un mejor modelo predictivo gracias, sobre todo, a la posibilidad de buscar el mejor hiperparámetro haciend fine tuning. oj_scoreboard &lt;- rbind( data.frame(Modelo = &quot;Single Tree&quot;, Accuracy = oj_cm_cart$overall[&quot;Accuracy&quot;], ROC = roc.car$auc), data.frame(Modelo = &quot;Single Tree (caret)&quot;, Accuracy = oj_cm_cart2$overall[&quot;Accuracy&quot;], ROC = roc.car2$auc)) %&gt;% arrange(desc(ROC)) knitr::kable(oj_scoreboard, row.names = FALSE) Modelo Accuracy ROC Single Tree (caret) 0.8544601 0.9162651 Single Tree 0.8591549 0.8848471 6.3 Árboles de regresión Un árbol de regresión simple se construye de manera similar a un árbol de clasificación simple y, al igual que el árbol de clasificación, rara vez se usan por sí solo (sobre todo en problemas complejos o de big data). De nuevo, basaremos el aprendizaje de esta metodología partiendo de un ejemplo real. Usaremos el conjunto de datos ISLR::Carseats que pretende predecir las ventas de sillitas de niños para coches (variable Sales) en 400 tiendas usando 10 variables que contienen información de las características de las sillas. cs_dat &lt;- ISLR::Carseats skimr::skim(cs_dat) Table 6.2: Data summary Name cs_dat Number of rows 400 Number of columns 11 _______________________ Column type frequency: factor 3 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts ShelveLoc 0 1 FALSE 3 Med: 219, Bad: 96, Goo: 85 Urban 0 1 FALSE 2 Yes: 282, No: 118 US 0 1 FALSE 2 Yes: 258, No: 142 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sales 0 1 7.50 2.82 0 5.39 7.49 9.32 16.27  CompPrice 0 1 124.97 15.33 77 115.00 125.00 135.00 175.00  Income 0 1 68.66 27.99 21 42.75 69.00 91.00 120.00  Advertising 0 1 6.64 6.65 0 0.00 5.00 12.00 29.00  Population 0 1 264.84 147.38 10 139.00 272.00 398.50 509.00  Price 0 1 115.80 23.68 24 100.00 117.00 131.00 191.00  Age 0 1 53.32 16.20 25 39.75 54.50 66.00 80.00  Education 0 1 13.90 2.62 10 12.00 14.00 16.00 18.00  De nuevo, partiremos nuestro conjunto de datos cs_dat (n = 400) en cs_train (80%, n = 321) y cs_test (20%, n = 79). set.seed(12345) partition &lt;- createDataPartition(y = cs_dat$Sales, p = 0.8, list = FALSE) cs_train &lt;- cs_dat[partition, ] cs_test &lt;- cs_dat[-partition, ] El primer paso es construir un árbol completo y luego realizar una validación cruzada para ayudar a seleccionar la complejidad de costo óptima (cp). La única diferencia ahora es que usaremos method = \"anova\" en la función rpart () para poder estimar un árbol de regresión. set.seed(1234) cs_mdl_cart_full &lt;- rpart(Sales ~ ., cs_train, method = &quot;anova&quot;) cs_mdl_cart_full n= 321 node), split, n, deviance, yval * denotes terminal node 1) root 321 2567.76800 7.535950 2) ShelveLoc=Bad,Medium 251 1474.14100 6.770359 4) Price&gt;=105.5 168 719.70630 5.987024 8) ShelveLoc=Bad 50 165.70160 4.693600 16) Population&lt; 201.5 20 48.35505 3.646500 * 17) Population&gt;=201.5 30 80.79922 5.391667 * 9) ShelveLoc=Medium 118 434.91370 6.535085 18) Advertising&lt; 11.5 88 290.05490 6.113068 36) CompPrice&lt; 142 69 193.86340 5.769420 72) Price&gt;=132.5 16 50.75440 4.455000 * 73) Price&lt; 132.5 53 107.12060 6.166226 * 37) CompPrice&gt;=142 19 58.45118 7.361053 * 19) Advertising&gt;=11.5 30 83.21323 7.773000 * 5) Price&lt; 105.5 83 442.68920 8.355904 10) Age&gt;=63.5 32 153.42300 6.922500 20) Price&gt;=85 25 66.89398 6.160800 40) ShelveLoc=Bad 9 18.39396 4.772222 * 41) ShelveLoc=Medium 16 21.38544 6.941875 * 21) Price&lt; 85 7 20.22194 9.642857 * 11) Age&lt; 63.5 51 182.26350 9.255294 22) Income&lt; 57.5 12 28.03042 7.707500 * 23) Income&gt;=57.5 39 116.63950 9.731538 46) Age&gt;=50.5 14 21.32597 8.451429 * 47) Age&lt; 50.5 25 59.52474 10.448400 * 3) ShelveLoc=Good 70 418.98290 10.281140 6) Price&gt;=107.5 49 242.58730 9.441633 12) Advertising&lt; 13.5 41 162.47820 8.926098 24) Age&gt;=61 17 53.37051 7.757647 * 25) Age&lt; 61 24 69.45776 9.753750 * 13) Advertising&gt;=13.5 8 13.36599 12.083750 * 7) Price&lt; 107.5 21 61.28200 12.240000 * Las ventas pronosticadas en la raíz son las ventas medias para el conjunto de datos de entrenamiento, 7.5 (los valores corresponden a miles de dolares). La primera división está en ShelveLoc = [Bad, Medium] vs Good (calidad). Aquí está el diagrama de árbol sin podar. rpart.plot(cs_mdl_cart_full, yesno = TRUE) Cada caja muestra el valor predicho del nodo (media) y la proporción de observaciones que están en el nodo (o nodos secundarios). rpart () estima el árbol completo y utiliza validación cruzada para probar el rendimiento de los posibles hiperparámetros de complejidad. Como antes, printcp () muestra los valores de cp candidatos que pueden verse en esta tabla. Estos datos pueden ser utilizados para decidir cómo podar el árbol. printcp(cs_mdl_cart_full) Regression tree: rpart(formula = Sales ~ ., data = cs_train, method = &quot;anova&quot;) Variables actually used in tree construction: [1] Advertising Age CompPrice Income Population Price ShelveLoc Root node error: 2567.8/321 = 7.9993 n= 321 CP nsplit rel error xerror xstd 1 0.262736 0 1.00000 1.00635 0.076664 2 0.121407 1 0.73726 0.74888 0.058981 3 0.046379 2 0.61586 0.65278 0.050839 4 0.044830 3 0.56948 0.67245 0.051638 5 0.041671 4 0.52465 0.66230 0.051065 6 0.025993 5 0.48298 0.62345 0.049368 7 0.025823 6 0.45698 0.61980 0.048026 8 0.024007 7 0.43116 0.62058 0.048213 9 0.015441 8 0.40715 0.58061 0.041738 10 0.014698 9 0.39171 0.56413 0.041368 11 0.014641 10 0.37701 0.56277 0.041271 12 0.014233 11 0.36237 0.56081 0.041097 13 0.014015 12 0.34814 0.55647 0.038308 14 0.013938 13 0.33413 0.55647 0.038308 15 0.010560 14 0.32019 0.57110 0.038872 16 0.010000 15 0.30963 0.56676 0.038090 Hay 16 posibles valores de cp en este modelo. El modelo con el parámetro de complejidad más pequeño permite la mayoría de las divisiones (nsplit). El parámetro de mayor complejidad corresponde a un árbol con solo un nodo raíz. rel error es el SSE relativo al nodo raíz. El SSE del nodo raíz es 2567.76800, por lo que su error rel es 2567.76800 / 2567.76800 = 1.0. Eso significa que el error absoluto del árbol completo (en CP = 0.01) es 0.30963 * 2567.76800 = 795.058. Podemos verificar estos resultados calculando el SSE de los valores predichos del modelo: data.frame(pred = predict(cs_mdl_cart_full, newdata = cs_train)) %&gt;% mutate(obs = cs_train$Sales, sq_err = (obs - pred)^2) %&gt;% summarise(sse = sum(sq_err)) sse 1 795.0525 La tabla también muestra, xerror que corresponde al SSE con validación cruzada y xstd a su error estándar. Si deseamos el error más bajo posible, podaremos el árbol con el SSE relativo más pequeño (xerror). Si deseamos equilibrar el poder predictivo con la simplicidad, podaremos al árbol más pequeño que esté dentro de 1 SE para el SSE relativo más pequeño. Al igual que en la sección anterior, la tabla CP no es muy útil para encontrar ese árbol, por lo que debemos añadir una columna para visualizar dicha información: cs_mdl_cart_full$cptable %&gt;% data.frame() %&gt;% mutate(min_xerror_idx = which.min(cs_mdl_cart_full$cptable[, &quot;xerror&quot;]), rownum = row_number(), xerror_cap = cs_mdl_cart_full$cptable[min_xerror_idx, &quot;xerror&quot;] + cs_mdl_cart_full$cptable[min_xerror_idx, &quot;xstd&quot;], eval = case_when(rownum == min_xerror_idx ~ &quot;min xerror&quot;, xerror &lt; xerror_cap ~ &quot;under cap&quot;, TRUE ~ &quot;&quot;)) %&gt;% dplyr::select(-rownum, -min_xerror_idx) CP nsplit rel.error xerror xstd xerror_cap eval 1 0.26273578 0 1.0000000 1.0063530 0.07666355 0.5947744 2 0.12140705 1 0.7372642 0.7488767 0.05898146 0.5947744 3 0.04637919 2 0.6158572 0.6527823 0.05083938 0.5947744 4 0.04483023 3 0.5694780 0.6724529 0.05163819 0.5947744 5 0.04167149 4 0.5246478 0.6623028 0.05106530 0.5947744 6 0.02599265 5 0.4829763 0.6234457 0.04936799 0.5947744 7 0.02582284 6 0.4569836 0.6198034 0.04802643 0.5947744 8 0.02400748 7 0.4311608 0.6205756 0.04821332 0.5947744 9 0.01544139 8 0.4071533 0.5806072 0.04173785 0.5947744 under cap 10 0.01469771 9 0.3917119 0.5641331 0.04136793 0.5947744 under cap 11 0.01464055 10 0.3770142 0.5627713 0.04127139 0.5947744 under cap 12 0.01423309 11 0.3623736 0.5608073 0.04109662 0.5947744 under cap 13 0.01401541 12 0.3481405 0.5564663 0.03830810 0.5947744 min xerror 14 0.01393771 13 0.3341251 0.5564663 0.03830810 0.5947744 under cap 15 0.01055959 14 0.3201874 0.5710951 0.03887227 0.5947744 under cap 16 0.01000000 15 0.3096278 0.5667561 0.03808991 0.5947744 under cap Bien, entonces el árbol más simple es el que tiene CP = 0.02599265 (5 divisiones). También podemos usar plotcp () para visualizar la relación entrexerrorycp`. plotcp(cs_mdl_cart_full, upper = &quot;splits&quot;) La línea discontinua se establece en el mínimo xerror + xstd. El eje superior muestra el número de divisiones en el árbol. El error relativo más pequeño está en CP = 0.01 (15 divisiones), pero el CP máximo debajo de la línea discontinua (una desviación estándar por encima del error mínimo) está en CP = 0.02599265 (5 divisiones). Utilizamos entonces la función prune () para podar el árbol especificando el coste-complejidad asociado a este CP. cs_mdl_cart &lt;- prune( cs_mdl_cart_full, cp = cs_mdl_cart_full$cptable[cs_mdl_cart_full$cptable[, 2] == 5, &quot;CP&quot;] ) rpart.plot(cs_mdl_cart, yesno = TRUE) El indicador más importante de ventas es ShelveLoc. Estos son los valores de importancia del modelo: cs_mdl_cart$variable.importance %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Feature&quot;) %&gt;% rename(Overall = &#39;.&#39;) %&gt;% ggplot(aes(x = fct_reorder(Feature, Overall), y = Overall)) + geom_pointrange(aes(ymin = 0, ymax = Overall), color = &quot;cadetblue&quot;, size = .3) + theme_minimal() + coord_flip() + labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Variable Importance with Simple Regression&quot;) El indicador más importante de ventas es ShelveLoc, luego Price, luego Age (edad media de la población donde está la tienda). Todas estas variables aparecen en el modelo final. CompPrice (precio del competidor) también es relevante. El último paso es hacer predicciones sobre el conjunto de datos de validación. Cuando la variable respuesta es continua usamos: la raíz del error cuadrático medio \\(RMSE = \\sqrt{(1/2) \\sum{(actual - pred)^2}})\\) y el errr absoluto medio \\(MAE = (1/n) \\sum{|actual - pred|}\\) La diferencia entre ambos es que RMSE penaliza más los errores grandes. Para un árbol de regresión, basta con indicar type=\"vector\") en la funciónpredict ()` (que es el valor por defecto). cs_preds_cart &lt;- predict(cs_mdl_cart, cs_test, type = &quot;vector&quot;) cs_rmse_cart &lt;- RMSE( pred = cs_preds_cart, obs = cs_test$Sales ) cs_rmse_cart [1] 2.363202 El proceso de poda conduce a un error de predicción promedio de 2.363 en el conjunto de datos de prueba. No está mal considerando que la desviación estándar de la variable Sales es 2.8. Podemos visualizar la relación entre los datos predichos y los observados mediante: data.frame(Predichos = cs_preds_cart, Observados = cs_test$Sales) %&gt;% ggplot(aes(x = Observados, y = Predichos)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth() + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats CART, predichos vs observados&quot;) 6.3.1 Entrenamiento con caret También podemos ajustar el modelo con caret::train () especificando method = \"rpart\". Construirmos el modelo usando 10-fold CV para optimizar el hiperparámetro CP. cs_trControl = trainControl( method = &quot;cv&quot;, number = 10, savePredictions = &quot;final&quot; ) Usaremos la misma estrategia que en el caso de los árboles de clasificación, dejaremos que el modelo busque el mejor parámetro de ajuste de CP con tuneLength y luego lo ajustaremos con tuneGrid. set.seed(1234) cs_mdl_cart2 = train( Sales ~ ., data = cs_train, method = &quot;rpart&quot;, tuneLength = 5, metric = &quot;RMSE&quot;, trControl = cs_trControl ) cs_mdl_cart2 CART 321 samples 10 predictor No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... Resampling results across tuning parameters: cp RMSE Rsquared MAE 0.04167149 2.209383 0.4065251 1.778797 0.04483023 2.243618 0.3849728 1.805027 0.04637919 2.275563 0.3684309 1.808814 0.12140705 2.400455 0.2942663 1.936927 0.26273578 2.692867 0.1898998 2.192774 RMSE was used to select the optimal model using the smallest value. The final value used for the model was cp = 0.04167149. El primer cp (0.04167149) presenta el RMSE más pequeño. Puedemos hacer una búsqueda más fina para mejorar el valor de cp usando un grid: set.seed(1234) cs_mdl_cart2 = train( Sales ~ ., data = cs_train, method = &quot;rpart&quot;, tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.01)), metric = &quot;RMSE&quot;, trControl = cs_trControl ) cs_mdl_cart2 CART 321 samples 10 predictor No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... Resampling results across tuning parameters: cp RMSE Rsquared MAE 0.00 2.055676 0.5027431 1.695453 0.01 2.135096 0.4642577 1.745937 0.02 2.095767 0.4733269 1.699235 0.03 2.131246 0.4534544 1.690453 0.04 2.146886 0.4411380 1.712705 0.05 2.284937 0.3614130 1.837782 0.06 2.265498 0.3709523 1.808319 0.07 2.282630 0.3597216 1.836227 0.08 2.282630 0.3597216 1.836227 0.09 2.282630 0.3597216 1.836227 0.10 2.282630 0.3597216 1.836227 RMSE was used to select the optimal model using the smallest value. The final value used for the model was cp = 0. En este ejemplo, parece que el árbol con mejor rendimiento es el que no ha sido podado. plot(cs_mdl_cart2) Este sería el modelo final rpart.plot(cs_mdl_cart2$finalModel) y estas las variables más importantes plot(varImp(cs_mdl_cart2), main=&quot;Importancia de variables para Regresión&quot;) Como siempre, debemos evaluar el modelo en nuestra muestra test: cs_preds_cart2 &lt;- predict(cs_mdl_cart2, cs_test, type = &quot;raw&quot;) data.frame(Actual = cs_test$Sales, Predicted = cs_preds_cart2) %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats CART, Predicted vs Actual (caret)&quot;) Observamos como el modelo sobreestima en el extremo inferior y subestima en el extremo superior. Podemos calcular el RMSE para estos datos: (cs_rmse_cart2 &lt;- RMSE(pred = cs_preds_cart2, obs = cs_test$Sales)) [1] 2.298331 Caret mejora las predicciones: cs_scoreboard &lt;- rbind( data.frame(Modelo = &quot;Single Tree&quot;, RMSE = cs_rmse_cart), data.frame(Modelo = &quot;Single Tree (caret)&quot;, RMSE = cs_rmse_cart2) ) %&gt;% arrange(RMSE) knitr::kable(cs_scoreboard, row.names = FALSE) Modelo RMSE Single Tree (caret) 2.298331 Single Tree 2.363202 6.4 Bagged trees Los CART tiene una capacidad predictiva moderada, es por ello que se han propuesto unos métodos que combinan varios árboles de decisión para producir un mejor rendimiento predictivo que utilizar un solo árbol de decisión. El principio fundamental detrás de estos modelos es que un grupo de predictores débiles puede conseguir un predictor con mejor capacidad predictiva. Tenemos dos tipos de estrategias: Bagging Boosting Bagging (Bootstrap Aggregation) se utiliza cuando nuestro objetivo es reducir la varianza de un árbol de decisión. La idea es crear varios subconjuntos de datos a partir de la muestra de entrenamiento elegida al azar con reemplazamiento. Cada subconjunto de datos se utiliza para entrenar un árbol de decisión. Como resultado, terminamos con un conjunto de diferentes modelos. Se utiliza el promedio de todas las predicciones de diferentes árboles, que es más robusto que considerar un solo árbol de decisión. Bagged trees En el Boosting se aprende de forma secuencial. Ajustamos árboles consecutivos (muestra aleatoria) y en cada paso, el objetivo es mejorar el error del árbol anterior. Boosted trees Como hemos dicho anteriormente, el algoritmo bagged construye B árboles decisión usando conjuntos de entrenamiento obtenidos mediante remuestreo y promedia las predicciones resultantes. Estos árboles crecen profundamente y no se podan. Por tanto, cada árbol individual tiene una alta varianza, pero un bajo sesgo. Promediar los B árboles ayuda a reducir la varianza. El valor predicho para una observación es la moda (clasificación) o la media (regresión) de los árboles. B generalmente es igual a ~ 25. Proceso para Bagged trees Para un conjunto de entrenamiento de tamaño \\(n\\), cada árbol se compone de \\(\\sim (1 - e^{-1})n = .632n\\) observaciones únicas in-bag y \\(.368n\\) out-of-bag. Las observaciones que no han sido seleccionadas en el re-muestreo se usan para evaluar la precisión del modelo. La capacidad glogal del método se obtiene promediando la capacidad de cada árbol. Esto tiene una desventaja obvia y es que si cada árbol tiene un rendimiento deficiente, el rendimiento promedio de muchos árboles seguirá siendo deficiente. Además, otra desventaja de este método es que no existe un árbol único con un conjunto de reglas para interpretar. En consecuencia, no queda claro qué variables son más importantes que otras y en algunos problemas (sobre todo biomédicos) esto puede ser una limitación importante. 6.4.1 Bagging árboles de clasificación Veamos de nuevo con un ejemplo cómo trabajar con estos métodos. Usaremos de nuevo los datos de zumos de naranja OJ. Esta vez usaremos un método bagging especificando method=\"treebag\". Caret no tiene hiperparámetros para este modelo, por lo que no es necesario usar tuneLegth ni tuneGrid. El tamaño de conjunto predeterminado es nbagg = 25 (a veces se puede tunear, pero en este caso lo dejaremos fijo). set.seed(1234) oj_mdl_bag &lt;- train( Purchase ~ ., data = oj_train, method = &quot;treebag&quot;, trControl = oj_trControl, metric = &quot;ROC&quot; ) oj_mdl_bag$finalModel Bagging classification trees with 25 bootstrap replications Veamos el rendimiento en el conjunto de datos test. pred_bag &lt;- predict(oj_mdl_bag, newdata = oj_test, type = &quot;raw&quot;) oj_cm_bag &lt;- confusionMatrix(pred_bag, oj_test$Purchase) oj_cm_bag Confusion Matrix and Statistics Reference Prediction CH MM CH 113 16 MM 17 67 Accuracy : 0.8451 95% CI : (0.7894, 0.8909) No Information Rate : 0.6103 P-Value [Acc &gt; NIR] : 6.311e-14 Kappa : 0.675 Mcnemar&#39;s Test P-Value : 1 Sensitivity : 0.8692 Specificity : 0.8072 Pos Pred Value : 0.8760 Neg Pred Value : 0.7976 Prevalence : 0.6103 Detection Rate : 0.5305 Detection Prevalence : 0.6056 Balanced Accuracy : 0.8382 &#39;Positive&#39; Class : CH La precisión es 0.8451, sorprendentemente peor que el 0.85915 del modelo de árbol único, pero esa es una diferencia que corresponde a tres predicciones en un conjunto de 213. Esta sería la curva ROC. pred_bag2 &lt;- predict(oj_mdl_bag, newdata = oj_test, type = &quot;prob&quot;)[,&quot;CH&quot;] roc.bag &lt;- roc(oj_test$Purchase, pred_bag2, print.auc=TRUE, ci=TRUE, plot=TRUE) Veamos cuáles son las variables más importantes plot(varImp(oj_mdl_bag), main=&quot;Importancia de variables con Bagging&quot;) Esta es la comparación entre métodos oj_scoreboard &lt;- rbind(oj_scoreboard, data.frame(Modelo = &quot;Bagging&quot;, Accuracy = oj_cm_bag$overall[&quot;Accuracy&quot;], ROC = roc.bag$auc) ) %&gt;% arrange(desc(ROC)) knitr::kable(oj_scoreboard, row.names = FALSE) Modelo Accuracy ROC Single Tree (caret) 0.8544601 0.9162651 Bagging 0.8450704 0.9099166 Single Tree 0.8591549 0.8848471 6.4.2 Bagging árboles de regresión Usemos bagging para predecir las ventas en los datos Carseats: set.seed(1234) cs_mdl_bag &lt;- train( Sales ~ ., data = cs_train, method = &quot;treebag&quot;, trControl = cs_trControl ) cs_mdl_bag Bagged CART 321 samples 10 predictor No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 289, 289, 289, 289, 289, 289, ... Resampling results: RMSE Rsquared MAE 1.681889 0.675239 1.343427 Veamos el rendimiento en el conjunto de datos test. El RMSE es 1.9185, pero el modelo predice en exceso en el extremo inferior de ventas y tampoco predice bien en el extremo superior (como un árbol simple). cs_preds_bag &lt;- bind_cols( Predicted = predict(cs_mdl_bag, newdata = cs_test), Actual = cs_test$Sales ) (cs_rmse_bag &lt;- RMSE(pred = cs_preds_bag$Predicted, obs = cs_preds_bag$Actual)) [1] 1.918473 cs_preds_bag %&gt;% ggplot(aes(x = Actual, y = Predicted)) + geom_point(alpha = 0.6, color = &quot;cadetblue&quot;) + geom_smooth(method = &quot;loess&quot;, formula = &quot;y ~ x&quot;) + geom_abline(intercept = 0, slope = 1, linetype = 2) + labs(title = &quot;Carseats Bagging, Predicted vs Actual (caret)&quot;) La importancia de las variables son: plot(varImp(cs_mdl_bag), main=&quot;Importancia de variables con Bagging&quot;) Y la comparación quedaría cs_scoreboard &lt;- rbind(cs_scoreboard, data.frame(Modelo = &quot;Bagging&quot;, RMSE = cs_rmse_bag) ) %&gt;% arrange(RMSE) knitr::kable(cs_scoreboard, row.names = FALSE) Modelo RMSE Bagging 1.918473 Single Tree (caret) 2.298331 Single Tree 2.363202 6.5 Random Forest Los Random Forest (bosques aleatorios) también son un conjunto de árboles de decisión (ensambladores) que mejoran los bagged trees mediante la creación de un bosque no correlacionados de árboles que, de nuevo, mejora la capacidad predictiva de un único árbol. Al igual que en el bagged (embolsado), el algoritmo construye varios árboles de decisión sobre muestras de entrenamiento bootstrap. Sin embargo, al construir estos árboles de decisión, cada vez que se considera una división en un árbol, se elige una muestra aleatoria de predictores (hiperparámetro m o mtry) como candidatos de división del conjunto completo de predictores \\(p\\). En cada división se toma una nueva muestra de predictores. Típicamente \\(m \\approx \\sqrt{p}\\). En consecuencia, los árboles bagged son un caso especial de los random forest cuando \\(m = p\\). Cada árbol del modelo random forest se construye de la siguiente forma: Si denotamos por \\(N\\) el número de casos en el conjunto de entrenamiento, seleccionaremos una muestra de esos \\(N\\) casos se forma aleatoria CON REEMPLAZAMIENTO. Esta muestra será el conjunto de entrenamiento para construir el árbol i-ésimo. Si denotamos por \\(M\\) el número total de varibles predictoras, seleccionaremos un número \\(m &lt; M\\) de variables y crearemos un árbol completo con esas variables. El valor \\(m\\) se mantiene constante durante la generación de todo el bosque. Cada árbol crece hasta su máxima extensión posible y NO hay proceso de poda. La predicción para nuevos individuos se hace a partir de la información obtenida de las predicciones de los \\(B\\) árboles (mayoría de votos para clasificación, promedio para regresión). La siguiente figura ilustra este proceso Random Forest Podemos estimar un random forest con cart indicando el argumento method = \"rf\". El hiperparámetro mtry (\\(m\\)) puede tomar cualquier valor de 1 a 17 (el número de predictores) y se espera que el valor óptimo esté cerca de \\(\\sqrt{17} \\approx 4\\). En cuanto al número de árboles (segundo hiperparámetro), hay estudios que demuestran que el rendimiento empeora cuando tenemos muchos árboles, sin embargo esto no está muy claro y por lo general se recomienda entrenar modelos con muchos árboles. Por defecto method = \"rf\" tiene 500 (argumento num.trees). set.seed(1234) oj_mdl_rf &lt;- train( Purchase ~ ., data = oj_train, method = &quot;rf&quot;, metric = &quot;ROC&quot;, tuneGrid = expand.grid(mtry = 3:10), trControl = oj_trControl, num.trees = 500 ) oj_mdl_rf Random Forest 857 samples 17 predictor 2 classes: &#39;CH&#39;, &#39;MM&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 772, 772, 771, 770, 771, 771, ... Resampling results across tuning parameters: mtry ROC Sens Spec 3 0.8655672 0.8565312 0.7185383 4 0.8685845 0.8641872 0.7122995 5 0.8682630 0.8470247 0.7183601 6 0.8672458 0.8412917 0.7124777 7 0.8695796 0.8412917 0.7183601 8 0.8668721 0.8393687 0.7213012 9 0.8652269 0.8432148 0.7153298 10 0.8671443 0.8413280 0.7152406 ROC was used to select the optimal model using the largest value. The final value used for the model was mtry = 7. El valor de ROC más alto se da en \\(m = 7\\) que es más alto de lo que esperábamos, pero fijémosnos que es un valor de ROC muy similar al que se obtiene con \\(m=4\\), por lo que por el principio de parsimonia podríamos usar dicho valor plot(oj_mdl_rf) También podemos visualizar los resultados con: plot_rf &lt;- function(model) { theme_set(theme_minimal()) u &lt;- model$results %&gt;% dplyr::select(mtry, ROC, Sens, Spec) %&gt;% gather(a, b, -mtry) u %&gt;% ggplot(aes(mtry, b)) + geom_line() + geom_point() + facet_wrap(~ a, scales = &quot;free&quot;) + labs(x = &quot;Número de predictores&quot;, y = NULL, title = &quot;Relación entre el número de predictores y el comportamiento del modelo&quot;) } oj_mdl_rf %&gt;% plot_rf() Podemos usar este modelo para hacer predicciones sobre la muestra test pred_rf &lt;- predict(oj_mdl_rf, newdata = oj_test, type = &quot;raw&quot;) oj_cm_rf &lt;- confusionMatrix(pred_rf, oj_test$Purchase) oj_cm_rf Confusion Matrix and Statistics Reference Prediction CH MM CH 112 16 MM 18 67 Accuracy : 0.8404 95% CI : (0.7841, 0.8869) No Information Rate : 0.6103 P-Value [Acc &gt; NIR] : 2.164e-13 Kappa : 0.6659 Mcnemar&#39;s Test P-Value : 0.8638 Sensitivity : 0.8615 Specificity : 0.8072 Pos Pred Value : 0.8750 Neg Pred Value : 0.7882 Prevalence : 0.6103 Detection Rate : 0.5258 Detection Prevalence : 0.6009 Balanced Accuracy : 0.8344 &#39;Positive&#39; Class : CH Y el área bajo la curva ROC sería: pred_rf2 &lt;- predict(oj_mdl_rf, newdata = oj_test, type = &quot;prob&quot;)[,&quot;CH&quot;] roc.rf &lt;- roc(oj_test$Purchase, pred_bag2, print.auc=TRUE, ci=TRUE, plot=TRUE) que compara con los modelos anteriores de esta forma: Y la comparación quedaría oj_scoreboard &lt;- rbind(oj_scoreboard, data.frame(Modelo = &quot;Random Forest&quot;, Accuracy = oj_cm_rf$overall[&quot;Accuracy&quot;], ROC = roc.rf$auc) ) %&gt;% arrange(desc(ROC)) knitr::kable(oj_scoreboard, row.names = FALSE) Modelo Accuracy ROC Single Tree (caret) 0.8544601 0.9162651 Bagging 0.8450704 0.9099166 Random Forest 0.8403756 0.9099166 Single Tree 0.8591549 0.8848471 Recordemos que la importancia de las variables se puede ver con la función varImp al igual que cualquier otro modelo basado en àrboles de decisión. NOTA: El ejemplo para Random Forest con árboles de regresión es igual que lo que vimos en la sección anterior. NOTA2: En este artículo se hace un benchmarking muy interesante para saber qué método y libería de R usar en función de las características de nuestro conjunto de datos. Speiser JL et al. (2012). A Comparison of Random Forest Variable Selection Methods for Classification Prediction Modeling Random forest classification is a popular machine learning method for developing prediction models in many research settings. Often in prediction modeling, a goal is to reduce the number of variables needed to obtain a prediction in order to reduce the burden of data collection and improve efficiency. Several variable selection methods exist for the setting of random forest classification; however, there is a paucity of literature to guide users as to which method may be preferable for different types of datasets. Using 311 classification datasets freely available online, we evaluate the prediction error rates, number of variables, computation times and area under the receiver operating curve for many random forest variable selection methods. We compare random forest variable selection methods for different types of datasets (datasets with binary outcomes, datasets with many predictors, and datasets with imbalanced outcomes) and for different types of methods (standard random forest versus conditional random forest methods and test based versus performance based methods). Based on our study, the best variable selection methods for most datasets are Jiangs method and the method implemented in the VSURF R package. For datasets with many predictors, the methods implemented in the R packages varSelRF and Boruta are preferable due to computational efficiency. A significant contribution of this study is the ability to assess different variable selection techniques in the setting of random forest classification in order to identify preferable methods based on applications in expert and intelligent systems. 6.6 Random Forest p&gt;&gt;n Problema: Aplicar Random Forest para conjunto de datos con muchas variables (caso p&gt;&gt;n) Posible estrategia: Creamos K subconjuntos de variable Llevamos a cabo una selección de las variables más importantes y nos quedamos con una parte de ellas. Por ejemplo, las M más informativas Combinamos las K*M variables y repetimos los pasos 1 y 2 Acabamos con M variables seleccionadas Aplicamos Random Forest Este enfoque podría causar la pérdida de algunas variables importantes, pero generalmente seleccionará las variables más informativas. Selección de K y M Breiman (2001) recomienda \\(m=p/3\\) en clasificación y \\(m=\\sqrt{p}\\) en regresión (mtry). ¿Puede servir esto de ayuda? Reguralized Random Forest Guided Regularized Random Forest Otra opción: Librería ranger "],["respuesta-no-balanceada.html", "7 Respuesta no balanceada", " 7 Respuesta no balanceada Cuando nos enfrentamos a problemas de clasificación en datos reales, puede ser un desafío tratar con un caso en el que una clase supera con creces a la otra (también conocidas como clases desequilibradas). En terminos prácticos podríamos, por ejemplo, predecir todos los inviduos como la categoría más frecuente y mejoraríamos la precisión que, como vimos en capítulos anteriores, por azar es del 50%. Estas son algunas de las técnicas más populares para tratar con el desequilibrio de clases, pero existen más. Proporcionamremos una descripción general rápida. Asignar pesos a cada clase: imponemos un coste mayor cuando se cometen errores en la clase minoritaria Muestreo descendente (under-sampling): eliminar observaciones al azar en la clase mayoritaria Muestreo ascendente (oversampling): replicar aleatoriamente observaciones en la clase minoritaria Además del muestreo descendente y ascendente, existen métodos híbridos que combinan el muestreo descendente con la generación de datos adicionales. Dos de los más populares son ROSE y SMOTE. El primer método necesita tener instalada la librería ROSE y el segundo la librería DMwR. Clases desbalanceadas Si queréis podéis encontrar una descripción más detallada en esta publicación de blog de Silicon Valley Data Science. Estos métodos también funcionan para los problemas multi-clase. En este artículo tenéis más información general sobre el tema. Debemos tener en cuenta que, en realidad, no deberíamos simplemente realizar un muestreo descendente o ascendente en nuestros datos de entrenamiento y luego ejecutar el modelo. Debemos tener en cuenta la validación cruzada y realizar un muestreo descendente o ascendente en cada sub-muestra de forma independiente para obtener una estimación real del rendimiento del modelo. También es importante señalar que estas técnicas de ponderación y muestreo tienen mayor impacto en métricas de evaluación del modelo como la precisión, porque mueven artificialmente el umbral para estar más cerca de lo que podría considerarse como la ubicación óptima en una curva ROC. Otras métricas como el AUC o la tasa de falsos o verdaderos positivos no se ven tan afectadas. Ilustremos cómo llevar a cabo estas técnicas con nuestro ejemplo de cáncer de mama. Usaremos los datos que ya están preprocesados según se hizo en la Sección 5 y que se encuentran en los objetos breast_train_prep y breast_test_prep. Efectivamente, los datos para este problema están desbalanceados respecto a la variable que queremos predecir table(breast_test_prep$diagnosis) B M 107 63 Afortunadamente, la librería caret hace que sea muy fácil incorporar técnicas de muestreo descendente y ascendente incluyendo validación cruzada. Simplemente basta con utilizar el argumento sampling en nuestro trainControl() y escoger el método down o up según nos convenga. El resto del proceso de creación de modelo permanece igual que los pasos llevados con cualquier otro método que hemos visto, por ejemlo KNN. ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5, verboseIter = FALSE, sampling = &quot;down&quot;) set.seed(1234) model_knn_under &lt;- caret::train(diagnosis ~ ., data = breast_train_prep, method = &quot;knn&quot;, trControl = ctrl) model_knn_under k-Nearest Neighbors 399 samples 19 predictor 2 classes: &#39;B&#39;, &#39;M&#39; No pre-processing Resampling: Cross-Validated (10 fold, repeated 5 times) Summary of sample sizes: 359, 359, 359, 359, 359, 359, ... Addtional sampling using down-sampling Resampling results across tuning parameters: k Accuracy Kappa 5 0.9443462 0.8815450 7 0.9483718 0.8893875 9 0.9493718 0.8920609 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 9. cm_under &lt;- confusionMatrix(breast_test_prep$diagnosis, predict(model_knn_under, newdata = breast_test_prep)) cm_under Confusion Matrix and Statistics Reference Prediction B M B 102 5 M 6 57 Accuracy : 0.9353 95% CI : (0.8872, 0.9673) No Information Rate : 0.6353 P-Value [Acc &gt; NIR] : &lt;2e-16 Kappa : 0.8608 Mcnemar&#39;s Test P-Value : 1 Sensitivity : 0.9444 Specificity : 0.9194 Pos Pred Value : 0.9533 Neg Pred Value : 0.9048 Prevalence : 0.6353 Detection Rate : 0.6000 Detection Prevalence : 0.6294 Balanced Accuracy : 0.9319 &#39;Positive&#39; Class : B Podemos comparar esta capacidad predictiva con la que tiene el modelo obiviando el problema del desbalanceo y que se estimaría así: crtl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, verboseIter = FALSE) model_knn &lt;- caret::train(diagnosis ~ ., data = breast_train_prep, method = &quot;knn&quot;, trControl = ctrl) cm_original &lt;- confusionMatrix(breast_test_prep$diagnosis, predict(model_knn, newdata = breast_test_prep)) cm_original Confusion Matrix and Statistics Reference Prediction B M B 101 6 M 5 58 Accuracy : 0.9353 95% CI : (0.8872, 0.9673) No Information Rate : 0.6235 P-Value [Acc &gt; NIR] : &lt;2e-16 Kappa : 0.8617 Mcnemar&#39;s Test P-Value : 1 Sensitivity : 0.9528 Specificity : 0.9062 Pos Pred Value : 0.9439 Neg Pred Value : 0.9206 Prevalence : 0.6235 Detection Rate : 0.5941 Detection Prevalence : 0.6294 Balanced Accuracy : 0.9295 &#39;Positive&#39; Class : B Estimemos ahora el modelo con un muestreo ascendente y usando, por ejemplo SMOTE (ROSE sea haría igual usando sampling=\"rose\") # # OVER # ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5, verboseIter = FALSE, sampling = &quot;up&quot;) model_knn_over &lt;- caret::train(diagnosis ~ ., data = breast_train_prep, method = &quot;knn&quot;, trControl = ctrl) cm_over &lt;- confusionMatrix(breast_test_prep$diagnosis, predict(model_knn_over, newdata = breast_test_prep)) # # SMOTE # ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5, verboseIter = FALSE, sampling = &quot;smote&quot;) model_knn_smote &lt;- caret::train(diagnosis ~ ., data = breast_train_prep, method = &quot;knn&quot;, trControl = ctrl) cm_smote &lt;- confusionMatrix(breast_test_prep$diagnosis, predict(model_knn_smote, newdata = breast_test_prep)) Y podemos comparar las predicciones con la función resamples (para introducir variabilidad) models &lt;- list(original = model_knn, under = model_knn_under, over = model_knn_over, smote = model_knn_smote) resampling &lt;- resamples(models) bwplot(resampling) O compararlas con otras métricas library(dplyr) select_measures &lt;- c(&quot;Sensitivity&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;Recall&quot;, &quot;F1&quot;) comparison &lt;- NULL for (name in names(models)) { model &lt;- get(paste0(&quot;cm_&quot;, name)) comparison.i &lt;- model$byClass[select_measures] comparison &lt;- rbind(comparison, comparison.i) %&gt;% as_tibble() } comparison &lt;- comparison %&gt;% add_column(model=names(models), .before=TRUE) comparison # A tibble: 4 x 6 model Sensitivity Specificity Precision Recall F1 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 original 0.953 0.906 0.944 0.953 0.948 2 under 0.944 0.919 0.953 0.944 0.949 3 over 0.935 0.903 0.944 0.935 0.940 4 smote 0.935 0.903 0.944 0.935 0.940 library(tidyr) comparison %&gt;% gather(x, y, Sensitivity:F1) %&gt;% ggplot(aes(x = x, y = y, color = model)) + geom_jitter(width = 0.2, alpha = 0.5, size = 3) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
