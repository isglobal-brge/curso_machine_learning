[["index.html", "Introducción al Aprendizaje automático en ciencias de la salud 1 Preámbulo 1.1 Instalación de librerías necesarias para el curso", " Introducción al Aprendizaje automático en ciencias de la salud Juan R González 2022-10-04 1 Preámbulo Este bookdown sirve como notas para el curso Introducción al Aprendizaje automático en ciencias de la salud impartido en el Insituto Aragonés de Ciencias de la Salud Objetivo general: Este es un curso totalmente introductorio que permitirá al alumno profundizar en las principales técnicas de aprendizaje automático aplicadas a ciencias de la salud Objetivos específicos: Introducir al alumno a los modelos básicos de predicción Conocer cómo tratar el problema de datos no balanceados Introducir al alumno a la creación de nomogramas y métodos que faciliten la aplicación de modelos predictivos aplicados a problemas de salud Programa: Introducción al aprendizaje automático Métodos de validación cruzada Regresión Logística Nomogramas Árboles de clasificación y regresión (CART) Métodos para tratar datos desbalanceados 1.1 Instalación de librerías necesarias para el curso Para poder reproducir todo el código de este libro se necesitan tener instaladas las siguientes librería install.packages(c(&quot;caret&quot;)) Los datos están accesibles en esta carpeta https://github.com/isglobal-brge/curso_machine_learning/tree/main/datos Este material está licenciado bajo una Creative Commons Attribution 4.0 International License. "],["introducción-al-aprendizaje-automático.html", "2 Introducción al Aprendizaje Automático", " 2 Introducción al Aprendizaje Automático El aprendizaje automático (AA) (Machine Learning en inglés) es una disciplina científica que suele incluirse en el ámbito de la Inteligencia Artificial (IA) que crea sistemas que aprenden automáticamente. Aprender en este contexto quiere decir identificar patrones complejos en millones de datos. La máquina/ordenador que realmente aprende es un algoritmo que usando datos existentes es capaz de predecir comportamientos futuros. Automáticamente, también en este contexto, implica que estos sistemas se mejoran de forma autónoma con el tiempo, sin intervención humana. En esta figura podemos observar la conexión que hay entre estas áreas y una más reciente conocida como aprendizaje profundo (AP) (Deep Learning en inglés) que veréis en el curso de Aprendizaje Automático 2. Relación entre AA, AI y AP La principal diferencia entre estas áreas radica en el objetivo (e.g pregunta científica) que queremos tratar. Así, la IA vendría a representar a un sistema no biológico que es inteligente basándose en reglas. El AA se basa en algoritmos que entrenan modelos usando datos existentes, y el AP se basa en algoritmos que parametriza redes neuronales de múltiples capas que representan los datos mediante diferentes niveles de abstracción. En la siguiente figura podemos ver la clasificación (de manera muy genérica) de los tipos de AA a los que podemos enfrentarnos Tipos de Aprendizaje Automático Sobre 2010 el AP (Deep learning) obtuvo una gran popularidad ya que ha permitido acercarse a sitemas de inteligencia artificial de forma más eficientge que ML. Los tres términos están ligados y cada uno forma una parte esencial de los otros. DL permite llevar a cabo ML, que en última instancia permite la AI. No obstante, es más fácil aprender ML como herramienta para AI. Debería realizarse un curso más avanzado para estudiar técnicas de DL que quizás no sean de mucha utilizad en problemas de biomedicina aplicados en ciencias de la salud, donde las bases de datos no son tan grandes ni complejas como las que pueden aparecer, por ejemplo, en el análisis de imágenes o en la genómica. En la siguiente figura podemos observar la principal diferencia entre AA y AP donde básicamente el AA pretende seleccionar aquellas variables que mejor predicien nuestra varaible respuesta y creando un modelo que ayude a dicha clasificación y el AP es una especi de caja negra donde todas las variables disponibles pasan a formar parte de un sistema que, replicando lo que hace el cerebro humano, aprende a cómo predecir nuestra variable resultado Un ejemplo claro y muy usado en nuestra vida cotidiana es la eliminación del rojo que aparecía antiguamente en las fotos. El modelo de AP es capaz de detectar un ojo en una imagen y elimiar dicho de color de forma automática Sin embargo, el AA trata problemas más sencillos que suelen ser a los que nos enfretamos en los estudios biomédicos en salud. En estadística, el AA se ha considerado como una ciencia independiente en la que se dispone de un conjunto de herramientas basadas en diferentes métodos y algoritmos que permiten clasificar individuos según una serie de variables. Concer estas técnicas estadísticas es de gran ayuda para la IA y el AP. Este es un ejemplo donde a partir de dos variables se intenta crear/entrenar un modelo (o regla de decisión - línea curva negra) que nos permita clasificar individuos para los que desconocemos su variable respuesta (figura de la derecha). Existen muchos métodos para llevar a cabo esta tarea y que permitan ser usados en la práctica clínica o en la toma de decisiones a nivel epidemiológico o de salud poblacional. Los cursos de aprendiaje automático normalmente suelen hablar de estos modelos/algoritmos de preducción Regresión logística Árboles de clasificación Análisis lineal discriminante KNN Regresión lasso (ridge, elastic net) Random Forest Boosting XGBoost En este primer curso introductorio hablaremos de los dos primeros métodos que son bastante usados en problemas de salud y también trataremos aspectos muy importantes para la creación de estos modelos predictivos que incluyen: cómo evaluar la capacidad predictiva de un modelo, cómo validar los modelos mediante validación cruzada, cómo tratar problemas de clasificación con grupos desbalanceados, y cómo crear nomogramas (y Shiny Apps automáticas) para usar estos modelos predictivos en la práctica clínica Con esta base, en un segundo curso más avanzado, podremos ver técnicas más sofisticadas que permitirán crear modelos con mejor capacidad predictiva que la regresión logística o los árboles de clasificación. Estas incluyen el boosting, XGboost o la regresión lasso que permite analizar datos con muchas más variables que individuos. "],["validación-cruzada.html", "3 Validación cruzada 3.1 Validación en un conjunto de datos externo 3.2 Leave-one-out cross validation (LOOCV) 3.3 K-fold cross validation (K-fold CV) 3.4 Uso de CV para estimar el hiper-parámetro 3.5 Uso de bootstrap 3.6 Imputación de datos faltantes (Información extra para los que venís al curso)", " 3 Validación cruzada La validación cruzada (CV por sus siglas en inglés) es la técnica que usamos para evaluar si un modelo está sobreajustado y para estimar cómo funcionará con nuevos datos. El sobreajuste es un peligro importante en el análisis predictivo, especialmente cuando se utilizan algoritmos de aprendizaje automático que, sin el ajuste adecuado, puede aprender datos de nuestra muestra casi a la perfección, esencialmente ajustando el ruido (o variabilidad). Cuando se utiliza un modelo de este tipo para predecir nuevos datos, con un ruido (o variabilidad) diferente, el rendimiento del modelo puede ser sorprendentemente malo. Usamos CV para ayudarnos a identificar y evitar tales situaciones. ¿Cómo podemos hacer esto? Muchos algoritmos de aprendizaje automático requieren que el usuario especifique ciertos parámetros (hiper-parámetros). Veremos más adelante que, por ejemplo, necesitaremos especificar un valor para \\(m\\) que corresponde al número de predictores elegidos al azar que se utilizarán en cada división de árbol cuando usemos random forest como algoritmo de aprendizaje. Cuanto menor sea \\(m\\), más simple será el árbol. Podemos usar CV para elegir el valor de \\(m\\) que minimiza la variación y reduce el sobreajuste. La regresión lineal no tiene parámetros que debe especificar el usuario, pero la CV aún nos ayuda a evaluar cuánto podría sobreajustarse un modelo a los datos de muestra. De manera breve, los algoritmos de cross-validation se pueden resumir como: Reserva una parte pequeña de los datos Crea (o entrena) el modelo usando el resto de datos Testa el modelo en los datos reservados. A continuación se describen algunas de las distintas técnicas de validación cruzada que existen. 3.1 Validación en un conjunto de datos externo La versión más simple de CV es el llamado método de conjunto de validación, que consta de los siguientes pasos: Dividir los datos de la muestra en dos partes: un conjunto de entrenamiento y otro de validacións. Los investigadores usan diferentes proporciones, pero es común seleccionar al azar el 70% de los datos como conjunto de entrenamiento y el 30% como conjunto de prueba o validación. . (Obviamente, debemos tener suficientes datos en la muestra para ajustar un modelo después de dividir los datos). Debido a que CV se basa en un muestreo aleatorio, nuestros resultados variarán a menos que usemos set.seed (). Demostraremos usando los datos de Hitters, que es un estudio sobre bateadores de USA donde se pretende crear un modelo que prediga el salario que tendría un jugador en función de sus habilidades. La variable de interés es el salario (Salary) y como es una variable continua usaremos un modelo de regresión lineal para ilustrar el concepto de validación cruzada ya que este modelo es conocido de otros cursos anteriores. Es importante notar que para el aprendizaje automático necesitamos que nuestra base de datos teng casos completos. Es decir, es importante no tener missings (existen métodos para imputar datos pero está fuera de lo que cubre este curso). Los datos están en la librería ISLR que se puede instalar de CRAN. También cargamos la libería tidyverse que nos hará falta para el manejo de datos library(arm) library(tidyverse) library(ISLR) set.seed(123) # para que los resultados sean comparables entre ordenadores Hitters_complete &lt;- Hitters[complete.cases(Hitters), ] rows &lt;- sample(nrow(Hitters_complete), .7 * nrow(Hitters_complete)) train &lt;- Hitters_complete[rows, ] test &lt;- Hitters_complete[-rows, ] Ajustar un modelo en el conjunto de entrenamiento usando un procedimiento de selección de variables apropiado. Crearemos dos modelos para comparar: uno con todas las variables, luego otro con solo las variables elegidas por regsubsets (). full_model &lt;- lm(Salary ~., data = train) select_model &lt;- lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = train) Utilizar ese modelo para predecir en el conjunto de prueba. El rendimiento en el conjunto de prueba es la estimación de CV para el rendimiento fuera de la muestra del modelo. Para ello se puede usar cualquier medida de ajuste, que para los modelos lineales puede ser el rmse (root mean square error). Veremos más adelante otras medidas de ajuste cuando nuestra variable de interés es categórica (e.g. caso/control, responde/no responde, ) rmse &lt;- function(fitted, actual){ sqrt(mean((fitted - actual)^2)) } results &lt;- data.frame(Model = c(&quot;Modelo completo muestra entrenamiento&quot;, &quot;Modelo seleccionado muestra entrenamiento&quot;, &quot;Modelo completo muestra validación&quot;, &quot;Modelo seleccionado muestra validación&quot;), RMSE = round(c(rmse(fitted(full_model), train$Salary), rmse(fitted(select_model), train$Salary), rmse(predict(full_model, newdata = test), test$Salary), rmse(predict(select_model, newdata = test), test$Salary)),1)) results Model RMSE 1 Modelo completo muestra entrenamiento 297.8 2 Modelo seleccionado muestra entrenamiento 326.1 3 Modelo completo muestra validación 368.2 4 Modelo seleccionado muestra validación 306.4 Podemos ver que el modelo completo está sobreajustado  el RMSE dentro de la muestra es mejor que el RMSE fuera de muestra  mientras que el modelo seleccionado elegido por regsubsets () usando BIC no está sobreajustado. De hecho, el modelo seleccionado funciona mucho mejor fuera de la muestra que dentro de la muestra, aunque este resultado en particular es probablemente una cuestión de azar, una función de división aleatoria que estamos usando. Sin embargo, en general, estos resultados ilustran el peligro de la complejidad del modelo y por qué tiene sentido elegir predictores utilizando medidas de ajuste del modelo que penalicen la complejidad. Los modelos simples tienden a generalizar mejor. Esta figura muestra estas relaciones: Sobreajuste 3.2 Leave-one-out cross validation (LOOCV) Este método funciona de la siguiente manera: Extrae una observación de los datos y usa el resto para entrenar el modelo Testa el modelo con la observación que ha sido extraída en el paso anterior y guarda el error asociado a esa predicción Repite el proceso para todos los puntos Calcula el error de predicción global usando el promedio de todos los errores estimados en el paso 2. Veremos más adelante cómo hacer estos cálculos con una libería específica. Aquellos que tengáis un nivel medio/alto de R quizás os podríais plantear este ejercicio (no es obligatorio - pondré la solución en moodle) EJERCICIO (no obligatorio): Crea una función R que lleve a cabo el procedimiento de LOOCV y estima el valor de LOOCV para el modelo completo (full_model) y el modelo seleccionado (select_model) del ejemplo anterior. 3.3 K-fold cross validation (K-fold CV) La diferencia con LOOCV es que este método evalúa el comportamiento del modelo en un conjunto de datos de distingo tamaño (K). El algoritmo es el siguiente: Separa los datos en k-subconjuntos (k-fold) de forma aleatoria Guarda uno de los subconjuntos de datos y entrena el modelo con el resto de individuos Testa el modelo con los datos resevados y guarda el error de predicción promedio. Repite el proceso hasta que los k subconjuntos hayan servido de muestra test. Calcula el promedio de los k errores que han sido guardados. Este valor es el error de cross-validación y nos sirve para evaluar el comportamiento de nuestro modelo como si lo usáramos en una base de datos externa. La principal ventaja de este método respecto a LOOCV es el coste computacional. Otra ventaja que no es tan obvia, es que este método a menudo da mejores estimaciones del error del modelo que LOOCV1. Una pregunta típica es cómo se escoje el valor óptimo de K. Valores pequeños de K da estimaciones sesgadas. Por otro lado, valores grandes de K están menos sesgados, pero tienen mucha variabilidad. En la práctica, normalmente se usan valores de k = 5 or k = 10, ya que estos valores se han mostrado de forma empírica como los que tienen tasas de error estimadas no demasiado sesgadas ni con mucha varianza. Al igual que en el caso anterior veremos unas liberías adecuadas para hacer estos análisis de forma eficiente. De momento se pòdría realizar el siguiente ejercicio (pondré la solución - no es obligatorio): EJERCICIO (no obligatorio): Crea una función R que lleve a cabo el procedimiento de K-fold CV y estima el valor de K-fold CV para el modelo completo (full_model) y el modelo seleccionado (select_model) del ejemplo anterior. Haz que la función tenga un parámetro que dependa de K, y da los resultados para K=5 y K=10. 3.4 Uso de CV para estimar el hiper-parámetro Si el algoritmo de aprendizaje automático que vamos a utilizar para realizar predicciones tiene un parámetro que controla el comportamiento (por ejemplo grado de polinomio en regresión no lineal, o el número de nodos en árboles de clasificación) éste podría elegirse de forma que minimizara el error de clasificación. Esta selección también puede dar problemas de sobre ajuste ya que podríamos seleccionar de forma que ajustara perféctamente a nuestros datos. Para evitar el problema, se puede utilizar cualquiera de las técnicas vistas con anterioridad. Aquí tenemos un ejemplo donde se ha usado un modelo de aprendizaje que se basa en introducir términos polinómicos de varaibles para realizar una mejor predicción mediante regresión lineal usando sólo términos lineales. Sobreajuste según un hiper-parámetro 3.5 Uso de bootstrap Si en vez de partir nuestra muestra en \\(K\\) submuestras, realizamos una selección aleatoria de muestras con reemplazamiento, nos encontraremos ante una aproximación de tipo bootstrap que es una técnica muy usada en estadística para hacer inferencia cuando la distribución del estadístico es desconocida basada en el remuestreo [aquí tenéis una descripción sencilla de esta metodología]. Boostrap Boostrap De manera que el procedimiento bootstrap aplicado a regresión sería: Sacar una muestra aleatoria con remplazamiento de tamaño \\(n\\) de nuestros datos (tenemos \\(n\\) observaciones) Guardar las muestras que no han sido seleccionadas (datos de prueba) Entrena el modelo con la muestra bootstrap Testa el modelo con los datos de prueba y guarda el error de predicción promedio. Repite el proceso \\(B\\) veces Calcula el promedio de los \\(B\\) errores que han sido guardados. Este valor es el error bootstrap y nos sirve para evaluar el comportamiento de nuestro modelo. 3.6 Imputación de datos faltantes (Información extra para los que venís al curso) La mayoría de métodos para aprendizaje automático requiren casos completos. Sin embargo, los datos reales a menudo tienen observaciones faltantes. La función lm (), analiza casos completos sin indicar nada al usuario, pero  ¿Deberíamos eliminar estas filas o imputar las observaciones que faltan? Casi siempre es mejor imputar, aunque, en la práctica puede que no valga la pena imputar algunas observaciones faltantes, ya que eliminarlas no suele cambiar el ajuste en absoluto. La imputación de datos faltantes es un tema extenso y complicado; aquí haremos una breve introducción y discutiremos los principales temas a tener en cuenta. Tipos de valores perdidos: Falta completamente al azar (MCAR por sus siglas en inglés): la probabilidad de que falte una observación es la misma para todos los casos. Eliminar los casos que faltan en esta instancia no causará sesgos, aunque es posible que perdamos información. Missing at random (MAR pos sus siglas en inglés): la probabilidad de que falte una observación depende de un mecanismo conocido. Por ejemplo, es menos probable que algunos grupos respondan encuestas. Si conocemos la pertenencia a un grupo, podemos eliminar las observaciones faltantes siempre que incluyamos el grupo como factor en una regresión. Sin embargo, generalmente podemos hacer algo mejor que simplemente eliminar estos casos. Missing not at random (MNAR por sus siglas en inglés) : la probabilidad de que falte una observación depende de algún mecanismo desconocido  una variable no observada. Tratar los problemas del MNAR es difícil o incluso imposible. Nos centraremos en los problemas MAR. Una solución simple es completar o imputar los valores MAR. Hay dos estrategias principales: Imputación simple reemplaza los valores perdidos según una estadística univariante o un modelo de regresión multivariable. Existen muchas librerías que implementan diferentes métodos (en este curso veremos algunas). En la imputación con medianas imputamos los datos faltantes usando la mediana de la variable que presenta datos faltantes (La mediana es mejor que la media cuando los datos de la columna están sesgados). Podemos imputar también usando KNN o random forest creando un modelo multivariante de las observaciones faltantes usando otras variables y usar ese modelo para predecir los valores faltantes. El problema con la imputación simple, teóricamente, es que la variabilidad de la variable imputada es menor de lo que habría sido la variabilidad en la variable real, creando un sesgo hacia 0 en los coeficientes. Por tanto, mientras que la eliminación pierde información, la imputación única puede provocar sesgos. (Sin embargo, no me queda claro cuán grande es este problema en la práctica). La imputación múltiple aborda estos problemas imputando los valores faltantes con un modelo multivariante, pero agregando la variabilidad de nuevo al volver a incluir la variación del error que normalmente veríamos en los datos. El término múltiple en la imputación múltiple se refiere a los múltiples conjuntos de datos creados en el proceso de estimación de los coeficientes de regresión. Los pasos son los siguientes: Crear \\(m\\) conjuntos de datos completos con valores perdidos imputados. Las imputaciones se realizan extrayendo aleatoriamente distribuciones de valores plausibles para cada vector de columna (variables). Ajustar un modelo lineal en cada conjunto de datos imputados y almacene \\(\\hat \\beta\\)s y SE. Promediar los \\(\\hat \\beta\\)s y combinar los SE para producir coeficientes basados en múltiples conjuntos de datos imputados. Específicamente, \\[\\hat \\beta_ {j} = \\frac {1} {m} \\sum_ {i} \\hat \\beta_ {ij}\\] y \\[s ^ 2_j = \\frac {1} {m} \\sum_{i} s^2_{ij} + var \\hat \\beta_ {ij} (1 + 1 / m),\\] donde \\(\\hat \\beta_{ij}\\) y \\(s_{ij}\\) son las estimaciones y los errores estándar del resultado imputado \\(i^{th}\\) para \\(i=1, ..., m\\) y para el parámetro \\(j^{th}\\). La imputación múltiple funciona mejor para la descripción que para la predicción, y probablemente sea preferible a la imputación única si sólo queremos estimar coeficientes. Para la predicción (como es el caso del aprendizaje automático), normalmente bastará con utilizar imputación simple. Demostraremos métodos de imputación utilizando los datos de Carseats del paquete ISLR. Este es un conjunto de datos simulado de ventas de asientos de coche, del cual eliminaremos aleatoriamente el 25% de las observaciones usando la función prodNA () en el paquete missForest (teniendo cuidado de dejar la variable de resultado, Sales, intacta). library(missForest) data(Carseats, package=&quot;ISLR&quot;) levels(Carseats$ShelveLoc) &lt;- c(&quot;Bad&quot;,&quot;Medium&quot;,&quot;Good&quot;) # Reordenamos los niveles de la variable set.seed(123) carseats_missx &lt;- prodNA(Carseats[,-1], noNA=.25) carseats_miss &lt;- cbind(Sales=Carseats[, 1], carseats_missx) glimpse(carseats_miss) Rows: 400 Columns: 11 $ Sales &lt;dbl&gt; 9.50, 11.22, 10.06, 7.40, 4.15, 10.81, 6.63, 11.85, 6.54, 4.69, 9.01, 11.96, 3.98, 10.96, 11.17, 8.71, 7.58, 12.29, 1~ $ CompPrice &lt;dbl&gt; 138, 111, 113, 117, 141, 124, 115, NA, NA, NA, 121, 117, NA, 115, 107, NA, 118, NA, 110, 129, 125, 134, 128, NA, 145,~ $ Income &lt;dbl&gt; 73, 48, 35, 100, 64, 113, NA, 81, 110, 113, 78, 94, NA, 28, 117, 95, 32, 74, 110, 76, NA, NA, 46, NA, 119, 32, 115, 1~ $ Advertising &lt;dbl&gt; 11, 16, NA, 4, 3, 13, NA, 15, 0, 0, 9, 4, 2, NA, 11, 5, NA, 13, 0, 16, 2, 12, 6, 0, 16, 0, 11, 0, NA, 15, NA, 16, 12,~ $ Population &lt;dbl&gt; 276, 260, 269, NA, 340, 501, 45, 425, 108, 131, 150, 503, NA, 29, 148, 400, 284, 251, 408, 58, 367, 239, 497, 292, 29~ $ Price &lt;dbl&gt; 120, NA, NA, 97, 128, 72, 108, 120, NA, 124, 100, NA, NA, NA, 118, 144, 110, 131, 68, 121, NA, 109, 138, NA, 113, 82,~ $ ShelveLoc &lt;fct&gt; Bad, NA, Good, NA, Bad, Bad, Good, NA, Good, Good, Bad, Medium, NA, Medium, Medium, Good, Medium, Medium, Medium, Goo~ $ Age &lt;dbl&gt; 42, 65, NA, 55, 38, NA, 71, 67, 76, 76, 26, 50, NA, 53, 52, 76, 63, 52, 46, 69, NA, NA, NA, 79, 42, 54, 50, 64, NA, 5~ $ Education &lt;dbl&gt; NA, 10, 12, NA, 13, 16, 15, 10, 10, 17, 10, 13, NA, NA, NA, 18, 13, 10, 17, 12, 18, NA, NA, NA, 12, 11, 11, 17, 11, 1~ $ Urban &lt;fct&gt; NA, Yes, Yes, Yes, Yes, NA, NA, Yes, No, NA, NA, Yes, Yes, Yes, Yes, No, Yes, Yes, No, NA, Yes, No, Yes, NA, Yes, No,~ $ US &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, No, Yes, NA, Yes, Yes, Yes, No, Yes, Yes, No, No, NA, Yes, Yes, NA, Yes, No, No, NA, No,~ Observamos que hay datos faltantes. Podemos tener una estadística global de la falta de información que hay en nuestros tanto de forma numérica mediante la librería skimr: skimr::skim(carseats_miss) Table 3.1: Data summary Name carseats_miss Number of rows 400 Number of columns 11 _______________________ Column type frequency: factor 3 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts ShelveLoc 93 0.77 FALSE 3 Goo: 164, Bad: 73, Med: 70 Urban 101 0.75 FALSE 2 Yes: 207, No: 92 US 104 0.74 FALSE 2 Yes: 193, No: 103 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sales 0 1.00 7.50 2.82 0 5.39 7.49 9.32 16.27  CompPrice 94 0.76 123.88 14.97 77 115.00 123.00 134.00 161.00  Income 100 0.75 68.63 27.85 21 43.50 68.50 90.25 120.00  Advertising 94 0.76 6.70 6.71 0 0.00 5.00 12.00 29.00  Population 107 0.73 268.47 147.29 10 144.00 272.00 402.00 509.00  Price 96 0.76 115.34 24.02 24 100.75 117.00 131.00 191.00  Age 104 0.74 52.88 16.04 25 39.00 54.00 65.00 80.00  Education 107 0.73 13.82 2.63 10 11.00 14.00 16.00 18.00  Comprobamos como la falta de información en las variables (excepto Sales) es de aproximadamente el 25% (columna complete_rate ~ 75%). Pero si tuviéramos que analizar datos de dos o más covariables, el porcentaje de datos completos disminuiría radicalmente. # Total de individuos nrow(carseats_miss) [1] 400 # Total de individuos con casos completos carseats_miss %&gt;% complete.cases() %&gt;% sum() [1] 23 Es decir, si tuviéramos que estimar un modelo con todas las variables de nuestra base de datos sólo dispondríamos de información efectiva para 23 individuos del total de 400. La librería VIM nos puede ayudar a tener esta información de forma gráfica: library(VIM) aggr(carseats_miss) Ahora faltan muchas observaciones. Cuando ajustamos un modelo de regresión para la variable Sales observamos que lm () analiza casos completos y se estima un modelo basado en un subconjunto muy pequeño de datos. lm(Sales ~ CompPrice + Income + Advertising + Population + Price, data = carseats_miss) Call: lm(formula = Sales ~ CompPrice + Income + Advertising + Population + Price, data = carseats_miss) Coefficients: (Intercept) CompPrice Income Advertising Population Price 6.23698 0.10467 0.01373 0.12638 -0.00121 -0.11283 Sólo tenemos 93 observaciones de las 400 originales! Demostraremos la imputación múltiple usando la función mice () de la librería mice. library(mice) names(Carseats) [1] &quot;Sales&quot; &quot;CompPrice&quot; &quot;Income&quot; &quot;Advertising&quot; &quot;Population&quot; &quot;Price&quot; &quot;ShelveLoc&quot; &quot;Age&quot; &quot;Education&quot; [10] &quot;Urban&quot; &quot;US&quot; carseats_imp &lt;- mice(carseats_miss, printFlag = F) El objeto carseats_imp incluye (entre muchas otras cosas) \\(m\\) conjuntos de datos imputados (la configuración predeterminada es \\(m\\) = 5). Los conjuntos de datos imputados difieren porque las imputaciones se extraen aleatoriamente de distribuciones de valores plausibles. Podemos visualizar la variabilidad de los predictores en estos conjuntos de datos imputados usando la función densityplot (). library(lattice) densityplot(carseats_imp) Las líneas azules continuas representan la distribución real de los predictores, mientras que las líneas rojas muestran las distribuciones imputadas. El siguiente paso es usar estos conjuntos de datos imputados para promediar los \\(\\beta\\)s y los SE utilizando la función pool () de la librería mice. carseats_model_imp &lt;- with(data = carseats_imp, exp = lm(Sales ~ CompPrice + Income + Advertising + Population + Price)) mi &lt;- summary(pool(carseats_model_imp)) Estos coeficientes son similares a los del modelo anterior ajustado utilizando los datos no imputados, pero deberían estar más cerca de los valores de la población porque, en lugar de simplemente eliminar los casos incompletos, utiliza información de distribución para hacer suposiciones fundamentadas sobre los datos faltantes. La imputación múltiple funciona mejor para fines de descripción  estimar coeficientes para informar en un artículo académico, por ejemplo  pero usarla para predecir nuevos datos es incómodo o imposible, por las siguientes razones: Si los nuevos datos están completos, podemos utilizar las estimaciones de coeficientes derivadas de la imputación múltiple en una ecuación de regresión para la predicción. Pero esto es difícil ya que hay que hacerlo manualmente. Usamos los datos originales de Carseats como ilustración. preds &lt;- mi[1, 2] + mi[2, 2]*Carseats$CompPrice + mi[3, 2]*Carseats$Income + mi[4, 2]*Carseats$Advertising + mi[5, 2]*Carseats$Population + mi[6, 2]*Carseats$Price head(preds) [1] 9.044928 10.251755 9.775482 8.417813 7.358304 12.816057 Si los nuevos datos no están completos, entonces estos coeficientes imputados son inútiles para predecir en filas con observaciones faltantes. Esto, por ejemplo, es el resultado de intentar predecir utilizando los datos con observaciones faltantes. preds &lt;- mi[1, 2] + mi[2, 2]*carseats_miss$CompPrice + mi[3, 2]*carseats_miss$Income + mi[4, 2]*carseats_miss$Advertising + mi[5, 2]*carseats_miss$Population + mi[6, 2]*carseats_miss$Price head(preds) [1] 9.044928 NA NA NA 7.358304 12.816057 La imputación múltiple, por lo tanto, no resuelve el principal problema al que nos enfrentamos a menudo con los datos faltantes, que es que, aunque hayamos ajustado con éxito un modelo en nuestros datos, el conjunto de validación también puede tener observaciones faltantes, y nuestras predicciones utilizando esos datos puede no poder realizarse. Podríamos usar uno de los conjuntos de datos imputados, pero entonces ya no estamos haciendo imputación múltiple sino imputación simple. En ese momento, los métodos disponibles en el paquete mice ya no ofrecen ninguna ventaja especial sobre los de los paquetes caret y missForest. De hecho, podrían ser peores ya que la función mice () no fue diseñado para producir la mejor imputación individual, sino más bien una gama de imputaciones plausibles. Usando caret, podemos hacer una imputación simple usando knnImpute, medianImpute o bagImpute. Estos métodos solo funcionan para variables numéricas, por lo que crearemos una función personalizada para convertir los factores  Shelveloc, Urban y US  en números enteros. (Al usar el conjunto de datos imputados para la regresión, podríamos dejar estas variables como números enteros, siempre que los valores enteros correspondan a los niveles de los factores). library(caret) make_df_numeric &lt;- function(df){ data.frame(sapply(df, function(x) as.numeric(x))) } carseats_miss_num &lt;- make_df_numeric(carseats_miss) med_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;medianImpute&quot;)), carseats_miss_num) knn_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;knnImpute&quot;)), carseats_miss_num) bag_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;bagImpute&quot;)), carseats_miss_num) El paquete missForest ofrece otra solución de imputación única, que es más simple que las funciones de caret porque maneja datos categóricos automáticamente. Si bien missForest funciona bien para conjuntos de datos pequeños y proporciona imputaciones de buena calidad, es muy lento en conjuntos de datos grandes. De hecho, lo mismo ocurrirá con la función bagImpute () de caret. En tales casos, podría tener sentido usar la función medianImpute () de caret en su lugar que es muy rápida. mf_imp &lt;- missForest(carseats_miss, verbose = F) missForest iteration 1 in progress...done! missForest iteration 2 in progress...done! missForest iteration 3 in progress...done! missForest iteration 4 in progress...done! missForest iteration 5 in progress...done! missForest iteration 6 in progress...done! missForest iteration 7 in progress...done! Comparemos los errores asociados con estos diferentes métodos de imputación. Podemos hacer esto porque, habiendo creado las observaciones faltantes en primer lugar, podemos comparar las observaciones imputadas con las observaciones verdaderas calculando la suma de los cuadrados de la diferencia. Para las imputaciones usando mice () calculamos los errores para cada uno de los 5 conjuntos de datos imputados. Los resultados de knnImpute () no son comparables porque la función automáticamente centra y escala las variables y los hemos omitido. comparison &lt;- data.frame(Method = c(&quot;mice 1&quot;, &quot;mice 2&quot;, &quot;mice 3&quot;, &quot;mice 4&quot;, &quot;mice 5&quot;, &quot;medianImpute&quot;, &quot;bagImpute&quot;, &quot;missForest&quot;), RMSE = c(rmse(make_df_numeric(complete(carseats_imp, 1)), make_df_numeric(Carseats)), rmse(make_df_numeric(complete(carseats_imp, 2)), make_df_numeric(Carseats)), rmse(make_df_numeric(complete(carseats_imp, 3)), make_df_numeric(Carseats)), rmse(make_df_numeric(complete(carseats_imp, 4)), make_df_numeric(Carseats)), rmse(make_df_numeric(complete(carseats_imp, 5)), make_df_numeric(Carseats)), rmse(med_imp, make_df_numeric(Carseats)), rmse(bag_imp, make_df_numeric(Carseats)), rmse(make_df_numeric(mf_imp$ximp), make_df_numeric(Carseats)))) comparison %&gt;% mutate(RMSE = round(RMSE)) %&gt;% arrange(RMSE) Method RMSE 1 mice 1 NA 2 mice 2 NA 3 mice 3 NA 4 mice 4 NA 5 mice 5 NA 6 medianImpute NA 7 bagImpute NA 8 missForest NA missforest obtiene los mejores resultados, aunque medianImpute compara muy bien. Los resultados de mice no son muy buenos, probablemente por las razones mencionadas anteriormente: está diseñado para una imputación múltiple, no simple. James et al. 2014 "],["regresión-logística.html", "4 Regresión logística 4.1 La función logit inversa 4.2 Ejemplo de regresión logística 4.3 Coeficientes de regresión logística como probabilidades 4.4 Coeficientes de regresión logística como razones de odds 4.5 Capacidad predictiva de un modelo de clasificación 4.6 Ejemplo de regresión logística: modelización de riesgo diabetes 4.7 Creación de un modelo y validación 4.8 Nomogramas", " 4 Regresión logística Este capítulo introduce la regresión logística como el método más sencillo para crear modelos predictivos en problemas de clasificación que es el principal objetivo del curso. Se cubrirán los siguientes temas: Conocer la función logística Cómo interpretar los coeficientes de los modelos Cómo evaluar la capacidad predictiva de un modelo Cómo interpretar variables Ilustrar un ejemplo de análisis completo Aprender a hacer nomogramas fijos y dinámicos Hasta ahora, nuestra variable de resultado era continua. Pero si la variable de resultado es binaria (0/1, No/Sí), entonces nos enfrentamos a un problema de clasificación. El objetivo de la clasificación es crear un modelo capaz de clasificar el resultado  y, cuando se usa el modelo para la predicción, nuevas observaciones en una de dos categorías. La regresión logística se introduce en el contexto de la epidemiología como un modelo de regresión que extiende el modelo lineal cuando nuestra variable respuesta es binaria, pero tambié es, probablemente, el método estadístico más utilizado para la clasificación y el más sencillo. Una de las grandes ventajas de estos modelos respecto a otros que veremo más adelante es que este método produce un modelo de probabilidad para nuestra variable resultado. En otras palabras, los valores ajustados en un modelo logístico o logit no son binarios sino que son probabilidades que representan la probabilidad de que el resultado pertenezca a una de nuestras dos categorías. Desafortunadamente, debemos afrontar nuevas complicaciones cuando trabajamos con regresión logística, lo que hace que estos modelos sean inherentemente más difíciles de interpretar que los modelos lineales. Las complicaciones surgen del hecho de que con la regresión logística modelamos la probabilidad de que \\(y\\) = 1, y la probabilidad siempre se escala entre 0 y 1. Pero el predictor lineal, \\(X_i \\beta\\), oscila entre \\(\\pm \\infty\\) (donde \\(X\\) representa todos los predictores del modelo). Esta diferencia de escala requiere transformar la variable de resultado, lo cual se logra con la función logit: \\[ \\text{logit}(x) = \\text{log}\\left( \\frac{x}{1-x} \\right) \\] La función logit asigna el rango del resultado (0,1) al rango del predictor lineal \\((-\\infty, +\\infty)\\). El resultado transformado, \\(\\text{logit} (x)\\), se expresa en logaritmos de probabilidades (\\(\\frac{x}{1-x}\\) se conoce como probabilidades del resultado - razón de odds en inglés - momios en castellano). Así que el modelo también se puede escribir como: \\[\\text{Pr}(y_i = 1) = p_i\\] \\[\\text{logit}(p_i) = X_i\\beta\\] Las probabilidades logarítmicas (e.g. el log-odds) no tienen interpretación (que no sea el signo y la magnitud) y deben transformarse nuevamente en cantidades interpretables, ya sea en probabilidades, usando el logit inverso, o en razones de probabilidades, mediante el uso de la función exponencial. Discutimos ambas transformaciones a continuación. 4.1 La función logit inversa El modelo logístico se puede escribir, alternativamente, usando el logit inverso: \\[ \\operatorname{Pr}(y_i = 1 | X_i) = \\operatorname{logit}^{-1}(X_i \\beta) \\] donde \\(y_i\\) es la respuesta binaria, \\(\\operatorname{logit}^{- 1}\\) es la función logit inversa y \\(X_i \\beta\\) es el predictor lineal. Podemos interpretar esta formulación diciendo que la probabilidad de que \\(y = 1\\) es igual al logit inverso del predictor lineal \\((X_i, \\ beta)\\). Por lo tanto, podemos expresar los valores ajustados del modelo logístico y los coeficientes como probabilidades utilizando la transformación logit inversa. Pero, ¿qué es exactamente el logit inverso? Pues es: \\[\\operatorname{logit}^{-1}(x) = \\frac{e^{x}}{1 + e^{x}}\\] donde \\(e\\) es la función exponencial. Podemos tener una idea de cómo la función logit inversa transforma el predictor lineal mediante una gráfica. Aquí usamos un rango arbitrario de valores de x en (-6, 6) para demostrar la transformación. x &lt;- seq(-6, 6, .01) y &lt;- exp(x)/(1 + exp(x)) ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_line() + ylab(expression(paste(logit^-1,&quot;(x)&quot;))) + ggtitle(expression(paste(&quot;y = &quot;, logit^-1,&quot;(x)&quot;))) Los valores \\(x\\), que van de -6 a 6, son comprimidos por la función logit inversa en el rango 0-1. El logit inverso es curvo, por lo que la diferencia esperada en \\(y\\) correspondiente a una diferencia fija en \\(x\\) no es constante. A valores bajos y valores altos de \\(x\\), un cambio de unidad corresponde a un cambio muy pequeño en \\(y\\), mientras que en la mitad de la curva un pequeño cambio en \\(x\\) corresponde a un cambio relativamente grande en \\(y\\). En la regresión lineal, la diferencia esperada en \\(y\\) correspondiente a una diferencia fija en \\(x\\) es, por el contrario, constante. Por lo tanto, cuando interpretamos los resultados logísticos debemos elegir en qué parte de la curva queremos evaluar la probabilidad del resultado, dado el modelo. 4.2 Ejemplo de regresión logística Ilustremos estos conceptos utilizando el conjunto de datos Default del ISLR. Este conjunto de datos simulado contiene una variable binaria que representa el incumplimiento en los pagos de la tarjeta de crédito (variable default), que modelaremos como una función de la variable balance (la cantidad de deuda que tiene la tarjeta) y la variable income (ingresos). Los datos pueden obtenerse mediante library(ISLR) data(Default) str(Default) &#39;data.frame&#39;: 10000 obs. of 4 variables: $ default: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ student: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 2 1 1 ... $ balance: num 730 817 1074 529 786 ... $ income : num 44362 12106 31767 35704 38463 ... Antes de Primero visualizaremos cómo es esta relación. library(gridExtra) ggplot(Default, aes(x = balance, y = income, col = default)) + geom_point(alpha = .4) + ggtitle(&quot;Balance vs. Income by Default&quot;) grid.arrange( ggplot(Default, aes(default, balance)) + geom_boxplot() + ggtitle(&quot;Balance by Default&quot;) + ylab(&quot;balance&quot;), ggplot(Default, aes(default, income)) + geom_boxplot() + ggtitle(&quot;Income by Default&quot;) + ylab(&quot;income&quot;), ncol = 2) Claramente, los valores altos de saldo están asociados con el incumplimiento en todos los niveles de ingresos. Esto sugiere que los ingresos en realidad no son un fuerte predictor de incumplimiento, en comparación con el saldo, que es exactamente lo que vemos en los diagramas de cajas. Exploremos estas relaciones usando la regresión logística. En R ajustamos un modelo logístico usando la función glm () con family = binomial.2 Centraremos y escalaremos las variables para facilitar la interpretación. glm(default ~ balance + income, data = Default, family = binomial) %&gt;% standardize %&gt;% display glm(formula = default ~ z.balance + z.income, family = binomial, data = Default) coef.est coef.se (Intercept) -6.13 0.19 z.balance 5.46 0.22 z.income 0.56 0.13 --- n = 10000, k = 3 residual deviance = 1579.0, null deviance = 2920.6 (difference = 1341.7) NOTA: Se puede apreciar la ventaja del uso de tidyverse (pipe) - no se necesita crear las variables estandarizadas, ni guardar el resultado para luego hacer un print Interpretamos esta salida exactamente como lo haríamos para un modelo lineal con un predictor centrado y escalado: Intercept: -6,13 representa las probabilidades logarítmicas (log odds) de incumplimiento cuando el saldo es promedio (835.37) y el ingreso es promedio (3.351698^{4} ). (Promedio porque las variables se han centrado). z.balance: 5.46 representa el cambio predicho en las probabilidades logarítmicas de incumplimiento asociado con un aumento de 1 unidad en el saldo (z.balance), manteniendo constante el ingreso (z.income). Un aumento de 1 unidad en el saldo (z.balance) es equivalente a un aumento de 2 desviaciones estándar en el saldo (balance) (967.43). Este coeficiente es estadísticamente significativo ya que 5.46 - 2 x .22 &gt; 0 (el IC del 95% que no contiene 0 indica significación estadística). z.income: .56 representa el cambio predicho en las probabilidades logarítmicas (log odds) de incumplimiento asociadas con un aumento de 1 unidad en el ingreso (z.income), manteniendo constante el balance (z.balance). Un aumento de 1 unidad en el ingreso (z.income) es equivalente a un aumento de 2 desviaciones estándar en el ingreso (income) (2.667328^{4}). Este coeficiente también es estadísticamente significativo ya que .56 - 2 x .13 &gt; 0. ¿Qué significa que las probabilidades logarítmicas de incumplimiento aumenten en 5.46 o .56? En términos precisos, ¿quién sabe? Para que estas cantidades tengan una mejor interpretación, necesitamos transformarlas, ya sea en probabilidades (odds) o en razones de probabilidades (razón de odds -&gt; odds ratio). Sin embargo, debemos señalar que el signo y la magnitud de los coeficientes si son informativas: la relación con el incumplimiento del pago es positiva en ambos casos y, como ya se había visto de forma gráfica en los diagramas de cajas, el efecto del saldo (balance) es mucho mayor que el del ingreso (income). 4.3 Coeficientes de regresión logística como probabilidades Podemos dar una interpretación más específica de la regresión logística más allá del efecto y magnitud. Para ello, podemos usar la función logit inversa para convertir las probabilidades logarítmicas (log-odds) de incumplimiento de pago en las tarjetas (cuando el saldo y los ingresos son promedio) en una probabilidad: invlogit &lt;- function(x) exp(x)/(1 + exp(x)) invlogit(-6.13 + 5.46 * 0 + .56 * 0) [1] 0.002171854 La probabilidad de incumplimiento para aquellos con un saldo promedio de tarjeta de crédito de (835.37) y un ingreso promedio de (3.351698^{4}) es de hecho bastante bajo: solo 0.002. Asimismo, podemos calcular el cambio en la probabilidad de incumplimiento en el pago asociado con un aumento de 1 unidad en el saldo, manteniendo el ingreso constante en el promedio (z.ingreso=0). Esto equivaldría a aumentar el saldo en casi 1000$, de 835.37 a 1802.8. invlogit(-6.13 + 5.46 * 1) - invlogit(-6.13 + 5.46 * 0) [1] 0.336325 4.4 Coeficientes de regresión logística como razones de odds También podemos interpretar los coeficientes de regresión logística como razones de odds (OR).3 Si dos resultados tienen probabilidades \\((p, 1-p)\\), entonces \\(\\frac {p} {1-p}\\) se conoce como odds (probabilidades o momio) del resultado. Las odds son simplemente diferentes formas de representar la misma información: \\(\\text{odds} = \\frac{p}{1-p}\\) y \\(p = \\frac{\\text{odds}} {1+ \\text{odds}}\\). Por ejemplo, una odds de 1 es equivalente a una odds de .5  es decir, resultados igualmente probables para \\(p\\) y \\(1-p\\): \\(\\text{odds(p = .5)} = \\frac{.5}{1-.5} = 1\\) y \\(p(\\text{oods} = 1) = \\frac{\\text{1}}{1 + 1} = .5.\\) La razón de dos odds es una OR: \\[ \\frac{\\frac{p_2}{1-p_2}}{\\frac{p_1}{1-p_1}} \\] Una razón de odds se puede interpretar como un cambio en la probabilidad. Por ejemplo, un cambio en la probabilidad de \\(p_1 = .33\\) a \\(p_2 = .5\\) da como resultado un OR de 2, de la siguiente manera: \\[ \\frac{\\frac{.5}{.5}}{\\frac{.33}{.66}} = \\frac{1}{.5} = 2 \\] También podemos interpretar el OR como el aumento porcentual de las probabilidades de un evento. Aquí, un OR de 2 equivale a aumentar las probabilidades en un 100%, de 0,5 a 1. Recordemos que representamos el modelo logit de esta manera: \\[ \\text{log} \\left(\\frac{p}{1-p}\\right) = \\alpha + \\beta x \\] La parte izquierda de la ecuación, expresado como logaritmos de probabilidades, está en la misma escala que la derecha derecho: \\(\\pm \\infty\\). Por lo tanto, no hay no linealidad en esta relación, y aumentar \\(x\\) en 1 unidad tiene el mismo efecto que en la regresión lineal: cambia el resultado predicho en \\(\\beta\\). Entonces, pasar de \\(x\\) a \\(x + 1\\) equivale a sumar \\(\\beta\\) a ambos lados de la ecuación anterior. Centrándonos solo en el lado izquierdo, tenemos, después de exponenciar, las probabilidades originales multiplicadas por \\(e^\\beta\\): \\[ e^{\\text{log} \\left(\\frac{p}{1-p}\\right) + \\beta} = \\frac{p}{1-p} *e^ \\beta \\] (ya que \\(e^{a+b} = e^a*e^b\\)). Podemos pensar en \\(e^\\beta\\) como el cambio de la odds del resultado cuando \\(x\\) cambia en 1 unidad, que se puede representar, utilizando la formulación anterior, como una OR: \\[ \\frac{\\frac{p_2}{1-p_2}}{\\frac{p_1}{1-p_1}} = \\frac{\\frac{p_1}{1-p_1} * e^\\beta }{\\frac{p_1}{1-p_1}} = e^\\beta. \\] Por lo tanto, \\(e^\\beta\\) se puede interpretar como el cambio en las probabilidades asociadas con un aumento de 1 unidad en \\(x\\), expresado en términos porcentuales. En el caso de OR = \\(\\frac{1}{. 5} = 2\\), el porcentaje de aumento en las probabilidades del resultado es del 100%. Cuando la OR es \\(&gt;2\\) se suele expresar como x-veces más (OR=3.5, hay 3.5 veces más probabilidad de observar el evento que no observarlo cuando se cambia \\(x\\) en 1 unidad), y cuando la OR es \\(&lt;1\\) se suele hablar de protección a no tener el evento y el porcentaje se calcula como 1-OR. Apliquemos esta información a nuestro modelo anterior de aplicando la exponencial a los coeficientes de saldo e ingresos: exp(5.46) [1] 235.0974 exp(.56) [1] 1.750673 Podemos interpretar estos ORs como el porcentaje o cambio multiplicativo en las probabilidades asociadas con un aumento de 1 unidad en el predictor (mientras se mantienen constantes los demás), de 1 a 235 (un aumento de 23,400%) en el caso de balance, y de 1 a 1,75 (un aumento del 75%) para los ingresos. Por ejemplo, podemos decir que la probabilidad de incumplimiento es un 75% mayor cuando los ingresos aumentan en 1 unidad. Cuando las variables predictoras son categóricas (como en biomedicina: sexo, estadío tumoral, fumar, beber, ) la interpretación se hace más sencilla porque el cambio de 1 unidad en estas variables, supone el cambio de una categoría respecto a la basal (ya que se usan dummy variables). Así, por ejemplo, si nuestro outcomes tener cáncer de pulmón o no, y nuestro predictor es ser fumanor o no, si nuestros análisis nos dan una OR de 6 asociado a ser fumador, la interpretación sería: \"La odds (probabilidad, abusando de lenguaje - también riesgo si el outcome es poco frecuente) de sufrir cáncer de pulmón es 6 veces mayor en los fumadores que en los no fumadores. 4.5 Capacidad predictiva de un modelo de clasificación Podemos evaluar el rendimiento (es decir, la capacidad predictiva) del modelo logístico utilizando el AIC, así como mediante el uso de otras medidas como: la desviación (deviance) residual, la precisión, la sensibilidad, la especificidad y el área bajo la curva (AUC). Al igual que AIC, la deviance es una medida de error, por lo que una deviance más baja significa un mejor ajuste a los datos. Esperamos que la desviación disminuya en 1 para cada predictor, por lo que con un predictor informativo (e.g. variable imporante para el modleo), la deviance disminuirá en más de 1. Deviance = \\(-2ln(L)\\), donde \\(ln\\) es el logaritmo natural y \\(L\\) es la función de verosimilitud . Veámoslo con nuestro ejemplo: logistic_model1 &lt;- glm(default ~ balance, data = Default, family = binomial) logistic_model1$deviance [1] 1596.452 logistic_model2 &lt;- glm(default ~ balance + income, data = Default, family = binomial) logistic_model2$deviance [1] 1578.966 En este caso, la deviance se redujo en más de 1, lo que indica que los ingresos mejoran el ajuste del modelo. Podemos hacer una prueba formal de la diferencia usando, como para los modelos lineales, la prueba de razón de verosimilitud: lmtest::lrtest(logistic_model1, logistic_model2) Likelihood ratio test Model 1: default ~ balance Model 2: default ~ balance + income #Df LogLik Df Chisq Pr(&gt;Chisq) 1 2 -798.23 2 3 -789.48 1 17.485 2.895e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 También podemos traducir las probabilidades de un modelo logístico para el incumplimiento del pago en predicciones de clase asignando Sí (predeterminado) a probabilidades mayores o iguales a .5 y No (sin valor predeterminado) a probabilidades menores que .5, y luego contar el número de veces que el modelo asigna la clase predeterminada correcta. Si dividimos este número por el total de observaciones, habremos calculado la precisión. La precisión se utiliza a menudo como medida del rendimiento del modelo. preds &lt;- ifelse(fitted(logistic_model2) &gt;= .5, &quot;Yes&quot;, &quot;No&quot;) (length(which(preds == Default$default)) / nrow(Default))*100 [1] 97.37 El modelo es 97.37% preciso. Valores superirores al 50% mostrarían una mejora en la predicción ya que por azar, se espera que el modelo tenga una precisión del 50%. Una forma sencilla de obtener un buen modelo de clasificación sería asignar a todos la categoría más frecuente. Por ejemplo, en nuestros datos, la clase mayoritaria es No para la variable incimpliminto por un amplio margen (9667 a 333). La mayoría de las personas no incumplen. ¿Cuál es nuestra precisión, entonces, si simplemente predecimos No para todos los casos? La proporción de No en los datos es 96.67%, por lo que si siempre predijimos No esa sería nuestra precisión (9667 / (9667 + 333) = .9667). El modelo logístico, sorprendentemente, ofrece solo una ligera mejora. Sin embargo, al evaluar el rendimiento del clasificador, debemos reconocer que no todos los errores son iguales y que la precisión tiene limitaciones como métrica de rendimiento. En algunos casos, el modelo puede haber predicho el incumplimiento cuando no lo había. Esto se conoce como falso positivo. En otros casos, el modelo puede haber predicho que no hubo incumplimiento cuando hubo incumplimiento. Esto se conoce como falso negativo. En la clasificación, utilizamos lo que se conoce como matriz de confusión para resumir estos diferentes tipos de errores del modelo, denominados así porque la matriz resume cómo se confunde el modelo. Usaremos la función confusionMatrix () de la librería caret para calcular rápidamente estos valores: confusionMatrix(as.factor(preds), Default$default, positive = &quot;Yes&quot;) Confusion Matrix and Statistics Reference Prediction No Yes No 9629 225 Yes 38 108 Accuracy : 0.9737 95% CI : (0.9704, 0.9767) No Information Rate : 0.9667 P-Value [Acc &gt; NIR] : 3.067e-05 Kappa : 0.4396 Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 Sensitivity : 0.3243 Specificity : 0.9961 Pos Pred Value : 0.7397 Neg Pred Value : 0.9772 Prevalence : 0.0333 Detection Rate : 0.0108 Detection Prevalence : 0.0146 Balanced Accuracy : 0.6602 &#39;Positive&#39; Class : Yes Esta función produce una gran cantidad de resultados. Podemos ver que informa la misma precisión que calculamos anteriormente: .97. La matriz de confusión 2 x 2 está en la parte superior. Podemos caracterizar estos 4 valores en la matriz de la siguiente manera: 9629 negativos verdaderos (TN): cuando el modelo predice correctamente No 108 verdaderos positivos (TP): cuando el modelo predice correctamente Sí 225 falsos negativos (FN): cuando el modelo predice incorrectamente No 38 falsos positivos (FP): cuando el modelo predice incorrectamente Sí La siguiente tabla resume estas posibilidades: Reference Predicted No Yes No TN FN Yes FP TP Hay dos medidas clave, además de la precisión, para caracterizar el rendimiento del modelo. Mientras que la precisión mide el error general, la sensibilidad y la especificidad miden errores específicos de clase. Precisión = 1 - (FP + FN) / Total: 1 - (38 + 225) / 10000 [1] 0.9737 Sensibilidad (o la tasa de verdaderos positivos): TP / (TP + FN). En este caso, la sensibilidad mide la proporción de incumplimientos que se clasificaron correctamente como tales. 108 / (108 + 225) [1] 0.3243243 Especificidad (o la tasa de verdaderos negativos): TN / (TN + FP). En este caso, la especificidad mide la proporción de no incumplimientos que se clasificaron correctamente como tales. 9629 / (9629 + 38) [1] 0.9960691 ¿Por qué deberíamos considerar estos errores específicos de clase? Todos los modelos tienen errores, pero no todos los errores del modelo son igualmente importantes. Por ejemplo, un falso negativo  prediciendo incorrectamente que un prestatario no incurrirá en incumplimiento  puede ser un error costoso para un banco, si el incumplimiento se hubiera podido prevenir mediante la intervención. Pero, por otro lado, un falso positivo, que predice incorrectamente que un prestatario incurrirá en incumplimiento, puede desencadenar una advertencia innecesaria que irrita a los clientes. Los errores que comete un modelo se pueden controlar ajustando el umbral de decisión utilizado para asignar probabilidades predichas a las clases. Usamos un umbral de probabilidad de .5 para clasificar los incumplimientos en los pagos. Si el umbral se establece en .1, por el contrario, la precisión general disminuiría, pero también lo haría el número de falsos negativos, lo que podría ser deseable. El modelo luego atraparía a más morosos, lo que ahorraría dinero al banco, pero eso tendría un costo: más falsos positivos (clientes potencialmente irritados). preds &lt;- as.factor(ifelse(fitted(logistic_model2) &gt;= .1, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, Default$default)$table Reference Prediction No Yes No 9105 90 Yes 562 243 La pregunta de cómo establecer el umbral de decisión . 5, .1 o algo más  debe responderse con referencia al contexto empresarial. Una curva de característica operativa del receptor (ROC por sus siglas en ingles) visualiza las compensaciones entre los tipos de errores trazando la especificidad frente a la sensibilidad. El cálculo del área bajo la curva ROC (AUC) nos permite, además, resumir el rendimiento del modelo y comparar modelos. La curva en sí muestra los tipos de errores que cometería el modelo en diferentes umbrales de decisión. Para crear una curva ROC usamos la función roc () del paquete pROC, y mostramos los valores de sensibilidad / especificidad asociados con los umbrales de decisión de .1 y .5: library(pROC) library(plotROC) invisible(plot(roc(factor(ifelse(Default$default == &quot;Yes&quot;, 1, 0)), fitted(logistic_model2)), print.thres = c(.1, .5), col = &quot;red&quot;, print.auc = T)) Un modelo con una precisión del 50%, es decir, un clasificador aleatorio, tendría una curva ROC que siguiera la línea de referencia diagonal. Un modelo con una precisión del 100%, un clasificador perfecto, tendría una curva ROC siguiendo los márgenes del triángulo superior. Cada punto de la curva ROC representa un par de sensibilidad / especificidad correspondiente a un umbral de decisión particular. Cuando establecimos el umbral de decisión en .1, la sensibilidad fue .73 (243 / (243 + 90)) y la especificidad fue .94 (9105 / (9105 + 562)). Ese punto se muestra en la curva. Del mismo modo, cuando establecimos el umbral de decisión en .5, la sensibilidad fue .32 y la especificidad fue .996. Ese punto también está en la curva. ¿Qué umbral de decisión es óptimo? Nuevamente, depende del problema (pensad en cáncer o en este ejemplo de dinero). Las curvas ROC nos permiten elegir los errores específicos de clase que podemos cometer. El AUC es el resumen de cómo funciona el modelo en cada umbral de decisión. En general, los modelos con AUC más altos son mejores. Esta medida nos servirá para comparar métodos de aprendizaje automático que iremos aprendiendo durante el curso. 4.6 Ejemplo de regresión logística: modelización de riesgo diabetes Para este ejemplo usaremos el conjunto de datos Pima, incluido en la librería MASS que contienen esta información: Una población de mujeres que tenían al menos 21 años, de ascendencia indígena Pima y que vivían cerca de Phoenix, Arizona, se sometieron a pruebas de diabetes de acuerdo con los criterios de la Organización Mundial de la Salud. Los datos fueron recopilados por el Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales de EE. UU. Usaremos información para 532 mujeres con datos completos después de eliminar los datos (principalmente faltantes) sobre la insulina sérica. El conjunto de datos incluye las siguientes variables: npreg: número de embarazos glu: concentración de glucosa en plasma a 2 horas en una prueba de tolerancia oral a la glucosa bp: presión arterial diastólica (mm Hg) piel: grosor del pliegue cutáneo del tríceps (mm) bmi: índice de masa corporal (peso en kg / (altura en m) ^ 2) ped: función del pedigrí de la diabetes age: Edad (años) type: Sí o No (diabetes) La variable resultado es type, que indica si una persona tiene diabetes. Los datos están divididos en un conjunto de entrenamiento y otro test que combinaremos para ilustrar este ejemplo. library(arm) library(MASS) data(&quot;Pima.tr&quot;) data(&quot;Pima.te&quot;) d &lt;- rbind(Pima.te, Pima.tr) str(d) &#39;data.frame&#39;: 532 obs. of 8 variables: $ npreg: int 6 1 1 3 2 5 0 1 3 9 ... $ glu : int 148 85 89 78 197 166 118 103 126 119 ... $ bp : int 72 66 66 50 70 72 84 30 88 80 ... $ skin : int 35 29 23 32 45 19 47 38 41 35 ... $ bmi : num 33.6 26.6 28.1 31 30.5 25.8 45.8 43.3 39.3 29 ... $ ped : num 0.627 0.351 0.167 0.248 0.158 0.587 0.551 0.183 0.704 0.263 ... $ age : int 50 31 21 26 53 51 31 33 27 29 ... $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 2 2 1 1 2 ... Todos los predictores son enteros o numéricos. Nuestro objetivo es construir un modelo logístico de diabetes para ilustrar cómo interpretar los coeficientes del modelo. 4.6.1 Modelo simple Comencemos con un modelo simple. bin_model1 &lt;- glm(type ~ bmi + age, data = d, family = binomial) display(bin_model1) glm(formula = type ~ bmi + age, family = binomial, data = d) coef.est coef.se (Intercept) -6.26 0.67 bmi 0.10 0.02 age 0.06 0.01 --- n = 532, k = 3 residual deviance = 577.2, null deviance = 676.8 (difference = 99.6) Intercept: -6.26 es el logaritmo de la probabilidad de tener diabetes cuando bmi = 0 y edad = 0. Dado que ni la edad ni el bmi pueden ser iguales a 0, el intercept no es interpretable en este modelo. Por tanto, tendría sentido centrar las variables para facilitar la interpretación. bmi: .1 es el cambio previsto en el log-odds de diabetes asociado con un aumento de 1 unidad en el bmi, manteniendo la edad constante. Este coeficiente es estadísticamente significativo ya que .1 - 2 x .02 &gt; 0. (Un IC del 95% que no contiene 0 indica significancia estadística) y también porque su p-valor asociacio mediante el test de score es \\(&lt;0.05\\) (usar la función summary () . Podemos traducir este coeficiente en un OR mediante la exponencial: \\(e^.1\\) = 1.11. Un aumento de 1 unidad en el IMC, manteniendo la edad constante, se asocia con un aumento del 11% en la odds (o, más coloquialmente, la probabilidad) de diabetes. edad: .06 es el cambio predicho en el log-oods de diabetes asociado con un aumento de 1 unidad en la edad, manteniendo constante el bmi. Este coeficiente también es estadísticamente significativo ya que .06 - 2 x .01&gt; 0. El OR para la edad es \\(e^.06\\) = 1.06 lo que indica un aumento del 6% en la probabilidad de sufrir diabetes asociada con un aumento de 1 unidad en la edad. 4.6.2 Agregar predictores y evaluar el ajuste Ahora ajustaremos un modelo completo (excluyendo skin, ya que parece medir casi lo mismo que bmi). ¿Mejora el ajuste? bin_model2 &lt;- glm(type ~ bmi + age + ped + glu + npreg + bp , data = d, family = binomial) display(bin_model2) glm(formula = type ~ bmi + age + ped + glu + npreg + bp, family = binomial, data = d) coef.est coef.se (Intercept) -9.59 0.99 bmi 0.09 0.02 age 0.03 0.01 ped 1.31 0.36 glu 0.04 0.00 npreg 0.12 0.04 bp -0.01 0.01 --- n = 532, k = 7 residual deviance = 466.5, null deviance = 676.8 (difference = 210.3) La deviance disminuye de 577 en el modelo anterior a 466.5 en este modelo, muy por encima de los 4 puntos que debería bajar simplemente al incluir 4 predictores adicionales. Además, el LRT nos indica que estas diferencias son estadísticamente significativas: lmtest::lrtest(bin_model1, bin_model2) Likelihood ratio test Model 1: type ~ bmi + age Model 2: type ~ bmi + age + ped + glu + npreg + bp #Df LogLik Df Chisq Pr(&gt;Chisq) 1 3 -288.60 2 7 -233.27 4 110.67 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El segundo modelo es mucho mejor que el primero, lo que también es evidente cuando observamos las matrices de confusión (con el umbral de decisión establecido en .5) preds &lt;- as.factor(ifelse(fitted(bin_model1) &gt; .5, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, d$type)$table Reference Prediction No Yes No 312 112 Yes 43 65 preds &lt;- as.factor(ifelse(fitted(bin_model2) &gt; .5, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, d$type)$table Reference Prediction No Yes No 318 73 Yes 37 104 Como era de esperar, el modelo completo comete menos errores. Podemos formalizar esta impresión calculando y comparando la precisión del modelo: 1 - (112 + 43) / (112 + 43 + 312 + 65) = 0.71 para el primer modelo, en comparación con 1 - (73 + 37) / (73 + 37 + 318 + 104) =r round (1 - (73 + 37) / (73 + 37 + 318 + 104 ), 2) para el segundo modelo más grande. Los números de verdaderos negativos son cercanos, pero el modelo más grande aumenta sustancialmente el número de verdaderos positivos y reduce el número de falsos negativos, prediciendo incorrectamente que una persona no tiene diabetes (aunque esta sigue siendo la clase de error más grande). Deberíamos comprobar para ver que estos modelos mejoran la precisión sobre un modelo que siempre predice la clase mayoritaria. En este conjunto de datos, No es la categoría mayoritaria con 66,7%. Entonces, si siempre predijimos No, estaríamos en lo correcto el 66.7% de las veces, que es una precisión menor que cualquiera de los modelos. Las curvas ROC proporcionan una descripción más sistemática del rendimiento del modelo en términos de errores de clasificación errónea. invisible(plot(roc(d$type, fitted(bin_model1)), col = &quot;red&quot;, main = &quot;ROC curves: logistic model 1 (red) vs. logistic model 2 (blue)&quot;)) invisible(plot(roc(d$type, fitted(bin_model2)), print.auc = T, col = &quot;blue&quot;, add = T)) El modelo más grande es claramente mejor: el AUC es más alto. 4.6.3 Análisis de interacciones Agreguemos una interacción entre dos predictores continuos, ped y glu. Centraremos y escalaremos las entradas para que el coeficiente de la interacción sea interpretable. bin_model3 &lt;- standardize(glm(type ~ bmi + ped * glu + age + npreg + bp , data = d, family = binomial)) display(bin_model3) glm(formula = type ~ z.bmi + z.ped * z.glu + z.age + z.npreg + z.bp, family = binomial, data = d) coef.est coef.se (Intercept) -1.02 0.13 z.bmi 1.29 0.26 z.ped 1.02 0.24 z.glu 2.31 0.27 z.age 0.53 0.30 z.npreg 0.87 0.29 z.bp -0.19 0.26 z.ped:z.glu -1.15 0.41 --- n = 532, k = 8 residual deviance = 460.0, null deviance = 676.8 (difference = 216.7) La interacción mejora el modelo pero no cambia la imagen general del ajuste del modelo en el gráfico de residuos agrupados (no incluido). Intercept : -1.02 es el log-odds de diabetes cuando todos los predictores son promedio (ya que hemos centrado y escalado las entradas). La probabilidad de tener diabetes para la mujer promedio en este conjunto de datos es logit\\(^{- 1}\\) (- 1.02) = 0.27. z.bmi: 1.29 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.bmi, manteniendo constantes los demás predictores. \\(e^{1.29}\\) = 3.63 por lo que un aumento de 1 unidad en z.bmi, manteniendo constantes los otros predictores, se asocia con un aumento del 263% en la probabilidad de sufrir diabetes. z.ped: 1.02 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.ped, cuando z.glu = 0 y manteniendo constantes los otros predictores. \\(e^{1.02}\\) = 2.77 por lo que un aumento de 1 unidad en z.ped, cuando z.glu = 0 y manteniendo los otros predictores constantes, se asocia con un aumento del 177% en la probabilidad de sufrir diabetes. z.glu: 2.31 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.glu, cuando z.ped = 0 y manteniendo constantes los demás predictores. \\(e^{2.31}\\) = 10.07 por lo que un aumento de 1 unidad en z.glu, cuando z.ped = 0 y manteniendo los otros predictores constantes, se asocia con un aumento del 907% en la probabilidad de sufrir diabetes. . el resto de predictores igual z.ped:z.glu : se añade -1.15 al log-odds de diabetes de z.ped cuando z.glu aumenta en 1 unidad, manteniendo constantes los otros predictores. O, alternativamente, se añade -1.15 al log-odds de diabetes de z.glu por cada unidad adicional de z.ped. Calculamos el OR, como en los otros casos, exponenciando: \\(e^ {- 1.15}\\) = 0.32. El OR para z.ped disminuye en un 68% (1 - .32 = .68) cuando z.glu aumenta en 1 unidad, manteniendo constantes los demás predictores. O, alternativamente, el OR para z.glu disminuye en un 68% con cada unidad adicional de z.ped. 4.6.4 Gráfico de la interacción Como siempre, debemos visualizar la interacción para comprenderla. Esto es especialmente necesario cuando las relaciones se expresan en términos de probabilidades logarítmicas y razones de probabilidades. Como hemos hecho anteriormente para fines de visualización, dicotomizaremos los predictores en la interacción y, en este caso, para facilitar la interpretación, presentaremos las relaciones en términos de probabilidades. El propósito de los gráficos es la comprensión y la ilustración, por lo que no nos preocupa demasiado la precisión estadística. Resumiremos las relaciones usando una curva loess (estimación no paramétrica de la regresión) para capturar la no linealidad del efecto del predictor (\\(\\pm \\infty\\)) al rango del resultado binario (0, 1). d$ped_bin &lt;- ifelse(d$ped &gt; mean(d$ped), &quot;above avg&quot;,&quot;below avg&quot;) d$glu_bin &lt;- ifelse(d$glu &gt; mean(d$glu), &quot;above avg&quot;,&quot;below avg&quot;) d$prob &lt;- fitted(bin_model3) d$type_num &lt;- ifelse(d$type == &quot;Yes&quot;, 1, 0) ggplot(d, aes(glu, type_num)) + geom_point() + stat_smooth(aes(glu, prob, col = ped_bin), se = F) + labs(y = &quot;Pr(diabetes)&quot;, title = &quot;Diabetes ~ glu varying by ped_bin&quot;) La relación entre glu y diabetes depende claramente de los niveles de ped. Como en el caso lineal, las líneas no paralelas indican una interacción. El coeficiente de log-odds negativo para la interacción del modelo indica que a medida que ped aumenta, la fuerza de la relación entre glu y type (diabetes) disminuye. Esto es exactamente lo que vemos en este gráfico: ggplot(d, aes(ped, type_num)) + geom_point() + stat_smooth(aes(ped, prob, col = glu_bin), se = F) + labs(y = &quot;Pr(diabetes)&quot;, title = &quot;Diabetes ~ ped varying by glu_bin&quot;) La interacción es más difícil de ver aquí porque la escala de ped está comprimida, con la mayoría de las observaciones cercanas a 0. Sin embargo, podemos ver que a medida que glu aumenta, la fuerza de la relación entre ped y diabetes disminuye. Nuevamente, las líneas no paralelas indican la presencia de una interacción. 4.6.5 Uso del modelo para predecir probabilidades El tamaño del efecto más grande en el modelo anterior con la interacción, con mucho, es z.glu. Por tanto, para comunicar los resultados de este modelo deberíamos concentrarnos en glu. Pero los coeficientes expresados como logaritmos de probabilidades son algo confusos y, lamentablemente, las razones de probabilidades no ayudan a aclarar mucho las cosas. Deberíamos ir al trabajo adicional de traducir los coeficientes del modelo en probabilidades, pero para hacerlo debemos identificar en qué parte de la curva de probabilidad nos gustaría evaluar glu. Escogeremos la región cercana al promedio de z.glu  0  y examinaremos el efecto de aumentar z.glu en 1 unidad (que es igual a 2 desviaciones estándar de glu) cuando los otros predictores son promedio. La forma más sencilla de hacer esto es crear una base de datos con los valores de predicción deseados para usar con la función predict (). basal &lt;- data.frame(z.bmi = 0, z.ped = 0, z.glu = 0, z.age = 0, z.npreg = 0, z.bp = 0) glucosa &lt;- data.frame(z.bmi = 0, z.ped = 0, z.glu = 1, z.age = 0, z.npreg = 0, z.bp = 0) (lo &lt;- as.numeric(invlogit(predict(bin_model3, newdata = basal)))) [1] 0.2652028 (hi &lt;- as.numeric(invlogit(predict(bin_model3, newdata = )))) [1] 0.716146591 0.025911688 0.017983270 0.029013744 0.893018807 0.665405710 0.395033985 0.211066662 0.431695033 0.225375587 0.032840692 [12] 0.443676401 0.022607120 0.418795847 0.194085484 0.411388311 0.122928759 0.771984610 0.760725696 0.088470567 0.953485181 0.785645203 [23] 0.028409690 0.026232726 0.055457646 0.038181372 0.757850071 0.007630220 0.110507313 0.066603557 0.213559028 0.017048355 0.294579371 [34] 0.213686248 0.245378175 0.147939308 0.058529536 0.047092679 0.169073996 0.023421517 0.434375748 0.123923484 0.796680824 0.257623355 [45] 0.236877492 0.032937565 0.075796893 0.459769771 0.014969537 0.214416331 0.409898562 0.024597682 0.736084480 0.071421984 0.863089942 [56] 0.109813203 0.415148971 0.192888308 0.671705524 0.707181753 0.222687051 0.074378276 0.047288218 0.250211485 0.102873418 0.451885105 [67] 0.016412055 0.839500999 0.593725195 0.887724350 0.023636511 0.972427462 0.421875342 0.240818110 0.193220347 0.275433738 0.029285367 [78] 0.858427986 0.792694816 0.428056807 0.241389504 0.359146427 0.159848949 0.126229936 0.089106804 0.965646716 0.021565093 0.504101659 [89] 0.255700597 0.303575110 0.316087819 0.695623695 0.059157940 0.023850831 0.703188586 0.768676985 0.198583192 0.736445668 0.019832290 [100] 0.911158959 0.794437117 0.023391174 0.082335119 0.536540158 0.491636172 0.875915114 0.830550408 0.453101254 0.050024142 0.017749687 [111] 0.151803493 0.175667620 0.900329614 0.125898408 0.410586667 0.586540636 0.832688763 0.035561910 0.252327286 0.110864806 0.054988088 [122] 0.131662542 0.393097130 0.492496368 0.513182966 0.032671230 0.036078371 0.218212971 0.686096496 0.281975363 0.145884637 0.351411775 [133] 0.239547073 0.725330877 0.392954359 0.354585338 0.328110094 0.183872054 0.649535926 0.189835213 0.750660386 0.100154335 0.253081898 [144] 0.306902095 0.138876568 0.112315917 0.602945913 0.033225614 0.021023766 0.126008933 0.074805807 0.593242488 0.276438109 0.030025810 [155] 0.388107478 0.814468072 0.819988723 0.320406177 0.512019175 0.189866084 0.019102564 0.778045038 0.063532324 0.021660839 0.087123658 [166] 0.371697775 0.017831740 0.174573799 0.072586081 0.050795966 0.321374939 0.390300727 0.242283766 0.101281785 0.477098938 0.177832066 [177] 0.851704388 0.421578801 0.059594070 0.801660505 0.263016503 0.158261796 0.629712769 0.680147379 0.627289909 0.143229332 0.371021820 [188] 0.048405335 0.155912833 0.844592024 0.774738072 0.074975116 0.094313006 0.034029316 0.156403796 0.765185930 0.162409946 0.941491860 [199] 0.098203334 0.096782855 0.013696456 0.086137707 0.665930779 0.328426313 0.230933621 0.045364772 0.067599667 0.128835144 0.552086895 [210] 0.192049128 0.391808734 0.634609576 0.138796509 0.034537822 0.402157076 0.473285370 0.915578006 0.109882027 0.060813059 0.078347861 [221] 0.511021110 0.046236210 0.637413042 0.057352160 0.060578365 0.328120324 0.117022441 0.150080125 0.033383541 0.548421342 0.727231653 [232] 0.273012605 0.006665104 0.023959522 0.212622505 0.263913260 0.304803089 0.282446484 0.528587177 0.053303093 0.042787259 0.895978588 [243] 0.951451693 0.260631793 0.057999466 0.077448786 0.039445750 0.078320973 0.521759602 0.120793953 0.109749728 0.115212481 0.063160685 [254] 0.315382778 0.186348273 0.242141093 0.800887574 0.866564275 0.147824356 0.483279794 0.458255040 0.058388003 0.052876258 0.244883311 [265] 0.761115562 0.021394588 0.531937078 0.751371443 0.307274782 0.801133302 0.005240345 0.656336098 0.109766806 0.101435208 0.023548802 [276] 0.056804667 0.081720211 0.513341731 0.014456472 0.127221754 0.744369844 0.498650317 0.182496315 0.605638441 0.021059011 0.094691510 [287] 0.633177077 0.189201907 0.229657632 0.818999683 0.081933947 0.793780642 0.798845854 0.137588419 0.275762056 0.057395933 0.445742740 [298] 0.584089416 0.070998635 0.209775098 0.174014948 0.294145785 0.743857124 0.126042513 0.893349297 0.712229694 0.217225821 0.135596074 [309] 0.343239883 0.187767754 0.233598292 0.822021476 0.082764167 0.102210424 0.113711375 0.122461316 0.828791325 0.107099346 0.054386091 [320] 0.931938668 0.326454054 0.678618259 0.863704140 0.217542759 0.056249096 0.816489720 0.491806299 0.084796292 0.943486215 0.302404878 [331] 0.132747361 0.037305147 0.053261653 0.902996173 0.042527808 0.787771463 0.031525411 0.220902831 0.054775507 0.642266770 0.520264613 [342] 0.707148463 0.902568015 0.695527581 0.848650675 0.466017086 0.291730904 0.027601848 0.133315806 0.771592655 0.383831136 0.090423918 [353] 0.020481869 0.115239515 0.038125985 0.017056834 0.070184592 0.755542001 0.050653582 0.481007350 0.195086963 0.187977925 0.169394579 [364] 0.377212811 0.151859655 0.475032680 0.186324471 0.612248541 0.023804839 0.007054577 0.085213924 0.171343641 0.900710787 0.150595032 [375] 0.257766676 0.013926514 0.035750449 0.185105131 0.253500436 0.369424730 0.640522481 0.841160836 0.060643162 0.051633327 0.731126069 [386] 0.154040714 0.063202801 0.337255163 0.087712142 0.059196122 0.446522479 0.839581017 0.899946965 0.051162921 0.569809764 0.085461942 [397] 0.080315826 0.343678259 0.504516436 0.092545675 0.175360179 0.310167781 0.720652085 0.094860697 0.905656358 0.432122047 0.742494468 [408] 0.704660222 0.065240504 0.044936864 0.086384200 0.460776518 0.058306508 0.538740077 0.360529087 0.413937339 0.248787673 0.063168325 [419] 0.205549345 0.143823024 0.121451790 0.204643255 0.028047843 0.262047997 0.797845942 0.020192814 0.067782381 0.880235920 0.067820578 [430] 0.093376849 0.145058897 0.885115279 0.023833728 0.172919691 0.023186441 0.343149266 0.491733925 0.334336420 0.182785009 0.107602309 [441] 0.159470782 0.480057756 0.718226410 0.110307283 0.282105476 0.504892387 0.023457816 0.246497165 0.288819782 0.406440777 0.333499471 [452] 0.789228958 0.024672867 0.158285199 0.915532293 0.083660243 0.616972917 0.156244026 0.066104438 0.117576505 0.582609228 0.935907656 [463] 0.494078320 0.906231057 0.044752423 0.098120327 0.817658808 0.063067848 0.048103513 0.094446457 0.009656172 0.457728456 0.371287692 [474] 0.732871561 0.239173153 0.048538537 0.427026826 0.838680152 0.005916331 0.639747396 0.192937289 0.044098532 0.307301900 0.771646053 [485] 0.893235289 0.633989608 0.325705459 0.861507987 0.924405484 0.211878609 0.232857992 0.553168378 0.199630964 0.167297598 0.636698113 [496] 0.102745817 0.412260537 0.037681077 0.499347951 0.561847838 0.108622049 0.036902563 0.275679034 0.142289294 0.929821372 0.853111736 [507] 0.052948764 0.186298288 0.034369934 0.468239290 0.021331580 0.617803456 0.083865824 0.751272138 0.115022996 0.496356896 0.458485256 [518] 0.303064449 0.512265744 0.935001789 0.011659745 0.243551235 0.234201771 0.733582682 0.439827183 0.132566837 0.200183137 0.254534757 [529] 0.620459940 0.187218540 0.128750723 0.801532793 round(hi - lo, 2) [1] 0.45 -0.24 -0.25 -0.24 0.63 0.40 0.13 -0.05 0.17 -0.04 -0.23 0.18 -0.24 0.15 -0.07 0.15 -0.14 0.51 0.50 -0.18 0.69 0.52 [23] -0.24 -0.24 -0.21 -0.23 0.49 -0.26 -0.15 -0.20 -0.05 -0.25 0.03 -0.05 -0.02 -0.12 -0.21 -0.22 -0.10 -0.24 0.17 -0.14 0.53 -0.01 [45] -0.03 -0.23 -0.19 0.19 -0.25 -0.05 0.14 -0.24 0.47 -0.19 0.60 -0.16 0.15 -0.07 0.41 0.44 -0.04 -0.19 -0.22 -0.01 -0.16 0.19 [67] -0.25 0.57 0.33 0.62 -0.24 0.71 0.16 -0.02 -0.07 0.01 -0.24 0.59 0.53 0.16 -0.02 0.09 -0.11 -0.14 -0.18 0.70 -0.24 0.24 [89] -0.01 0.04 0.05 0.43 -0.21 -0.24 0.44 0.50 -0.07 0.47 -0.25 0.65 0.53 -0.24 -0.18 0.27 0.23 0.61 0.57 0.19 -0.22 -0.25 [111] -0.11 -0.09 0.64 -0.14 0.15 0.32 0.57 -0.23 -0.01 -0.15 -0.21 -0.13 0.13 0.23 0.25 -0.23 -0.23 -0.05 0.42 0.02 -0.12 0.09 [133] -0.03 0.46 0.13 0.09 0.06 -0.08 0.38 -0.08 0.49 -0.17 -0.01 0.04 -0.13 -0.15 0.34 -0.23 -0.24 -0.14 -0.19 0.33 0.01 -0.24 [155] 0.12 0.55 0.55 0.06 0.25 -0.08 -0.25 0.51 -0.20 -0.24 -0.18 0.11 -0.25 -0.09 -0.19 -0.21 0.06 0.13 -0.02 -0.16 0.21 -0.09 [177] 0.59 0.16 -0.21 0.54 0.00 -0.11 0.36 0.41 0.36 -0.12 0.11 -0.22 -0.11 0.58 0.51 -0.19 -0.17 -0.23 -0.11 0.50 -0.10 0.68 [199] -0.17 -0.17 -0.25 -0.18 0.40 0.06 -0.03 -0.22 -0.20 -0.14 0.29 -0.07 0.13 0.37 -0.13 -0.23 0.14 0.21 0.65 -0.16 -0.20 -0.19 [221] 0.25 -0.22 0.37 -0.21 -0.20 0.06 -0.15 -0.12 -0.23 0.28 0.46 0.01 -0.26 -0.24 -0.05 0.00 0.04 0.02 0.26 -0.21 -0.22 0.63 [243] 0.69 0.00 -0.21 -0.19 -0.23 -0.19 0.26 -0.14 -0.16 -0.15 -0.20 0.05 -0.08 -0.02 0.54 0.60 -0.12 0.22 0.19 -0.21 -0.21 -0.02 [265] 0.50 -0.24 0.27 0.49 0.04 0.54 -0.26 0.39 -0.16 -0.16 -0.24 -0.21 -0.18 0.25 -0.25 -0.14 0.48 0.23 -0.08 0.34 -0.24 -0.17 [287] 0.37 -0.08 -0.04 0.55 -0.18 0.53 0.53 -0.13 0.01 -0.21 0.18 0.32 -0.19 -0.06 -0.09 0.03 0.48 -0.14 0.63 0.45 -0.05 -0.13 [309] 0.08 -0.08 -0.03 0.56 -0.18 -0.16 -0.15 -0.14 0.56 -0.16 -0.21 0.67 0.06 0.41 0.60 -0.05 -0.21 0.55 0.23 -0.18 0.68 0.04 [331] -0.13 -0.23 -0.21 0.64 -0.22 0.52 -0.23 -0.04 -0.21 0.38 0.26 0.44 0.64 0.43 0.58 0.20 0.03 -0.24 -0.13 0.51 0.12 -0.17 [353] -0.24 -0.15 -0.23 -0.25 -0.20 0.49 -0.21 0.22 -0.07 -0.08 -0.10 0.11 -0.11 0.21 -0.08 0.35 -0.24 -0.26 -0.18 -0.09 0.64 -0.11 [375] -0.01 -0.25 -0.23 -0.08 -0.01 0.10 0.38 0.58 -0.20 -0.21 0.47 -0.11 -0.20 0.07 -0.18 -0.21 0.18 0.57 0.63 -0.21 0.30 -0.18 [397] -0.18 0.08 0.24 -0.17 -0.09 0.04 0.46 -0.17 0.64 0.17 0.48 0.44 -0.20 -0.22 -0.18 0.20 -0.21 0.27 0.10 0.15 -0.02 -0.20 [419] -0.06 -0.12 -0.14 -0.06 -0.24 0.00 0.53 -0.25 -0.20 0.62 -0.20 -0.17 -0.12 0.62 -0.24 -0.09 -0.24 0.08 0.23 0.07 -0.08 -0.16 [441] -0.11 0.21 0.45 -0.15 0.02 0.24 -0.24 -0.02 0.02 0.14 0.07 0.52 -0.24 -0.11 0.65 -0.18 0.35 -0.11 -0.20 -0.15 0.32 0.67 [463] 0.23 0.64 -0.22 -0.17 0.55 -0.20 -0.22 -0.17 -0.26 0.19 0.11 0.47 -0.03 -0.22 0.16 0.57 -0.26 0.37 -0.07 -0.22 0.04 0.51 [485] 0.63 0.37 0.06 0.60 0.66 -0.05 -0.03 0.29 -0.07 -0.10 0.37 -0.16 0.15 -0.23 0.23 0.30 -0.16 -0.23 0.01 -0.12 0.66 0.59 [507] -0.21 -0.08 -0.23 0.20 -0.24 0.35 -0.18 0.49 -0.15 0.23 0.19 0.04 0.25 0.67 -0.25 -0.02 -0.03 0.47 0.17 -0.13 -0.07 -0.01 [529] 0.36 -0.08 -0.14 0.54 4.7 Creación de un modelo y validación La creación de un modelo predictivo mediante regresión logística se puede llevar a cabo mediante técnicas de stepwise que ya hemos visto en otros cursos. En R, podemos llevar a cabo dicha metodología usando la función step (que se basa en el test de razón de verosimilitud) o stepAIC (que usa el criterio de Akaike - AIC) de la librería MASS. Lo aprendido anteriormente en el tema de validación cruzada nos servirá oara evaluar la capacidad de un modelo. Veámoslo con un ejemplo. Supongamos que queremos elegir el mejor modelo para predecir el riesgo de diabetes y queremos validarlo con valización cruzada. Todos los pasos y métodos que hemos aprendido en las lecciones anteriores, podemos realizarlos de la siguiente manera. Usaremos los datos train y test que hay por defecto (Pima.tr y Pima.te). Para la validación cruzada usaremos la librería caret que veremos en detalle más adelante. Empecemos seleccionando el mejor modelo en los datos de entrenamiento con un stepwise hacia atrás mod.all &lt;- glm(type ~ ., data=Pima.tr, family=&quot;binomial&quot;) mod &lt;- stats::step(mod.all, trace=FALSE, direction=&quot;backward&quot;) summary(mod) Call: glm(formula = type ~ npreg + glu + bmi + ped + age, family = &quot;binomial&quot;, data = Pima.tr) Deviance Residuals: Min 1Q Median 3Q Max -2.0009 -0.6816 -0.3664 0.6467 2.2898 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -9.938059 1.541571 -6.447 1.14e-10 *** npreg 0.103142 0.064517 1.599 0.10989 glu 0.031809 0.006667 4.771 1.83e-06 *** bmi 0.079672 0.032649 2.440 0.01468 * ped 1.811417 0.661048 2.740 0.00614 ** age 0.039286 0.020967 1.874 0.06097 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 256.41 on 199 degrees of freedom Residual deviance: 178.47 on 194 degrees of freedom AIC: 190.47 Number of Fisher Scoring iterations: 5 Podemos evaluar la capacidad predictiva en la muestra test mediante preds &lt;- predict(mod, newdata = Pima.te, type=&quot;response&quot;) preds &lt;- as.factor(ifelse(preds &gt;= .5, &quot;Yes&quot;, &quot;No&quot;) ) confusionMatrix(preds, Pima.te$type) Confusion Matrix and Statistics Reference Prediction No Yes No 199 42 Yes 24 67 Accuracy : 0.8012 95% CI : (0.7542, 0.8428) No Information Rate : 0.6717 P-Value [Acc &gt; NIR] : 1.116e-07 Kappa : 0.5294 Mcnemar&#39;s Test P-Value : 0.03639 Sensitivity : 0.8924 Specificity : 0.6147 Pos Pred Value : 0.8257 Neg Pred Value : 0.7363 Prevalence : 0.6717 Detection Rate : 0.5994 Detection Prevalence : 0.7259 Balanced Accuracy : 0.7535 &#39;Positive&#39; Class : No y calcular la capacidad predictiva en la muestra train del modelo completo utilizando un método de 5-fold cross-validation mediante: library(caret) mod.train &lt;- train(type ~ ., data=Pima.tr, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.train Generalized Linear Model 200 samples 7 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 160, 160, 159, 160, 161 Resampling results: Accuracy Kappa 0.7553283 0.4381235 la capcidad predictiva para el modelo seleccionado mediante stepwise se calcularía mediante: mod.train.2 &lt;- train(formula(mod), data=Pima.tr, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.train.2 Generalized Linear Model 200 samples 5 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 160, 159, 160, 160, 161 Resampling results: Accuracy Kappa 0.7854503 0.5003265 Comprobamos como el modelo completo tiene mejor accuracy, pero ya hemos visto que esto es un problema de sobreajuste. Validemos los modelos con la muestra test: mod.test &lt;- train(type ~ ., data=Pima.te, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.test Generalized Linear Model 332 samples 7 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 266, 265, 266, 266, 265 Resampling results: Accuracy Kappa 0.7769787 0.4543523 mod.test.2 &lt;- train(formula(mod), data=Pima.te, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.test.2 Generalized Linear Model 332 samples 5 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 266, 266, 265, 265, 266 Resampling results: Accuracy Kappa 0.7949796 0.5130989 Vemos que en este caso el modelo obtenido mediante stepwise tiene Notemos que la librería caret es muy flexible y nos permite mediante una única función train, estimar el modelo y evaluarlo mediante validación cruzada usando el argumento trControl. A lo largo del curso veremos que cambiando el argumento method podremos usar cualquier método de aprendizaje automático que existe. También veremos otros argumentos que nos permitirán llevar a cabo otros pasos necesarios a la hora de crear un modelo predictivo. 4.8 Nomogramas Una vez que ya tenemos creado y validad un modelo predictivo, nos puede interesar aplicarlo en otros individuos para poder tomar decisiones. Para ello, podemos usar nomogramas. Un nomograma es una representación gráfica que permite realizar con rapidez cálculos numéricos aproximados. Dentro del campo de la medicina, es frecuente que este tipo de gráficos este asociado al calculo de probabilidades de ocurrencia de un evento o una característica asociada a una enfermedad. Es lo que se conoce como Medicina Translacional. Aunque existen otro tipo de herramientas de cálculo vía web para estas probabilidades (Shiny), el uso de nomogramas esta muy extendido en el campo de la biomedicina como por ejemplo en el calculo de probabilidades de recurrencia en distintos tipos de cáncer, la probabilidad de supervivencia a un mes tras un infarto, o el pronóstico tras un diagnóstico de cáncer a cierto tiempo (en ese caso se usan modelos de supervivencia. Así pues, la regresión logística será una de las herramientas con una aplicación más directa y sencilla en el aprendizaje automático, donde el uso de los modelos predictivos suelen tener un aplicabilidad directa en la población. Existen numerosas herramientas para crear nomogramas en R, empecemos con la creación de nomogramas sencillos. Para ello usaremos los datos del ejemplo de diabetes con el modelo que hemos obtenido y validado anteriormente. Para usar la librería rms necesitamos que el modelo esté estimado con la función lrm () library(rms) t.data &lt;- datadist(Pima.tr) options(datadist = &#39;t.data&#39;) mod.lrm &lt;- lrm(type ~ npreg + glu + bmi + ped + age, data=Pima.tr) nom &lt;- nomogram(mod.lrm, fun = plogis, funlabel=&quot;Risk of diabetes&quot;) plot(nom) Supongamos que llega a la consulta una persona con un bmi de 35. Eso sumaría 30 puntos (basta con subir hacia arriba y ver qué valor de Points corresponde a un bmi de 35). Una edad de 50 años (~22 puntos), una función del pedigrí de la diabetes de 1.8 (~62 puntos), una glucosa de 120 (~50 puntos) y 0 embarazos que sumaría 0 puntos. En total, el paciente suma un total de 164 puntos. Si ahora vamos a la línea de Total Points y proyectamos sobre el predictor lineal de aproximadamente ~1.9 que se asocia con un riesgo de diabetes ligeramente superior al 80% (proyectar sobre Risk of diabetes). Obviamente estos cálculos se pueden hacer de forma más directa calculando la predicción sobre este individuo con el objeto de R indiv &lt;- data.frame(bmi=35, age=50, ped=1.8, glu=120, npreg=0) predict(mod.lrm, newdata = indiv, type=&quot;lp&quot;) 1 1.892376 predict(mod.lrm, newdata = indiv, type = &quot;fitted&quot;) 1 0.8690262 Estos cálculos se pueden programar en R y hace una función, o también una aplicación Shiny para aquellos médicos que no sepan usar R (los nomogramas se siguen utilizando). Otra opción es que hagamos uso de una librería para hacer nomogramas dinámicos (crea Shiny app) de forma sencilla con la librería DynNom library(DynNom) DynNom(mod, Pima.tr) Con estas simples instrucciones obtendríamos esta aplicación Shiny (Figura abajo) donde cada intervalo de confianza corresponde a un cálculo obtenido variando alguna de las variables predictoras Nomograma para el modelo de predicción para diabetes GLM significa modelo lineal generalizado. Esta función se ajustará a una variedad de modelos no lineales. Por ejemplo, podríamos especificar una regresión de Poisson con family = poisson. Para obtener más detalles, se puede consultar este texto [How to interpret odds ratio in logistic regression?] (Https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
