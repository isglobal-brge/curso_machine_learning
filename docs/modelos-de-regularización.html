<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Modelos de regularización | Introducción al Aprendizaje automático en ciencias de la salud</title>
  <meta name="description" content="9 Modelos de regularización | Introducción al Aprendizaje automático en ciencias de la salud" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Modelos de regularización | Introducción al Aprendizaje automático en ciencias de la salud" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Modelos de regularización | Introducción al Aprendizaje automático en ciencias de la salud" />
  
  
  

<meta name="author" content="Juan R González" />


<meta name="date" content="2023-07-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="respuesta-no-balanceada.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Automático</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preámbulo</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#instalación-de-librerías-necesarias-para-el-curso"><i class="fa fa-check"></i><b>1.1</b> Instalación de librerías necesarias para el curso</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introducción-al-aprendizaje-automático.html"><a href="introducción-al-aprendizaje-automático.html"><i class="fa fa-check"></i><b>2</b> Introducción al Aprendizaje Automático</a></li>
<li class="chapter" data-level="3" data-path="validación-cruzada.html"><a href="validación-cruzada.html"><i class="fa fa-check"></i><b>3</b> Validación cruzada</a>
<ul>
<li class="chapter" data-level="3.1" data-path="validación-cruzada.html"><a href="validación-cruzada.html#validación-en-un-conjunto-de-datos-externo"><i class="fa fa-check"></i><b>3.1</b> Validación en un conjunto de datos externo</a></li>
<li class="chapter" data-level="3.2" data-path="validación-cruzada.html"><a href="validación-cruzada.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>3.2</b> Leave-one-out cross validation (LOOCV)</a></li>
<li class="chapter" data-level="3.3" data-path="validación-cruzada.html"><a href="validación-cruzada.html#k-fold-cross-validation-k-fold-cv"><i class="fa fa-check"></i><b>3.3</b> K-fold cross validation (K-fold CV)</a></li>
<li class="chapter" data-level="3.4" data-path="validación-cruzada.html"><a href="validación-cruzada.html#uso-de-cv-para-estimar-el-hiper-parámetro"><i class="fa fa-check"></i><b>3.4</b> Uso de CV para estimar el hiper-parámetro</a></li>
<li class="chapter" data-level="3.5" data-path="validación-cruzada.html"><a href="validación-cruzada.html#uso-de-bootstrap"><i class="fa fa-check"></i><b>3.5</b> Uso de bootstrap</a></li>
<li class="chapter" data-level="3.6" data-path="validación-cruzada.html"><a href="validación-cruzada.html#imputación-de-datos-faltantes-información-extra-para-los-que-venís-al-curso"><i class="fa fa-check"></i><b>3.6</b> Imputación de datos faltantes (Información extra para los que venís al curso)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regresión-logística.html"><a href="regresión-logística.html"><i class="fa fa-check"></i><b>4</b> Regresión logística</a>
<ul>
<li class="chapter" data-level="4.1" data-path="regresión-logística.html"><a href="regresión-logística.html#la-función-logit-inversa"><i class="fa fa-check"></i><b>4.1</b> La función logit inversa</a></li>
<li class="chapter" data-level="4.2" data-path="regresión-logística.html"><a href="regresión-logística.html#ejemplo-de-regresión-logística"><i class="fa fa-check"></i><b>4.2</b> Ejemplo de regresión logística</a></li>
<li class="chapter" data-level="4.3" data-path="regresión-logística.html"><a href="regresión-logística.html#coeficientes-de-regresión-logística-como-probabilidades"><i class="fa fa-check"></i><b>4.3</b> Coeficientes de regresión logística como probabilidades</a></li>
<li class="chapter" data-level="4.4" data-path="regresión-logística.html"><a href="regresión-logística.html#coeficientes-de-regresión-logística-como-razones-de-odds"><i class="fa fa-check"></i><b>4.4</b> Coeficientes de regresión logística como razones de odds</a></li>
<li class="chapter" data-level="4.5" data-path="regresión-logística.html"><a href="regresión-logística.html#capacidad-predictiva-de-un-modelo-de-clasificación"><i class="fa fa-check"></i><b>4.5</b> Capacidad predictiva de un modelo de clasificación</a></li>
<li class="chapter" data-level="4.6" data-path="regresión-logística.html"><a href="regresión-logística.html#ejemplo-de-regresión-logística-modelización-de-riesgo-diabetes"><i class="fa fa-check"></i><b>4.6</b> Ejemplo de regresión logística: modelización de riesgo diabetes</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="regresión-logística.html"><a href="regresión-logística.html#modelo-simple"><i class="fa fa-check"></i><b>4.6.1</b> Modelo simple</a></li>
<li class="chapter" data-level="4.6.2" data-path="regresión-logística.html"><a href="regresión-logística.html#agregar-predictores-y-evaluar-el-ajuste"><i class="fa fa-check"></i><b>4.6.2</b> Agregar predictores y evaluar el ajuste</a></li>
<li class="chapter" data-level="4.6.3" data-path="regresión-logística.html"><a href="regresión-logística.html#análisis-de-interacciones"><i class="fa fa-check"></i><b>4.6.3</b> Análisis de interacciones</a></li>
<li class="chapter" data-level="4.6.4" data-path="regresión-logística.html"><a href="regresión-logística.html#gráfico-de-la-interacción"><i class="fa fa-check"></i><b>4.6.4</b> Gráfico de la interacción</a></li>
<li class="chapter" data-level="4.6.5" data-path="regresión-logística.html"><a href="regresión-logística.html#uso-del-modelo-para-predecir-probabilidades"><i class="fa fa-check"></i><b>4.6.5</b> Uso del modelo para predecir probabilidades</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="regresión-logística.html"><a href="regresión-logística.html#creación-de-un-modelo-y-validación"><i class="fa fa-check"></i><b>4.7</b> Creación de un modelo y validación</a></li>
<li class="chapter" data-level="4.8" data-path="regresión-logística.html"><a href="regresión-logística.html#nomogramas"><i class="fa fa-check"></i><b>4.8</b> Nomogramas</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>5</b> La librería <code>caret</code></a>
<ul>
<li class="chapter" data-level="5.1" data-path="caret.html"><a href="caret.html#pre-procesado"><i class="fa fa-check"></i><b>5.1</b> Pre-procesado</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="caret.html"><a href="caret.html#creación-de-variables"><i class="fa fa-check"></i><b>5.1.1</b> Creación de variables</a></li>
<li class="chapter" data-level="5.1.2" data-path="caret.html"><a href="caret.html#predictores-con-poca-variabilidad"><i class="fa fa-check"></i><b>5.1.2</b> Predictores con poca variabilidad</a></li>
<li class="chapter" data-level="5.1.3" data-path="caret.html"><a href="caret.html#identificación-de-predictores-correlacionados"><i class="fa fa-check"></i><b>5.1.3</b> Identificación de predictores correlacionados</a></li>
<li class="chapter" data-level="5.1.4" data-path="caret.html"><a href="caret.html#centrado-y-escalado"><i class="fa fa-check"></i><b>5.1.4</b> Centrado y escalado</a></li>
<li class="chapter" data-level="5.1.5" data-path="caret.html"><a href="caret.html#imputación"><i class="fa fa-check"></i><b>5.1.5</b> Imputación</a></li>
<li class="chapter" data-level="5.1.6" data-path="caret.html"><a href="caret.html#pre-procesado-con-la-librería-recipes"><i class="fa fa-check"></i><b>5.1.6</b> Pre-procesado con la librería <code>recipes</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="caret.html"><a href="caret.html#visualización"><i class="fa fa-check"></i><b>5.2</b> Visualización</a></li>
<li class="chapter" data-level="5.3" data-path="caret.html"><a href="caret.html#ejemplo-completo-creación-de-modelo-diagnóstico-para-cáncer-de-mama"><i class="fa fa-check"></i><b>5.3</b> Ejemplo completo: creación de modelo diagnóstico para cáncer de mama</a></li>
<li class="chapter" data-level="5.4" data-path="caret.html"><a href="caret.html#creación-de-un-modelo-predictivo"><i class="fa fa-check"></i><b>5.4</b> Creación de un modelo predictivo</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html"><i class="fa fa-check"></i><b>6</b> Árboles de decisión</a>
<ul>
<li class="chapter" data-level="6.1" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#árboles-de-clasificación"><i class="fa fa-check"></i><b>6.1</b> Árboles de clasificación</a></li>
<li class="chapter" data-level="6.2" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#área-bajo-la-curva-roc"><i class="fa fa-check"></i><b>6.2</b> Área bajo la curva ROC</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#entrenamiento-con-caret"><i class="fa fa-check"></i><b>6.2.1</b> Entrenamiento con <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#árboles-de-regresión"><i class="fa fa-check"></i><b>6.3</b> Árboles de regresión</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#entrenamiento-con-caret-1"><i class="fa fa-check"></i><b>6.3.1</b> Entrenamiento con <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#bagged-trees"><i class="fa fa-check"></i><b>6.4</b> Bagged trees</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#bagging-árboles-de-clasificación"><i class="fa fa-check"></i><b>6.4.1</b> Bagging árboles de clasificación</a></li>
<li class="chapter" data-level="6.4.2" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#bagging-árboles-de-regresión"><i class="fa fa-check"></i><b>6.4.2</b> Bagging árboles de regresión</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#random-forest"><i class="fa fa-check"></i><b>6.5</b> Random Forest</a></li>
<li class="chapter" data-level="6.6" data-path="árboles-de-decisión.html"><a href="árboles-de-decisión.html#random-forest-pn"><i class="fa fa-check"></i><b>6.6</b> Random Forest p&gt;&gt;n</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>7</b> Boosting</a>
<ul>
<li class="chapter" data-level="7.1" data-path="boosting.html"><a href="boosting.html#cómo-funciona-el-boosting"><i class="fa fa-check"></i><b>7.1</b> Cómo funciona el <em>boosting</em></a></li>
<li class="chapter" data-level="7.2" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>7.2</b> AdaBoost</a></li>
<li class="chapter" data-level="7.3" data-path="boosting.html"><a href="boosting.html#gbm-básico"><i class="fa fa-check"></i><b>7.3</b> GBM básico</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="boosting.html"><a href="boosting.html#boostingHiperparam"><i class="fa fa-check"></i><b>7.3.1</b> Hiperparámetros</a></li>
<li class="chapter" data-level="7.3.2" data-path="boosting.html"><a href="boosting.html#implementación"><i class="fa fa-check"></i><b>7.3.2</b> Implementación</a></li>
<li class="chapter" data-level="7.3.3" data-path="boosting.html"><a href="boosting.html#estrategia-general-de-tuning"><i class="fa fa-check"></i><b>7.3.3</b> Estrategia general de <em>tuning</em></a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="boosting.html"><a href="boosting.html#gbms-estocásticos"><i class="fa fa-check"></i><b>7.4</b> GBMs estocásticos</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="boosting.html"><a href="boosting.html#hiperparámetros-estocásticos"><i class="fa fa-check"></i><b>7.4.1</b> Hiperparámetros estocásticos</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="boosting.html"><a href="boosting.html#xgboost"><i class="fa fa-check"></i><b>7.5</b> XGBoost</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="boosting.html"><a href="boosting.html#reguralización"><i class="fa fa-check"></i><b>7.5.1</b> Reguralización</a></li>
<li class="chapter" data-level="7.5.2" data-path="boosting.html"><a href="boosting.html#estrategia-de-tuning"><i class="fa fa-check"></i><b>7.5.2</b> Estrategia de <em>tuning</em></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="boosting.html"><a href="boosting.html#otros-algoritmos-gbm"><i class="fa fa-check"></i><b>7.6</b> Otros algoritmos GBM</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="respuesta-no-balanceada.html"><a href="respuesta-no-balanceada.html"><i class="fa fa-check"></i><b>8</b> Respuesta no balanceada</a></li>
<li class="chapter" data-level="9" data-path="modelos-de-regularización.html"><a href="modelos-de-regularización.html"><i class="fa fa-check"></i><b>9</b> Modelos de regularización</a>
<ul>
<li class="chapter" data-level="9.1" data-path="modelos-de-regularización.html"><a href="modelos-de-regularización.html#introducción"><i class="fa fa-check"></i><b>9.1</b> Introducción</a></li>
<li class="chapter" data-level="9.2" data-path="modelos-de-regularización.html"><a href="modelos-de-regularización.html#cómo-regularizar"><i class="fa fa-check"></i><b>9.2</b> Cómo regularizar</a></li>
<li class="chapter" data-level="9.3" data-path="modelos-de-regularización.html"><a href="modelos-de-regularización.html#implementación-en-r"><i class="fa fa-check"></i><b>9.3</b> Implementación en R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="modelos-de-regularización.html"><a href="modelos-de-regularización.html#ejemplo-ridge-regression"><i class="fa fa-check"></i><b>9.3.1</b> Ejemplo: Ridge Regression</a></li>
<li class="chapter" data-level="9.3.2" data-path="modelos-de-regularización.html"><a href="modelos-de-regularización.html#ejemplo-lasso"><i class="fa fa-check"></i><b>9.3.2</b> Ejemplo: Lasso</a></li>
<li class="chapter" data-level="9.3.3" data-path="modelos-de-regularización.html"><a href="modelos-de-regularización.html#ejemplo-elastic-net"><i class="fa fa-check"></i><b>9.3.3</b> Ejemplo: Elastic Net</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción al Aprendizaje automático en ciencias de la salud</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelos-de-regularización" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">9</span> Modelos de regularización<a href="modelos-de-regularización.html#modelos-de-regularización" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introducción" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Introducción<a href="modelos-de-regularización.html#introducción" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En este capítulo, exploraremos los modelos de regularización (y el concepto de regresión penalizada), que es una técnica poderosa para lidiar con el sobreajuste en modelos de aprendizaje automático. El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento pero tiene un rendimiento deficiente en datos no vistos. La regularización es un método que introduce un término de penalización en la función de pérdida del modelo para evitar el sobreajuste. A grandes rasgos podemos resumir que</p>
<p>Hay dos tipos comunes de regularización utilizados en la regresión penalizada:</p>
<ol style="list-style-type: decimal">
<li><p>Regularización L1 (Regresión Lasso): Agrega el valor absoluto de los coeficientes como término de penalización a la función de pérdida.</p></li>
<li><p>Regularización L2 (Regresión Ridge): Agrega el cuadrado de los coeficientes como término de penalización a la función de pérdida.</p></li>
</ol>
<p>La regularización ridge es suave y la regularización lasso es áspera tal y como se puede ver en la siguiente figura</p>
<p><img src="figures/lasso_ridge_1.jpg" /></p>
<p>Es esta diferencia entre las restricciones suaves y ásperas lo que resulta en que el Lasso tenga estimaciones de coeficientes que son exactamente cero, mientras que Ridge no lo hace. Ilustramos esto aún más en la siguiente figura. La solución de mínimos cuadrados se marca como <span class="math inline">\(\hat{\beta}\)</span>, mientras que el diamante azul y el círculo representan las restricciones de regresión Lasso y Ridge (como en la Figura anterior). Si la penalización (<span class="math inline">\(t\)</span> en la figura anterior) es lo suficientemente grande (aumentar la penalización hace que el diamante y el círculo sean más grandes, respectivamente), entonces las regiones de restricción contendrán a <span class="math inline">\(\hat{\beta}\)</span>, por lo que las estimaciones de Ridge y Lasso serán iguales a las estimaciones de mínimos cuadrados.</p>
<p>Las curvas que están centradas alrededor de <span class="math inline">\(\hat{\beta}\)</span> representan regiones de RSS constante. A medida que las elipses se alejan de las estimaciones de coeficientes de mínimos cuadrados, el RSS aumenta. Las estimaciones de coeficientes de regresión Lasso y Ridge se dan en el primer punto en el que una elipse toca la región de restricción.</p>
<p><img src="figures/lasso_ridge_1.jpg" /></p>
<p>La regularización proporciona varios beneficios:</p>
<ul>
<li>Ayuda a prevenir el sobreajuste al reducir la complejidad del modelo.</li>
<li>Mejora la generalización, lo que hace que el modelo tenga un mejor rendimiento en datos no vistos.</li>
<li>Puede realizar automáticamente la selección de características al establecer algunos coeficientes en cero.</li>
</ul>
<p>Regularización L1 (Regresión Lasso)
La regresión Lasso agrega la suma de los valores absolutos de los coeficientes como término de penalización a la función de pérdida. Esta penalización fomenta que algunos coeficientes sean exactamente cero, lo que efectivamente realiza la selección de características.</p>
</div>
<div id="cómo-regularizar" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Cómo regularizar<a href="modelos-de-regularización.html#cómo-regularizar" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El procedimiento habitual para ajustar un modelo de regresión lineal es emplear mínimos cuadrados, es decir, utilizar como criterio de error la suma de cuadrados residual</p>
<p><span class="math display">\[\mbox{RSS} = \sum\limits_{i=1}^{n}\left(  y_{i} - \beta_0 - \boldsymbol{\beta}^t \mathbf{x}_{i} \right)^{2}\]</span></p>
<p>Si el modelo lineal es razonablemente adecuado, utilizar RSS va a dar lugar a estimaciones con poco sesgo, y si además <span class="math inline">\(n\gg p\)</span>, entonces el modelo también va a tener poca varianza. Las dificultades surgen cuando <span class="math inline">\(p\)</span> es grande o cuando hay correlaciones altas entre las variables predictoras: tener muchas variables dificulta la interpretación del modelo, y si además hay problemas de colinealidad o se incumple <span class="math inline">\(n\gg p\)</span>, entonces la estimación del modelo va a tener muchas varianza y el modelo estará sobreajustado. La solución pasa por forzar a que el modelo tenga menos complejidad para así reducir su varianza. Una forma de conseguirlo es mediante la regularización (regularization o shrinkage) de la estimación de los parámetros <span class="math inline">\(\beta_1, \beta_2,\ldots, \beta_p\)</span> que consiste en considerar todas las variables predictoras pero forzando a que algunos de los parámetros se estimen mediante valores muy próximos a cero, o directamente con ceros. Esta técnica va a provocar un pequeño aumento en el sesgo pero a cambio una notable reducción en la varianza y una interpretación más sencilla del modelo resultante.</p>
<p>Como ya hemos anticipado, hay dos formas básicas de lograr esta simplificación de los parámetros (con la consiguiente simplificación del modelo), utilizando una penalización cuadrática (norma <span class="math inline">\(L_2\)</span>) o en valor absoluto (norma <span class="math inline">\(L_1\)</span>):</p>
<p>Hay dos formas básicas de lograr esta simplificación de los parámetros (con la consiguiente simplificación del modelo), utilizando una penalización cuadrática (norma <span class="math inline">\(L_2\)</span>) o en valor absoluto (norma <span class="math inline">\(L_1\)</span>):</p>
<ul>
<li><p><em>Ridge regression</em>
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS} + \lambda\sum_{j=1}^{p}\beta_{j}^{2}\]</span></p>
<p>Equivalentemente,
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS}\]</span>
sujeto a
<span class="math display">\[\sum_{j=1}^{p}\beta_{j}^{2} \le s\]</span></p></li>
<li><p><em>Lasso</em> [<em>least absolute shrinkage and selection operator</em>]
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} RSS + \lambda\sum_{j=1}^{p}|\beta_{j}|\]</span></p>
<p>Equivalentemente,
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS}\]</span>
sujeto a
<span class="math display">\[\sum_{j=1}^{p}|\beta_{j}| \le s\]</span></p></li>
</ul>
<p>Una formulación unificada consiste en considerar el problema
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} RSS + \lambda\sum_{j=1}^{p}|\beta_{j}|^d\]</span></p>
<p>Si <span class="math inline">\(d=0\)</span>, la penalización consiste en el número de variables utilizadas, por tanto se corresponde con el problema de selección de variables; <span class="math inline">\(d=1\)</span> se corresponde con <em>lasso</em> y <span class="math inline">\(d=2\)</span> con <em>ridge</em>.</p>
<p>La ventaja de utilizar <em>lasso</em> es que va a forzar a que algunos parámetros sean cero, con lo cual también se realiza una selección de las variables más influyentes.
Por el contrario, <em>ridge regression</em> va a incluir todas las variables predictoras en el modelo final, si bien es cierto que algunas con parámetros muy próximos a cero: de este modo va a reducir el riesgo del sobreajuste, pero no resuelve el problema de la interpretabilidad.
Otra posible ventaja de utilizar <em>lasso</em> es que cuando hay variables predictoras correlacionadas tiene tendencia a seleccionar una y anular las demás (esto también se puede ver como un inconveniente, ya que pequeños cambios en los datos pueden dar lugar a distintos modelos), mientras que <em>ridge</em> tiende a darles igual peso.</p>
<p>Una generalización de <em>lasso</em> muy utilizada en ciencias ómicas es <em>elastic net</em> que combina las ventajas de <em>ridge</em> y <em>lasso</em>, minimizando
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \ \mbox{RSS} + \lambda \left( \frac{1 - \alpha}{2}\sum_{j=1}^{p}\beta_{j}^{2} + \alpha \sum_{j=1}^{p}|\beta_{j}| \right)\]</span>
con <span class="math inline">\(0 \leq \alpha \leq 1\)</span>.</p>
<p><strong>IMPORTANTE:</strong> Es crucial estandarizar (centrar y reescalar) las variables predictoras antes de realizar estas técnicas. Fijémonos en que, así como <span class="math inline">\(\mbox{RSS}\)</span> es insensible a los cambios de escala, la penalización es muy sensible. Previa estandarización, el término independiente <span class="math inline">\(\beta_0\)</span> (que no interviene en la penalización) tiene una interpretación muy directa, ya que
<span class="math display">\[\widehat \beta_0 = \bar y =\sum_{i=1}^n \frac{y_i}{n}\]</span></p>
<p>Los dos métodos de regularización comentados dependen del hiperparámetro <span class="math inline">\(\lambda\)</span> (equivalentemente, <span class="math inline">\(s\)</span>). Como en cualquier otro método de aprendizaje automático, es muy importante seleccionar adecuadamente el valor del hiperparámetro. Un método que podemos usar es, por ejemplo, <em>validación cruzada</em>. Hay algoritmos muy eficientes que permiten el ajuste, tanto de <em>ridge regression</em> como de <em>lasso</em> de forma conjunta (simultánea) para todos los valores de <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="implementación-en-r" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Implementación en R<a href="modelos-de-regularización.html#implementación-en-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Hay varios paquetes que implementan estos métodos: <code>h2o</code>, <code>elasticnet</code>, <code>penalized</code>, <code>lasso2</code>, <code>biglasso</code>, etc., pero el paquete <code>glmnet</code>(<a href="https://glmnet.stanford.edu" class="uri">https://glmnet.stanford.edu</a>)` es el más usado y tiene una implementación muy eficiente (tener el cuenta que la minimización puede ser costosa computacionalmente)</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="modelos-de-regularización.html#cb307-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span></code></pre></div>
<p>El paquete <code>glmnet</code> no emplea formulación de modelos (es decir, no usa el simbolo ‘~’), hay que establecer la respuesta <code>y</code> y la matriz numérica <code>x</code> correspondiente a las variables explicativas.
Por tanto no se pueden incluir directamente predictores categóricos, habrá que codificarlos empleando variables auxiliares numéricas. Se puede emplear la función <code>model.matrix()</code>(o <code>Matrix::sparse.model.matrix()</code> si el conjunto de datos es muy grande) para construir la matriz de diseño <code>x</code> a partir de una fórmula (alternativamente se pueden emplear la herramientas implementadas en el paquete <code>caret</code>). Además, esta función tampoco admite datos faltantes.</p>
<p>La función principal es:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="modelos-de-regularización.html#cb308-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glmnet</span>(x, y, family, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="cn">NULL</span>, ...)</span></code></pre></div>
<ul>
<li><p><code>family</code>: familia del modelo lineal generalizado (ver Sección <a href="#reg-glm"><strong>??</strong></a>); por defecto <code>"gaussian"</code> (modelo lineal con ajuste cuadrático), también admite <code>"binomial"</code>, <code>"poisson"</code>, <code>"multinomial"</code>, <code>"cox"</code> o <code>"mgaussian"</code> (modelo lineal con respuesta multivariante).</p></li>
<li><p><code>alpha</code>: parámetro <span class="math inline">\(\alpha\)</span> de elasticnet <span class="math inline">\(0 \leq \alpha \leq 1\)</span>. Por defecto <code>alpha = 1</code> penalización <em>lasso</em> (<code>alpha = 0</code> para <em>ridge regression</em>).</p></li>
<li><p><code>lambda</code>: secuencia (opcional) de valores de <span class="math inline">\(\lambda\)</span>; si no se especifica se establece una secuencia por defecto (en base a los argumentos adicionales <code>nlambda</code> y <code>lambda.min.ratio</code>). Se devolverán los ajustes para todos los valores de esta secuencia (también se podrán obtener posteriormente para otros valores).</p></li>
</ul>
<p>Entre los métodos genéricos disponibles del objeto resultante, <code>coef()</code> y <code>predict()</code> permiten obtener los coeficientes y las predicciones para un valor concreto de <span class="math inline">\(\lambda\)</span>, que se debe especificar mediante el argumento <code>s = valor</code> (los autores usan <code>s</code> en vez de <code>lambda</code> ya que inicialmente usaban ese nombre).</p>
<p>El valor “óptimo” del hiperparámetro <span class="math inline">\(\lambda\)</span> se puede calcular mediante validación cruzada con la siguiente instrucción:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="modelos-de-regularización.html#cb309-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cv.glmnet</span>(x, y, family, alpha, lambda, <span class="at">type.measure =</span> <span class="st">&quot;default&quot;</span>, <span class="at">nfolds =</span> <span class="dv">10</span>, ...)</span></code></pre></div>
<p>Esta función también devuelve los ajustes con toda la muestra de entrenamiento (en la componente <code>$glmnet.fit</code>) y se puede emplear el resultado directamente para predecir o obtener los coeficientes del modelo. Por defecto, selecciona <span class="math inline">\(\lambda\)</span> mediante la regla de “un error estándar” (componente <code>$lambda.1se</code>), aunque también calcula el valor óptimo (componente <code>$lambda.min</code>; que se puede seleccionar con estableciendo <code>s = "lambda.min"</code>). Para más detalles consultar la vignette del paquete <a href="https://glmnet.stanford.edu/articles/glmnet.html">An Introduction to glmnet</a>.</p>
<p>Para ilustrar cómo llevar a cabo estos análisis utilizaremos los datos de cáncer de mama que ya hemos trabajado en el capítulo de preproceso de datos. Este ejemplo tiene mucho sentido (aunque no se cumpla que <span class="math inline">\(p \gg n\)</span>) ya que también existe mucha correlación entre las variables dependientes, que es un caso donde también se suele utilizar estos modelos.</p>
<p>Los datos ya pre-procesados se pueden cargar de la siguiente manera (nota, yo uso “data” porque están en ese directorio en mi ordenador):</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="modelos-de-regularización.html#cb310-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/breast.Rdata&quot;</span>)</span></code></pre></div>
<p>Ahora debemos extraer la matriz de datos <span class="math inline">\(x\)</span> (variables independientes) y el vector con la variable que queremos predecir <span class="math inline">\(y\)</span> (<em>outcome</em>). Recordemos que las variables son las siguientes:</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="modelos-de-regularización.html#cb311-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(breast_train_prep)</span></code></pre></div>
<pre><code> [1] &quot;texture_mean&quot;            &quot;area_mean&quot;               &quot;smoothness_mean&quot;         &quot;compactness_mean&quot;       
 [5] &quot;symmetry_mean&quot;           &quot;fractal_dimension_mean&quot;  &quot;texture_se&quot;              &quot;area_se&quot;                
 [9] &quot;smoothness_se&quot;           &quot;compactness_se&quot;          &quot;concavity_se&quot;            &quot;concave points_se&quot;      
[13] &quot;symmetry_se&quot;             &quot;fractal_dimension_se&quot;    &quot;smoothness_worst&quot;        &quot;compactness_worst&quot;      
[17] &quot;concave points_worst&quot;    &quot;symmetry_worst&quot;          &quot;fractal_dimension_worst&quot; &quot;diagnosis&quot;              </code></pre>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="modelos-de-regularización.html#cb313-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(breast_train_prep[, <span class="sc">-</span><span class="fu">ncol</span>(breast_train_prep)])</span>
<span id="cb313-2"><a href="modelos-de-regularización.html#cb313-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> breast_train_prep<span class="sc">$</span>diagnosis</span></code></pre></div>
<p>También podemos extraer x de forma más “clara” con tidyverse</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="modelos-de-regularización.html#cb314-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> breast_train_prep <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>diagnosis) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span></code></pre></div>
<p>o usando el diseño del modelo sin el intercept (nota: en vez de ‘.’ se pueden poner el nombre de las variables predictoras)</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="modelos-de-regularización.html#cb315-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(diagnosis <span class="sc">~</span> <span class="sc">-</span> <span class="dv">1</span> <span class="sc">+</span> . , <span class="at">data=</span>breast_train_prep)</span></code></pre></div>
<div id="ejemplo-ridge-regression" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> Ejemplo: Ridge Regression<a href="modelos-de-regularización.html#ejemplo-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ajustamos los modelos de regresión ridge (con la secuencia de valores de <span class="math inline">\(\lambda\)</span> por defecto) con la función <code>glmnet()</code> con <code>alpha=0</code> (ridge penalty) y la opción <code>family = binomial</code> porque estamos ante un problema de clasificación binaria:</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="modelos-de-regularización.html#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb316-2"><a href="modelos-de-regularización.html#cb316-2" aria-hidden="true" tabindex="-1"></a>fit.ridge <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">family =</span> binomial)</span>
<span id="cb316-3"><a href="modelos-de-regularización.html#cb316-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.ridge, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">label =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="fig_out/unnamed-chunk-220-1.png" width="672" /></p>
<p>Podemos obtener el modelo o predicciones para un valor concreto de <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="modelos-de-regularización.html#cb317-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit.ridge, <span class="at">s =</span> <span class="dv">2</span>) <span class="co"># lambda = 2</span></span></code></pre></div>
<pre><code>20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
                                   s1
(Intercept)             -0.5555635957
texture_mean             0.0735405177
area_mean                0.1206458718
smoothness_mean          0.0413289772
compactness_mean         0.0815272858
symmetry_mean            0.0433533589
fractal_dimension_mean  -0.0226349104
texture_se              -0.0062811918
area_se                  0.0911190933
smoothness_se           -0.0165352515
compactness_se           0.0342444176
concavity_se             0.0449677702
`concave points_se`      0.0603394789
symmetry_se             -0.0114723777
fractal_dimension_se     0.0003898276
smoothness_worst         0.0591408831
compactness_worst        0.0844100996
`concave points_worst`   0.1249016680
symmetry_worst           0.0631948865
fractal_dimension_worst  0.0395917038</code></pre>
<p>Para seleccionar el parámetro de penalización por validación cruzada empleamos <code>cv.glmnet()</code> (usamos la semilla para que sea reproducible en cualquier ordenador):</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="modelos-de-regularización.html#cb319-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb319-2"><a href="modelos-de-regularización.html#cb319-2" aria-hidden="true" tabindex="-1"></a>cv.ridge <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb319-3"><a href="modelos-de-regularización.html#cb319-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.ridge)</span></code></pre></div>
<p><img src="fig_out/unnamed-chunk-222-1.png" width="672" /></p>
<p>En este caso el parámetro óptimo (según la regla de un error estándar) sería:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="modelos-de-regularización.html#cb320-1" aria-hidden="true" tabindex="-1"></a>cv.ridge<span class="sc">$</span>lambda<span class="fl">.1</span>se</span></code></pre></div>
<pre><code>[1] 0.05646944</code></pre>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="modelos-de-regularización.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cv.ridge$lambda.min</span></span></code></pre></div>
<p>y el correspondiente modelo contiene todas las variables explicativas:</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="modelos-de-regularización.html#cb323-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(cv.ridge) <span class="co"># s = &quot;lambda.1se&quot;</span></span></code></pre></div>
<pre><code>20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
                                 s1
(Intercept)             -0.72707742
texture_mean             0.51292464
area_mean                0.79924145
smoothness_mean          0.10549462
compactness_mean         0.26016484
symmetry_mean            0.09346757
fractal_dimension_mean  -0.34641170
texture_se              -0.01697213
area_se                  0.59895989
smoothness_se           -0.06167636
compactness_se          -0.05639525
concavity_se             0.14078849
`concave points_se`      0.21961982
symmetry_se             -0.14877280
fractal_dimension_se    -0.17141822
smoothness_worst         0.27744261
compactness_worst        0.32579681
`concave points_worst`   0.68781501
symmetry_worst           0.36925943
fractal_dimension_worst  0.15699353</code></pre>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="modelos-de-regularización.html#cb325-1" aria-hidden="true" tabindex="-1"></a><span class="co"># coef(cv.ridge, s = &quot;lambda.min&quot;)</span></span></code></pre></div>
<p>Finalmente evaluamos la precisión en la muestra de test:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="modelos-de-regularización.html#cb326-1" aria-hidden="true" tabindex="-1"></a>newx <span class="ot">&lt;-</span> breast_test_prep <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>diagnosis) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb326-2"><a href="modelos-de-regularización.html#cb326-2" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv.ridge, <span class="at">newx =</span> newx, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>) <span class="co"># s = &quot;lambda.1se&quot;</span></span>
<span id="cb326-3"><a href="modelos-de-regularización.html#cb326-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-4"><a href="modelos-de-regularización.html#cb326-4" aria-hidden="true" tabindex="-1"></a><span class="co"># es necesario poner `pred` como un vector</span></span>
<span id="cb326-5"><a href="modelos-de-regularización.html#cb326-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-6"><a href="modelos-de-regularización.html#cb326-6" aria-hidden="true" tabindex="-1"></a>xtab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">as.vector</span>(pred), breast_test_prep<span class="sc">$</span>diagnosis) </span>
<span id="cb326-7"><a href="modelos-de-regularización.html#cb326-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb326-8"><a href="modelos-de-regularización.html#cb326-8" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(xtab)</span></code></pre></div>
<pre><code>Confusion Matrix and Statistics

   
      B   M
  B 105   6
  M   2  57
                                          
               Accuracy : 0.9529          
                 95% CI : (0.9094, 0.9795)
    No Information Rate : 0.6294          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.8978          
                                          
 Mcnemar&#39;s Test P-Value : 0.2888          
                                          
            Sensitivity : 0.9813          
            Specificity : 0.9048          
         Pos Pred Value : 0.9459          
         Neg Pred Value : 0.9661          
             Prevalence : 0.6294          
         Detection Rate : 0.6176          
   Detection Prevalence : 0.6529          
      Balanced Accuracy : 0.9430          
                                          
       &#39;Positive&#39; Class : B               
                                          </code></pre>
</div>
<div id="ejemplo-lasso" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> Ejemplo: Lasso<a href="modelos-de-regularización.html#ejemplo-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>También podríamos ajustar modelos lasso con la opción por defecto de <code>glmnet()</code> (<code>alpha = 1</code>, lasso penalty).
Pero en este caso lo haremos al mismo tiempo que seleccionamos el parámetro de penalización por validación cruzada:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="modelos-de-regularización.html#cb328-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb328-2"><a href="modelos-de-regularización.html#cb328-2" aria-hidden="true" tabindex="-1"></a>cv.lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb328-3"><a href="modelos-de-regularización.html#cb328-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.lasso)</span></code></pre></div>
<p><img src="fig_out/unnamed-chunk-226-1.png" width="672" /></p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="modelos-de-regularización.html#cb329-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.lasso<span class="sc">$</span>glmnet.fit, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">label =</span> <span class="cn">TRUE</span>)    </span>
<span id="cb329-2"><a href="modelos-de-regularización.html#cb329-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">log</span>(cv.lasso<span class="sc">$</span>lambda<span class="fl">.1</span>se), <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb329-3"><a href="modelos-de-regularización.html#cb329-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">log</span>(cv.lasso<span class="sc">$</span>lambda.min), <span class="at">lty =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="fig_out/unnamed-chunk-226-2.png" width="672" /></p>
<p>El modelo resultante (oneSE rule) solo contiene 9 variables explicativas:</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="modelos-de-regularización.html#cb330-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(cv.lasso) <span class="co"># s = &quot;lambda.1se&quot;</span></span></code></pre></div>
<pre><code>20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
                                 s1
(Intercept)             -0.55527533
texture_mean             0.89281924
area_mean                2.06965054
smoothness_mean          .         
compactness_mean         .         
symmetry_mean            .         
fractal_dimension_mean   .         
texture_se               .         
area_se                  1.30248883
smoothness_se            .         
compactness_se          -0.02882114
concavity_se             .         
`concave points_se`      .         
symmetry_se             -0.08721175
fractal_dimension_se    -0.01522821
smoothness_worst         0.28720064
compactness_worst        .         
`concave points_worst`   2.01615906
symmetry_worst           0.66553914
fractal_dimension_worst  .         </code></pre>
<p>Por tanto este método también podría ser empleando para la selección de variables (puede hacerse automáticamente estableciendo <code>relax = TRUE</code>, ajustará los modelos sin regularización).</p>
<p>Finalmente evaluamos también la precisión en la muestra de test:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="modelos-de-regularización.html#cb332-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv.lasso, <span class="at">newx =</span> newx, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb332-2"><a href="modelos-de-regularización.html#cb332-2" aria-hidden="true" tabindex="-1"></a>xtab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">as.vector</span>(pred), breast_test_prep<span class="sc">$</span>diagnosis) </span>
<span id="cb332-3"><a href="modelos-de-regularización.html#cb332-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-4"><a href="modelos-de-regularización.html#cb332-4" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(xtab)</span></code></pre></div>
<pre><code>Confusion Matrix and Statistics

   
      B   M
  B 105   4
  M   2  59
                                          
               Accuracy : 0.9647          
                 95% CI : (0.9248, 0.9869)
    No Information Rate : 0.6294          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.9238          
                                          
 Mcnemar&#39;s Test P-Value : 0.6831          
                                          
            Sensitivity : 0.9813          
            Specificity : 0.9365          
         Pos Pred Value : 0.9633          
         Neg Pred Value : 0.9672          
             Prevalence : 0.6294          
         Detection Rate : 0.6176          
   Detection Prevalence : 0.6412          
      Balanced Accuracy : 0.9589          
                                          
       &#39;Positive&#39; Class : B               
                                          </code></pre>
<p>Podemos observar que lo hace mejor que el modelo Ridge que tiene más sobreajuste.</p>
</div>
<div id="ejemplo-elastic-net" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> Ejemplo: Elastic Net<a href="modelos-de-regularización.html#ejemplo-elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Podemos ajustar modelos elastic net para un valor concreto de <code>alpha</code> empleando la función <code>glmnet()</code>, pero las opciones del paquete no incluyen la selección de este hiperparámetro.
Aunque se podría implementar fácilmente (como se muestra en <code>help(cv.glmnet)</code>), resulta mucho más cómodo emplear el método <code>"glmnet"</code> de <code>caret</code> (NOTA: en este caso no hace falta indicarle `family = binomial” ya que le pasamos una variable factor como variable respuesta):</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="modelos-de-regularización.html#cb334-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb334-2"><a href="modelos-de-regularización.html#cb334-2" aria-hidden="true" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">&quot;glmnet&quot;</span>) </span></code></pre></div>
<pre><code>   model parameter                    label forReg forClass probModel
1 glmnet     alpha        Mixing Percentage   TRUE     TRUE      TRUE
2 glmnet    lambda Regularization Parameter   TRUE     TRUE      TRUE</code></pre>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="modelos-de-regularización.html#cb336-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb336-2"><a href="modelos-de-regularización.html#cb336-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Se podría emplear train(fidelida ~ ., data = train, ...)</span></span>
<span id="cb336-3"><a href="modelos-de-regularización.html#cb336-3" aria-hidden="true" tabindex="-1"></a>caret.glmnet <span class="ot">&lt;-</span> <span class="fu">train</span>(x, y, <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>,</span>
<span id="cb336-4"><a href="modelos-de-regularización.html#cb336-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>),</span>
<span id="cb336-5"><a href="modelos-de-regularización.html#cb336-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(</span>
<span id="cb336-6"><a href="modelos-de-regularización.html#cb336-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">.alpha =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">10</span>),  <span class="co"># optimize an elnet regression</span></span>
<span id="cb336-7"><a href="modelos-de-regularización.html#cb336-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">.lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.2</span>, <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb336-8"><a href="modelos-de-regularización.html#cb336-8" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb336-9"><a href="modelos-de-regularización.html#cb336-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb336-10"><a href="modelos-de-regularización.html#cb336-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb336-11"><a href="modelos-de-regularización.html#cb336-11" aria-hidden="true" tabindex="-1"></a>caret.glmnet</span></code></pre></div>
<pre><code>glmnet 

399 samples
 19 predictor
  2 classes: &#39;B&#39;, &#39;M&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 359, 359, 359, 360, 359, 359, ... 
Resampling results across tuning parameters:

  alpha      lambda      Accuracy   Kappa    
  0.0000000  0.00000000  0.9575000  0.9070474
  0.0000000  0.01052632  0.9575000  0.9070474
  0.0000000  0.02105263  0.9575000  0.9070474
  0.0000000  0.03157895  0.9575000  0.9070474
  0.0000000  0.04210526  0.9575000  0.9070474
  0.0000000  0.05263158  0.9500000  0.8905369
  0.0000000  0.06315789  0.9475000  0.8846684
  0.0000000  0.07368421  0.9475000  0.8840697
  0.0000000  0.08421053  0.9475000  0.8840697
  0.0000000  0.09473684  0.9475000  0.8840697
  0.0000000  0.10526316  0.9424359  0.8731520
  0.0000000  0.11578947  0.9399359  0.8671269
  0.0000000  0.12631579  0.9399359  0.8671269
  0.0000000  0.13684211  0.9374359  0.8617234
  0.0000000  0.14736842  0.9349359  0.8561699
  0.0000000  0.15789474  0.9349359  0.8561699
  0.0000000  0.16842105  0.9349359  0.8561699
  0.0000000  0.17894737  0.9349359  0.8561699
  0.0000000  0.18947368  0.9299359  0.8444259
  0.0000000  0.20000000  0.9274359  0.8387244
  0.1111111  0.00000000  0.9675000  0.9300161
  0.1111111  0.01052632  0.9750000  0.9457920
  0.1111111  0.02105263  0.9575000  0.9070474
  0.1111111  0.03157895  0.9550000  0.9011875
  0.1111111  0.04210526  0.9500000  0.8902304
  0.1111111  0.05263158  0.9475000  0.8840697
  0.1111111  0.06315789  0.9475000  0.8840697
  0.1111111  0.07368421  0.9475000  0.8840697
  0.1111111  0.08421053  0.9449359  0.8784093
  0.1111111  0.09473684  0.9449359  0.8784093
  0.1111111  0.10526316  0.9449359  0.8784093
  0.1111111  0.11578947  0.9449359  0.8784093
  0.1111111  0.12631579  0.9424359  0.8723842
  0.1111111  0.13684211  0.9399359  0.8668307
  0.1111111  0.14736842  0.9374359  0.8614272
  0.1111111  0.15789474  0.9324359  0.8505967
  0.1111111  0.16842105  0.9324359  0.8496833
  0.1111111  0.17894737  0.9299359  0.8439755
  0.1111111  0.18947368  0.9274359  0.8381070
  0.1111111  0.20000000  0.9274359  0.8381070
  0.2222222  0.00000000  0.9625000  0.9190533
  0.2222222  0.01052632  0.9750000  0.9457920
  0.2222222  0.02105263  0.9600000  0.9121584
  0.2222222  0.03157895  0.9550000  0.9011875
  0.2222222  0.04210526  0.9525000  0.8954917
  0.2222222  0.05263158  0.9475000  0.8840697
  0.2222222  0.06315789  0.9475000  0.8840697
  0.2222222  0.07368421  0.9474359  0.8838147
  0.2222222  0.08421053  0.9474359  0.8838147
  0.2222222  0.09473684  0.9474359  0.8838147
  0.2222222  0.10526316  0.9499359  0.8890778
  0.2222222  0.11578947  0.9499359  0.8890778
  0.2222222  0.12631579  0.9474359  0.8836724
  0.2222222  0.13684211  0.9449359  0.8781189
  0.2222222  0.14736842  0.9424359  0.8724112
  0.2222222  0.15789474  0.9399359  0.8663861
  0.2222222  0.16842105  0.9349359  0.8548098
  0.2222222  0.17894737  0.9349359  0.8548098
  0.2222222  0.18947368  0.9374359  0.8600412
  0.2222222  0.20000000  0.9324359  0.8489281
  0.3333333  0.00000000  0.9625000  0.9190533
  0.3333333  0.01052632  0.9700000  0.9345266
  0.3333333  0.02105263  0.9650000  0.9237261
  0.3333333  0.03157895  0.9575000  0.9067571
  0.3333333  0.04210526  0.9575000  0.9067571
  0.3333333  0.05263158  0.9575000  0.9061517
  0.3333333  0.06315789  0.9550000  0.9005982
  0.3333333  0.07368421  0.9524359  0.8949378
  0.3333333  0.08421053  0.9499359  0.8890778
  0.3333333  0.09473684  0.9474359  0.8836724
  0.3333333  0.10526316  0.9474359  0.8836724
  0.3333333  0.11578947  0.9474359  0.8836724
  0.3333333  0.12631579  0.9449359  0.8782670
  0.3333333  0.13684211  0.9424359  0.8727135
  0.3333333  0.14736842  0.9399359  0.8665406
  0.3333333  0.15789474  0.9374359  0.8606720
  0.3333333  0.16842105  0.9374359  0.8606720
  0.3333333  0.17894737  0.9349359  0.8551185
  0.3333333  0.18947368  0.9324359  0.8490823
  0.3333333  0.20000000  0.9324359  0.8490823
  0.4444444  0.00000000  0.9625000  0.9190533
  0.4444444  0.01052632  0.9725000  0.9403866
  0.4444444  0.02105263  0.9675000  0.9291315
  0.4444444  0.03157895  0.9625000  0.9174256
  0.4444444  0.04210526  0.9600000  0.9120202
  0.4444444  0.05263158  0.9600000  0.9120202
  0.4444444  0.06315789  0.9524359  0.8949378
  0.4444444  0.07368421  0.9499359  0.8890778
  0.4444444  0.08421053  0.9499359  0.8890778
  0.4444444  0.09473684  0.9474359  0.8836724
  0.4444444  0.10526316  0.9474359  0.8836724
  0.4444444  0.11578947  0.9474359  0.8832072
  0.4444444  0.12631579  0.9474359  0.8832072
  0.4444444  0.13684211  0.9374359  0.8609871
  0.4444444  0.14736842  0.9374359  0.8609871
  0.4444444  0.15789474  0.9374359  0.8609871
  0.4444444  0.16842105  0.9349359  0.8551185
  0.4444444  0.17894737  0.9324359  0.8492500
  0.4444444  0.18947368  0.9299359  0.8432138
  0.4444444  0.20000000  0.9274359  0.8373452
  0.5555556  0.00000000  0.9625000  0.9190533
  0.5555556  0.01052632  0.9700000  0.9348331
  0.5555556  0.02105263  0.9650000  0.9231334
  0.5555556  0.03157895  0.9625000  0.9174256
  0.5555556  0.04210526  0.9600000  0.9120202
  0.5555556  0.05263158  0.9575000  0.9064667
  0.5555556  0.06315789  0.9524359  0.8949464
  0.5555556  0.07368421  0.9499359  0.8890778
  0.5555556  0.08421053  0.9474359  0.8836724
  0.5555556  0.09473684  0.9474359  0.8832072
  0.5555556  0.10526316  0.9449359  0.8776537
  0.5555556  0.11578947  0.9449359  0.8776537
  0.5555556  0.12631579  0.9399359  0.8660774
  0.5555556  0.13684211  0.9349359  0.8551185
  0.5555556  0.14736842  0.9349359  0.8551185
  0.5555556  0.15789474  0.9349359  0.8551185
  0.5555556  0.16842105  0.9324359  0.8495650
  0.5555556  0.17894737  0.9199359  0.8200478
  0.5555556  0.18947368  0.9149359  0.8081288
  0.5555556  0.20000000  0.9124359  0.8017349
  0.6666667  0.00000000  0.9600000  0.9137920
  0.6666667  0.01052632  0.9725000  0.9402385
  0.6666667  0.02105263  0.9675000  0.9286869
  0.6666667  0.03157895  0.9625000  0.9174256
  0.6666667  0.04210526  0.9575000  0.9064667
  0.6666667  0.05263158  0.9550000  0.9006068
  0.6666667  0.06315789  0.9524359  0.8949464
  0.6666667  0.07368421  0.9549359  0.9001890
  0.6666667  0.08421053  0.9499359  0.8892301
  0.6666667  0.09473684  0.9449359  0.8776537
  0.6666667  0.10526316  0.9424359  0.8717852
  0.6666667  0.11578947  0.9399359  0.8663798
  0.6666667  0.12631579  0.9324359  0.8495650
  0.6666667  0.13684211  0.9299359  0.8435288
  0.6666667  0.14736842  0.9299359  0.8435288
  0.6666667  0.15789474  0.9224359  0.8259163
  0.6666667  0.16842105  0.9199359  0.8202085
  0.6666667  0.17894737  0.9149359  0.8076035
  0.6666667  0.18947368  0.9099359  0.7961880
  0.6666667  0.20000000  0.9049359  0.7842832
  0.7777778  0.00000000  0.9575000  0.9086749
  0.7777778  0.01052632  0.9725000  0.9402385
  0.7777778  0.02105263  0.9675000  0.9286869
  0.7777778  0.03157895  0.9650000  0.9229791
  0.7777778  0.04210526  0.9575000  0.9064667
  0.7777778  0.05263158  0.9524359  0.8949464
  0.7777778  0.06315789  0.9524359  0.8944812
  0.7777778  0.07368421  0.9499359  0.8890758
  0.7777778  0.08421053  0.9474359  0.8835223
  0.7777778  0.09473684  0.9374359  0.8614375
  0.7777778  0.10526316  0.9374359  0.8614375
  0.7777778  0.11578947  0.9324359  0.8501824
  0.7777778  0.12631579  0.9324359  0.8501824
  0.7777778  0.13684211  0.9299359  0.8444747
  0.7777778  0.14736842  0.9274359  0.8386061
  0.7777778  0.15789474  0.9224359  0.8270298
  0.7777778  0.16842105  0.9199359  0.8205370
  0.7777778  0.17894737  0.9149359  0.8089607
  0.7777778  0.18947368  0.9124359  0.8027495
  0.7777778  0.20000000  0.9049359  0.7842832
  0.8888889  0.00000000  0.9575000  0.9086749
  0.8888889  0.01052632  0.9750000  0.9456439
  0.8888889  0.02105263  0.9675000  0.9286869
  0.8888889  0.03157895  0.9625000  0.9174256
  0.8888889  0.04210526  0.9500000  0.8900863
  0.8888889  0.05263158  0.9474359  0.8845420
  0.8888889  0.06315789  0.9424359  0.8737368
  0.8888889  0.07368421  0.9374359  0.8624851
  0.8888889  0.08421053  0.9374359  0.8624851
  0.8888889  0.09473684  0.9374359  0.8620488
  0.8888889  0.10526316  0.9349359  0.8564953
  0.8888889  0.11578947  0.9374359  0.8622030
  0.8888889  0.12631579  0.9349359  0.8565014
  0.8888889  0.13684211  0.9299359  0.8450921
  0.8888889  0.14736842  0.9224359  0.8274950
  0.8888889  0.15789474  0.9149359  0.8102110
  0.8888889  0.16842105  0.9124359  0.8041747
  0.8888889  0.17894737  0.9124359  0.8032061
  0.8888889  0.18947368  0.9073718  0.7913259
  0.8888889  0.20000000  0.9023718  0.7784392
  1.0000000  0.00000000  0.9550000  0.9035657
  1.0000000  0.01052632  0.9725000  0.9400904
  1.0000000  0.02105263  0.9650000  0.9239989
  1.0000000  0.03157895  0.9525000  0.8962252
  1.0000000  0.04210526  0.9525000  0.8960710
  1.0000000  0.05263158  0.9424359  0.8735983
  1.0000000  0.06315789  0.9399359  0.8681929
  1.0000000  0.07368421  0.9374359  0.8626394
  1.0000000  0.08421053  0.9349359  0.8570859
  1.0000000  0.09473684  0.9324359  0.8518433
  1.0000000  0.10526316  0.9324359  0.8518433
  1.0000000  0.11578947  0.9299359  0.8461418
  1.0000000  0.12631579  0.9349359  0.8565014
  1.0000000  0.13684211  0.9349359  0.8565014
  1.0000000  0.14736842  0.9299359  0.8450921
  1.0000000  0.15789474  0.9224359  0.8276472
  1.0000000  0.16842105  0.9173718  0.8159432
  1.0000000  0.17894737  0.9098718  0.7981742
  1.0000000  0.18947368  0.8998077  0.7740354
  1.0000000  0.20000000  0.8896795  0.7496070

Accuracy was used to select the optimal model using the largest value.
The final values used for the model were alpha = 0.1111111 and lambda = 0.01052632.</code></pre>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="modelos-de-regularización.html#cb338-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(caret.glmnet, <span class="at">highlight =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="fig_out/unnamed-chunk-229-1.png" width="672" /></p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="modelos-de-regularización.html#cb339-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(caret.glmnet<span class="sc">$</span>finalModel, newx, </span>
<span id="cb339-2"><a href="modelos-de-regularización.html#cb339-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">type=</span><span class="st">&quot;class&quot;</span>, <span class="at">s =</span> caret.glmnet<span class="sc">$</span>finalModel<span class="sc">$</span>lambdaOpt)</span>
<span id="cb339-3"><a href="modelos-de-regularización.html#cb339-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb339-4"><a href="modelos-de-regularización.html#cb339-4" aria-hidden="true" tabindex="-1"></a>xtab <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">as.vector</span>(pred), breast_test_prep<span class="sc">$</span>diagnosis) </span>
<span id="cb339-5"><a href="modelos-de-regularización.html#cb339-5" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(xtab)</span></code></pre></div>
<pre><code>Confusion Matrix and Statistics

   
      B   M
  B 104   3
  M   3  60
                                          
               Accuracy : 0.9647          
                 95% CI : (0.9248, 0.9869)
    No Information Rate : 0.6294          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.9243          
                                          
 Mcnemar&#39;s Test P-Value : 1               
                                          
            Sensitivity : 0.9720          
            Specificity : 0.9524          
         Pos Pred Value : 0.9720          
         Neg Pred Value : 0.9524          
             Prevalence : 0.6294          
         Detection Rate : 0.6118          
   Detection Prevalence : 0.6294          
      Balanced Accuracy : 0.9622          
                                          
       &#39;Positive&#39; Class : B               
                                          </code></pre>
<p>Observamos que <code>alpha</code> está cerca de 1 (Lasso) y tiene un <code>alpha</code> un poco mayor que al que se obtiene con la regresión Lasso, pero la matriz de confusión es la misma.</p>

</div>
</div>
</div>








            </section>

          </div>
        </div>
      </div>
<a href="respuesta-no-balanceada.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/isglobal-brge/curso_machine_learning/tree/master/docs08-penalizaed_regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
